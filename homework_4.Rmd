---
title: "Homework 4"
author: "Author: Matthew J. Cossley"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float:
        collapsed: true
        smooth_scroll: true
    toc_depth: 3
    fig_caption: yes
    number_sections: false
    theme: cosmo
fontsize: 14pt
---

<div style="color:#990000">
**Note that this homework is not yet finalised. I am posting
it here for practice purposes only.**
<br></br>
</div>

<style type="text/css">
.table {
    width: 40%;
***
</style>


```{r, echo=F}
library(data.table)
library(ggplot2)
library(ggpubr)
```

## General instructions
When responding to every question that requires you to
perform a hypothesis test, please follow the instructions
laid out below.

<b>1\. Specify the null and alternative hypotheses ($H_0$ and
   $H_1$) in terms of a distribution and population
   parameter.</b>

You will need to perform this step in order to correctly
perform any hypothesis test, but for the homework, just
writing it in comments is adequate (your comments won't be
graded).
   
<b>2\. Specify the type I error rate -- denoted by the symbol
   $\alpha$ -- you are willing to tolerate.</b>

For every problem in this homework use $\alpha=0.05$. Again,
writing this in comments is adequate (your comments won't be
graded).
   
<b>3\. Specify the sample statistic that you will use to
   estimate the population parameter in step 1 and state how
   it is distributed under the assumption that $H_0$ is
   true.</b>

* Create a vector named `theta` that contains the possible
outcomes of $\widehat{\theta}$.

   * If $\widehat{\theta}$ is discrete with a finite range
   then `theta_hat_x` must contain all possible outcomes.
   
   * If $\widehat{\theta}$ is continuous with a finite range
   then use `theta_hat_x` must start at the minimum valued
   outcome, end at the maximum valued outcome, and contain
   all the values between these two endpoints in increments
   of `0.01`.
   
   * If $\widehat{\theta}$ is discrete with an infinite
   range then `theta_hat_x` must start 4 standard deviations
   below the the mean, end 3 standard deviations above the
   mean, and must contain all the values between these two
   endpoints in increments of `0.01`.
   
   * If $\widehat{\theta}$ is continuous with an infinite
   range then `theta_hat_x` must start 4 standard deviations
   below the the mean, end 3 standard deviations above the
   mean, and must contain all the values between these two
   endpoints in increments of `0.01`.

* Create a variable named `theta_hat_p` and set its value to
the probability mass (if discrete) or probability density
(if continuous) corresponding to each possible outcome of
$\widehat{\theta}$ stored in variable `theta_hat_x`.

* Plot the $\widehat{\theta}$ sampling distribution. Put the
possible outcomes of $\widehat{\theta}$ on the x-axis and
put the probability density or mass (whichever is
appropriate) on the y-axis. Use only `ggplot()` and
`geom_line()` (if continuous) or `geom_point()` (if
discrete). You should use this plot to contextualize the
p-value and critical value computed below (i.e., use it to
check your work!).
   
<b>4\. Obtain a random sample and use it to compute the
   sample statistic from step 3. Call this value
   $\widehat{\theta}_{\text{obs}}$</b>

* Create a variable named `theta_hat_obs` and set its value to
$\widehat{\theta}_{\text{obs}}$.

<b>5\. If $\widehat{\theta}_{\text{obs}}$ is very unlikely to
   occur under the assumption that $H_0$ is true, then
   reject $H_0$. Otherwise, do not reject $H_0$.</b>

* Create a variable named `p_val` and set its value to
$P(\widehat{\theta}_{\text{obs}} | H_0)$.

* Create a value named `decision` and set its value either to
`reject H0` or `fail to reject H0`.

* If performing a one-tailed test, create a variable named
`theta_hat_crit` and set its value to the critical value. If
performing a 2-tailed test, create two variables,
`theta_hat_crit_lower` and `theta_hat_crit_upper` and set
their values to the corresponding critical values.

* Calculate the 95% confidence interval estimate of the
parameter in step 1. Create a variable named `ci_lower` and
set its value to the lower bound of this confidence
interval, and create a variable named `ci_upper` and set its
value to the upper bound of this confidence interval. Make
sure that the decision you reach on the basis of this
confidence interval is the same as you reached using the
p-value.

6\. If there is a built-in R function that will perform the
test in question, use and verify that it returns the same
results as running through steps 1 through 5 in long form. 

* Create a variable named `r_result` and set its value to
the result of the built-in R function. If no built-in
function exists, set `r_result <- "does not exist"`.

7\. Create problem-specific variables to store the work form
the previous steps to facilitate grading.

* For a one-tailed test use:
```{r, eval=F}
prob_xa <- theta_hat_x
prob_xb <- theta_hat_p
prob_xc <- theta_hat_obs
prob_xd <- pval
prob_xe <- theta_hat_crit
prob_xf <- ci_lower
prob_xg <- ci_upper
```

* For a two-tailed test use:
```{r, eval=F}
prob_xa <- theta_hat_x
prob_xb <- theta_hat_p
prob_xc <- theta_hat_obs
prob_xd <- pval_lower
prob_xe <- pval_upper
prob_xf <- theta_hat_crit_lower
prob_xg <- theta_hat_crit_upper
prob_xh <- ci_lower
prob_xi <- ci_upper
```

* Be sure to replace the `x` in the `prob_x` above with the
problem number.

## Problem 0

* Load `data.table` and `ggplot2` but no other packages.

* Do not call `install.packages` anywhere in this script.
Once you installed them once, there is no need to install
them again. If you need to install some other package in the
future, do so by using `install.packages` function in the
console.

* Make sure you are starting with a clean environment by
running `rm(list=ls())`. Running this line of code will
erase any variable defined before it. Do not put this line
of code after anything important to keep for grading.

* Create a variable named `my_name` and set its value equal
to a **character** vector (i.e., letters surrounded by `""`)
containing your name.

* Create a variable named `my_student_id` and set its value
equal to a **character** vector (i.e., letters surrounded by
`""`) containing your student id.

## Problem 1

**(a)** load the data located in the csv file linked below
into a data.table named `d`.

https://crossley.github.io/cogs2020/data/nhp_cat_learn/ii_gabor.csv

This data file contains the results from a monkey performing
a category learning experiment similar to those that we have
seen a handful of times already in this class. On each trial
of the experiment the monkey sees a sine-wave grating and
must learn through trial and error whether that grating is a
member of category A or category B.

**(b)** Change the column names to the following

`col_names <- c('cat', 'x', 'y', 'resp', 'rt', 'phase')`

**(c)** Add a column named `trial` to the data.table that
indicates the trial of the experiment. Note that `trial`
should start at 1 and increment by 1 all the way to the
number of total rows in `d`.

**(d)** Add a column named `block` to the `data.table` that
indicates the block of the experiment, assuming a block size
of 100 trials per block. Note that a block size of 100 does
not divide evenly into the number of rows in `d` (i.e.,
8196), so you will have to be careful and clever in dealing
with this. However you deal with it, the final 96 trials in
`d` should correspond to a block value of 82.

**(e)** Add a column named `acc` to the `data.table` that
indicates whether the response on each trial was correct or
incorrect (e.g., `cat == resp`).

**(f)** Create a new data.table named `dd` that contains
columns for the mean accuracy per block (name this column
`acc_mean`) and the SEM (name this column `acc_err`) per
`block`, grouped by each unique combination of `block` and
`phase`.

**(g)** Plot the mean accuracy per block (`block` on the
x-axis, `acc_mean` on the y-axis). Include error bars at
each data point that represent SEM. Plot phase 1 and phase 2
in different colours.

## Note on problem 2 - 4
In the dataset from Problem 1, each trial is a Bernoulli
trial with probability of success $p$. This means that
trials are distributed $X \sim Bernoulli(p)$. This also
means that blocks of trials are distributed $X \sim
Binomial(n,p)$, where $n$ is the block size (i.e., number of
trials per block).

## Problem 2
* Test the hypothesis that, for the first 100 trials of
phase 1, the monkey is doing no better and no worse than
guessing (i.e., $p = .5$).

## Problem 3
* Test the hypothesis that, for the last 100 trials of phase
1, the monkey is doing better than guessing (i.e., $p =
.5$).

## Problem 4
* Test the hypothesis that, for the first 100 trials of
phase 2, the monkey is doing worse than guessing (i.e., $p =
.5$).

## Note on problems 5-8
Above, we viewed each trial as $Bernoulli(p)$, and each $n$
trials as $Binomial(n,p)$. An alternative framing is to
ignore each individual trial, and instead consider the
random variable

$X = \text{mean accuracy per block}$

In this framing, the experiment results are sampled from
blocks of trials, not from individual trials.

## Problem 5
* Make a deep copy of `d` named `d2`, and then redefine the
block column of `d2` to reflect a block size of 25 trials
per block instead of 100. Then, using `d2` as a base, create
a new data.table named `dd2` that contains columns for the
mean accuracy per block (name this column `acc_mean`) and
the SEM (name this column `acc_err`) per `block`, grouped by
each unique combination of `block` and `phase`. Repeat the
mean accuracy plot using the new block size to visually
check that your work is doing what you think it's doing
(this plot will not be graded).

## Problem 6
* Test the hypothesis that the mean accuracy in the first 16
blocks of phase 1 is no greater or worse than chance (i.e.
50% correct responses).

## Problem 7
* Test the hypothesis that the mean accuracy in the last 16
blocks of phase 1 is greater than chance.

## Problem 8
* Test the hypothesis that the mean accuracy in the first 16
blocks of phase 2 less than chance.

## Problem 9
What accounts for the drop in accuracy between phase 1 and
phase 2? To answer this question, investigate the stimuli
the monkey was required to learn between phase 1 and phase
2. Using `ggplot()` and `geom_point()`, make a plot of
stimuli (`x=x, y=y`) coloured by category membership label
(`cat`). Include a separate version of this plot for each
phase in different panels by using `facet_wrap()`. Save the
resulting plot to a variable named `g`.

## problem 10
For the first 16 blocks of phase 1, assume that the true
state of the universe is $\mu=.6$. Given $H_0:\mu=.5$

* compute $\alpha$ and store its value in a variable named `ans_9a`

* compute $\beta$ and store its value in a variable named `ans_9b`

* compute $power$ and store its value in a variable named `ans_9c`

## Problem 11
The number of action potentials generated from a given
neuron in its baseline state when observed for a fixed
period of time is well described by a Poisson distribution.
The Poisson distribution is fully characterised by a single
parameter $\lambda$ called the rate parameter. It turns out
that $\lambda$ is also the mean of the Poisson distribution.
Suppose a group of researchers measured a particular neuron
and observed 5 action potentials in a given time period. Let
$X$ be the Poisson random variable that generated this
result. Test the hypothesis that $\lambda > 4$. You will
need to search the web or use the built-in R help to look
for functions that tell you about Poisson probabilities etc.

## Problem 12

```{r, echo=F, fig.width=10}
n <- 2

mu_x_0 <- 5
sigma_x <- 2 / sqrt(n)
x_crit <- qnorm(0.95, mu_x_0, sigma_x, lower.tail=F)
mu_x_1 <- x_crit

x <- seq(mu_x_0 - 5*sigma_x, mu_x_1 + 5*sigma_x, 0.01)
fx0 <- dnorm(x, mu_x_0, sigma_x)
fx1 <- dnorm(x, mu_x_1, sigma_x)
d <- data.table(x, fx0, fx1)
d[x <= x_crit, region0 := 'I'] # confidence
d[x > x_crit, region0 := 'II'] # alpha
d[x <= x_crit, region1 := 'III'] # beta
d[x > x_crit, region1 := 'IV'] # power

ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx0)) +
  geom_line(aes(y=fx1)) +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d, aes(x=x, ymin=0, ymax=fx0, fill=region0), alpha=0.25) +
  geom_ribbon(data=d, aes(x=x, ymin=0, ymax=fx0, fill=region0), alpha=0.25) +
  geom_ribbon(data=d, aes(x=x, ymin=0, ymax=fx1, fill=region1), alpha=0.25) +
  geom_ribbon(data=d, aes(x=x, ymin=0, ymax=fx1, fill=region1), alpha=0.25) +
  scale_x_continuous(breaks=c(mu_x_1, mu_x_0), labels=c('H1', 'H0')) +
  ylab('Probability Density') +
  theme(legend.title = element_blank())
```

Please respond by assigning `"confidence"`, `"alpha"`,
`"beta"`, or `"power"` to the variables requested below.

* What quantity does region I in the above plot correspond
to? Store your answer in a variable named `ans_12a`.

* What quantity does region II in the above plot correspond
to? Store your answer in a variable named `ans_12b`.

* What quantity does region III in the above plot correspond
to? Store your answer in a variable named `ans_12c`.

* What quantity does region IV in the above plot correspond
to? Store your answer in a variable named `ans_12d`.

* What is the numeric value (to one decimal place) of the
power in this example? Store your answer in a variable named
`ans_12e`.

* Assuming a type I error rate of 0.05, what is the numeric
value (to one decimal place) of confidence of this example?
Store your answer in a variable named `ans_12f`.

* Assuming that the illustrated distributions correspond to
the distribution of sample means, will increasing the sample
size increase the distance between the mean of H1 and H0
distributions? Store your answer using `ans_12g <- "YES"` or
`ans_12g <- "NO"`.

