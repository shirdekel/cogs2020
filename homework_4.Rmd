---
title: "Homework 4"
author: "Author: Matthew J. Cossley"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float:
        collapsed: true
        smooth_scroll: true
    toc_depth: 3
    fig_caption: yes
    number_sections: false
    theme: cosmo
fontsize: 14pt
---

<style type="text/css">
.table {
    width: 40%;
***
</style>


```{r, echo=F}
library(data.table)
library(ggplot2)
library(ggpubr)
```

## General instructions
When responding to every question that requires you to
perform a hypothesis test, please follow the instructions
laid out below.

<b>1\. Specify the null and alternative hypotheses ($H_0$ and
   $H_1$) in terms of a distribution and population
   parameter $\theta$.</b>

You will need to perform this step in order to correctly
perform any hypothesis test, but for the homework, just
writing it in comments is adequate (your comments won't be
graded). E.g., if asked to test the hypothesis that the true
mean of a random variable is greater than 0.5, you might
write:

```{r, eval=F}
# In this problem, the parameter theta that we are
# interested in is the population mean mu

# H0: mu = 0.5
# H1: mu > 0.5
```

<b>2\. Specify the type I error rate -- denoted by the symbol
   $\alpha$ -- you are willing to tolerate.</b>

For every problem in this homework use $\alpha=0.05$. Again,
writing this in comments is adequate (your comments won't be
graded). E.g., you could write:

```{r, eval=F}
# set type I error rate
# alpha = 0.05
```
   
<b>3\. Specify the sample statistic that you will use to
   estimate the population parameter in step 1 and state how
   it is distributed under the assumption that $H_0$ is
   true.</b>
   
* A good way to begin is to again write out some comments.
To continue with the example from step 1, you might begin by
writing:

```{r, eval=F}
# theta_hat = mu_hat
# mu_hat = x_bar
# x_bar ~ N(mu_x_bar, sig_x_bar)
# mu_x_bar = mu_x
# sig_x_bar = sig_x

# NOTE: This example assumes sig_x is known.
```

* Create a vector named `theta_hat_x` that contains the possible
outcomes of $\widehat{\theta}$.

   * If $\widehat{\theta}$ is discrete with a finite range
   then `theta_hat_x` must contain all possible outcomes of
   $\widehat{\theta}$.
   
   * If $\widehat{\theta}$ is continuous with a finite range
   then use `theta_hat_x` must start at the minimum valued
   outcome of $\widehat{\theta}$, end at the maximum valued
   outcome of $\widehat{\theta}$, and contain all the values
   between these two endpoints in increments of `0.01`.
   
   * If $\widehat{\theta}$ is discrete with an infinite
   range then `theta_hat_x` must start 4 standard deviations
   below the the mean of the $\widehat{\theta}$ sampling
   distribution assuming the null hypothesis is true, end 3
   standard deviations above the mean of the
   $\widehat{\theta}$ sampling distribution, and must
   contain all the values between these two endpoints in
   increments of `0.01`.
   
   * If $\widehat{\theta}$ is continuous with an infinite
   range then `theta_hat_x` must start 4 standard deviations
   below the the mean of the $\widehat{\theta}$ sampling
   distribution, end 4 standard deviations above the mean of
   the $\widehat{\theta}$ sampling distribution, and must
   contain all the values between these two endpoints in
   increments of `0.01`.
   
* Carrying on with the example, you would write:

```{r, eval=F}
# theta_hat = mu_hat
# mu_hat = x_bar

# x_bar is continuous

# NOTE: You will have to think about whether or not x_bar
# has a finite or infinite range for each particular
# scenario. For this exampl, lets assume it is infinite
# range.

theta_hat_x <- seq(mu_x_bar - 4*sig_x_bar, mu_x_bar + 4*sig_x_bar, 0.01)
```

* Create a variable named `theta_hat_p` and set its value to
the probability mass (if discrete) or probability density
(if continuous) corresponding to each possible outcome of
$\widehat{\theta}$ stored in variable `theta_hat_x`.

```{r, eval=F}
# Since theta_hat = x_bar and x_bar is normal, we use dnorm()
# Assuming that you have previously defined  mu_x_bar and
# sig_x_bar, you could write:
theta_hat_p <- dnorm(theta_hat_x, mu_x_bar, sig_x_bar)
```

* Plot the $\widehat{\theta}$ sampling distribution. Put the
possible outcomes of $\widehat{\theta}$ on the x-axis and
put the probability density or mass (whichever is
appropriate) on the y-axis. Use only `ggplot()` and
`geom_line()` (if continuous) or `geom_point()` (if
discrete). You should use this plot to contextualize the
p-value and critical value computed below (i.e., use it to
check your work!).

```{r, eval=F}
d <- data.table(theta_hat_x, theta_hat_p)
ggplot(d, aes(theta_hat_x, theta_hat_p)) +
   geom_line()  
   # geom_line() because in this example theta_har is x_bar is continuous
```

   
<b>4\. Obtain a random sample and use it to compute the
   sample statistic from step 3. Call this value
   $\widehat{\theta}_{\text{obs}}$</b>

* Create a variable named `theta_hat_obs` and set its value to
$\widehat{\theta}_{\text{obs}}$.

```{r, eval=F}
# Obtaining the observed value will be matter of wrangling a
# data.table (named dt in the code example below) or maybe
# simply reading the problem carefully. In this example,
# look something like:
x_bar_obs <- dt[, mean(x)]
theta_hat_obs <- x_bar_obs
```

<b>5\. If $\widehat{\theta}_{\text{obs}}$ is very unlikely to
   occur under the assumption that $H_0$ is true, then
   reject $H_0$. Otherwise, do not reject $H_0$.</b>

* Create a variable named `p_val` and set its value to
$P(\widehat{\theta}_{\text{obs}} | H_0)$.

```{r, eval=F}
# In this example, p_hat_obs is x_bar_obs which comes from a
# Normal so we use pnorm() with lower.tail determined by the
# alternative hypothesis.
p_val <- pnorm(x_bar_obs, mu_x_bar, sig_x_bar, lower.tail=FALSE)
```

* Create a value named `decision` and set its value either to
`"reject H0"` or `"fail to reject H0"`.

```{r, eval=F}
# Lets assume p_val < 0.05. 
# Then we would write:
decision <- 'reject H0'
```

* If performing a one-tailed test, create a variable named
`theta_hat_crit` and set its value to the critical value. If
performing a 2-tailed test, create two variables,
`theta_hat_crit_lower` and `theta_hat_crit_upper` and set
their values to the corresponding critical values.

```{r, eval=F}
# Our running example is a one-tailed test so we only have
# one critical value. Furthermore, x_bar is normal so we use
# the qnorm() function to get the x_bar outcome that
# corresponds to P(X > x | H0) < 0.05.
theta_hat_crit <- qnorm(0.05, mu_x_bar, sig_x_bar, lower.tail=FALSE)
```

* Calculate the 95% confidence interval estimate of the
parameter in step 1. Create a variable named `ci_lower` and
set its value to the lower bound of this confidence
interval, and create a variable named `ci_upper` and set its
value to the upper bound of this confidence interval. Make
sure that the decision you reach on the basis of this
confidence interval is the same as you reached using the
p-value.

```{r, eval=F}
# The width of the confidence interval is easy to obtain if
# you remember that it is the same as the width between the
# two critical values if your test was a two-tailed test.
x_bar_crit_lower <- qnorm(0.025, mu_x_bar, sig_x_bar, lower.tail=TRUE)
x_bar_crit_upper <- qnorm(0.025, mu_x_bar, sig_x_bar, lower.tail=FALSE)
x_bar_ci_width <- x_bar_crit_upper - x_bar_crit_lower

# The centre of the confidence interval is just the
# theta_hat_obs, which in this case is x_bar_obs.
x_bar_ci_lower <- x_bar_obs - x_bar_xi_width / 2
x_bar_ci_upper <- x_bar_obs + x_bar_xi_width / 2
```


<b>6\. If there is a built-in R function that will perform the
test in question, use and verify that it returns the same
results as running through steps 1 through 5 in long form.</b>

* Create a variable named `r_result` and set its value to
the result of the built-in R function. If no built-in
function exists, set `r_result <- "does not exist"`.

```{r, eval=F}
# In our running example we know sig_x, and so we are
# performing a Normal test. There is no built-in R function
# to perform this test.
r_result <- "does not exist"

# If you did not know sig_x, and you were consequently
# performing a t-test, then you would use code similar to:
x <- dt[, x]
r_result <- t.test(x, mu=0.5, alternative='greater')
```

<b>7\. Create problem-specific variables to store the work form
the previous steps to facilitate grading.</b>

* For a one-tailed test use:
```{r, eval=F}
prob_x_theta_hat_x <- theta_hat_x
prob_x_theta_hat_p <- theta_hat_p
prob_x_theta_hat_obs <- theta_hat_obs
prob_x_pval <- p_val
prob_x_theta_hat_crit <- theta_hat_crit
prob_x_ci_lower <- theta_hat_ci_lower
prob_x_ci_upper <- theta_hat_ci_upper
prob_x_decision <- decision
prob_x_r_result <- r_result
```

* For a two-tailed test use:
```{r, eval=F}
prob_x_theta_hat_x <- theta_hat_x
prob_x_theta_hat_p <- theta_hat_p
prob_x_theta_hat_obs <- theta_hat_obs
prob_x_pval_lower <- p_val_lower
prob_x_pval_upper <- p_val_upper
prob_x_theta_hat_crit_lower <- theta_hat_crit_lower
prob_x_theta_hat_crit_upper <- theta_hat_crit_upper
prob_x_ci_lower <- theta_hat_ci_lower
prob_x_ci_upper <- theta_hat_ci_upper
prob_x_decision <- decision
prob_x_r_result <- r_result
```

* Be sure to replace the `x` in the `prob_x` above with the
problem number.

* **NOTE:** In all cases, but especially when performing
t-tests, please think carefully about what
$\widehat{\theta}_{\text{obs}}$ is estimating. In
particular, be sure that the variable `theta_hat_obs`
contains a number that estimates the parameter in step 1.

## Problem 0

* Load `data.table` and `ggplot2` but no other packages.

* Do not call `install.packages` anywhere in this script.
Once you installed them once, there is no need to install
them again. If you need to install some other package in the
future, do so by using `install.packages` function in the
console.

* Make sure you are starting with a clean environment by
running `rm(list=ls())`. Running this line of code will
erase any variable defined before it. Do not put this line
of code after anything important to keep for grading.

* Create a variable named `my_name` and set its value equal
to a **character** vector (i.e., letters surrounded by `""`)
containing your name.

* Create a variable named `my_student_id` and set its value
equal to a **character** vector (i.e., letters surrounded by
`""`) containing your student id.

## Problem 1

**(a)** load the data located in the csv file linked below
into a data.table named `d`.

https://crossley.github.io/cogs2020/data/nhp_cat_learn/ii_gabor.csv

This data file contains the results from a monkey performing
a category learning experiment similar to those that we have
seen a handful of times already in this class. On each trial
of the experiment the monkey sees a sine-wave grating and
must learn through trial and error whether that grating is a
member of category A or category B.

**(b)** Change the column names to the following

`col_names <- c('cat', 'x', 'y', 'resp', 'rt', 'phase')`

**(c)** Add a column named `trial` to the data.table that
indicates the trial of the experiment. Note that `trial`
should start at 1 and increment by 1 all the way to the
number of total rows in `d`.

**(d)** Add a column named `block` to the `data.table` that
indicates the block of the experiment, assuming a block size
of 100 trials per block. Note that a block size of 100 does
not divide evenly into the number of rows in `d` (i.e.,
8196), so you will have to be careful and clever in dealing
with this. However you deal with it, the final 96 trials in
`d` should correspond to a block value of 82.

**(e)** Add a column named `acc` to the `data.table` that
indicates whether the response on each trial was correct or
incorrect (e.g., `cat == resp`).

**(f)** Create a new data.table named `dd` that contains
columns for the mean accuracy per block (name this column
`acc_mean`) and the SEM (name this column `acc_err`) per
block, grouped by each unique combination of `block` and
`phase`. Please ensure that `dd` contains only `acc_mean`,
`acc_err`, `block`, and `phase` columns.

**(g)** Replicate the following plot exactly using only
`ggplot()`, `geom_line()` and `geom_pointrange()`. Save
resulting ggplot object as a variable named `g`.

```{r, echo=F}
library(data.table)
library(ggplot2)

rm(list=ls())

# 1
# a 
d <- fread('https://crossley.github.io/cogs2020/data/nhp_cat_learn/ii_gabor.csv')

# b
col_names <- c('cat', 'x', 'y', 'resp', 'rt', 'phase')
setnames(d, col_names)

# c
d[, trial := 1:.N]

# d
n_trials <- d[, .N]
block_size = 100
n_blocks <- n_trials / block_size + 1
block <- rep(1:n_blocks, each=block_size)
block <- block[1:n_trials]
d[, block := block]

# e
d[, acc := cat == resp]

# f
dd <- d[, .(acc_mean=mean(acc), acc_err=sd(acc) / sqrt(.N)), .(block, phase)]

# g
g <- ggplot(dd, aes(block, acc_mean, colour=factor(phase))) + 
  geom_line() +
  geom_pointrange(aes(x=block, 
                      ymin=acc_mean-acc_err, 
                      ymax=acc_mean+acc_err))
g
```


## Problem 2 - 4
In the dataset from Problem 1, each trial is a Bernoulli
trial with probability of success $p$. This means that
trials are distributed $X \sim Bernoulli(p)$. This also
means that blocks of trials are distributed $X \sim
Binomial(n,p)$, where $n$ is the block size (i.e., number of
trials per block).

**Problem 2.** Test the hypothesis that, for the first 100
trials of phase 1, the monkey is doing no better and no
worse than guessing (i.e., $p = .5$).

**Problem 3.** Test the hypothesis that, for the last 100
trials of phase 1, the monkey is doing better than guessing
(i.e., $p = .5$). For this problem, only `prob_x_r_result`
will be graded.

**Problem 4.** Test the hypothesis that, for the first 100
trials of phase 2, the monkey is doing worse than guessing
(i.e., $p = .5$). For this problem, only `prob_x_r_result`
will be graded.

## Problem 5-8
Above, we viewed each trial as $Bernoulli(p)$, and each $n$
trials as $Binomial(n,p)$. An alternative framing is to
ignore each individual trial, and instead consider the
random variable

$X = \text{mean accuracy per block}$

In this framing, the experiment results are sampled from
blocks of trials, not from individual trials.

**Problem 5.** Make a deep copy of `d` named `d2`, and then
redefine the block column of `d2` to reflect a block size of
25 trials per block instead of 100. Then, using `d2` as a
base, create a new data.table named `dd2` that contains
columns for the mean accuracy per block (name this column
`acc_mean`) and the SEM (name this column `acc_err`) per
`block`, grouped by each unique combination of `block` and
`phase`. Please ensure that `dd2` contains only `acc_mean`,
`acc_sem`, `block`, and `phase` columns. Repeat the mean
accuracy plot using the new block size to visually check
that your work is doing what you think it's doing (this plot
will not be graded). 

Also, when reporting confidence intervals, remember that you
are to report an interval estimate of the parameter in step
1. This means that if you are performing a t-test, you will
have to be careful when converting between t and X
distributions.

**Problem 6.** Test the hypothesis that the mean accuracy in
the first 16 blocks of phase 1 is no greater or worse than
chance (i.e. 50% correct responses).

**Problem 7.** Test the hypothesis that the mean accuracy in
the last 16 blocks of phase 1 is greater than chance. For
this problem, only `prob_x_r_result` will be graded.

**Problem 8.** Test the hypothesis that the mean accuracy in
the first 16 blocks of phase 2 less than chance. For this
problem, only `prob_x_r_result` will be graded.

## Problem 9
What accounts for the drop in accuracy between phase 1 and
phase 2? To answer this question, investigate the stimuli
the monkey was required to learn between phase 1 and phase 2
by replicating the plot below exactly, and using the
data.table `d` as a the plot data. The ggplot function
`facet_wrap()` will be helpful to you. Save the resulting
plot to a variable named `ans_9`.

```{r, echo=F}
ggplot(d, aes(x, y, colour=factor(cat))) +
  geom_point() +
  facet_wrap(~phase)
```

## problem 10
For the first 16 blocks of phase 1, assuming that you are
performing a one-tailed test, and assuming that the true
state of the universe is $\mu=.6$. Given $H_0:\mu=.5$

* compute $\alpha$ and store its value in a variable named `ans_10a`

* compute $\beta$ and store its value in a variable named `ans_10b`

* compute $power$ and store its value in a variable named `ans_10c`

## Problem 11
The number of action potentials generated from a given
neuron in its baseline state when observed for a fixed
period of time (1 second) is well described by a Poisson
distribution. The Poisson distribution is fully
characterised by a single parameter $\lambda$ called the
rate parameter. It turns out that $\lambda$ is also the mean
of the Poisson distribution. Suppose a group of researchers
measured a particular neuron and observed 5 action
potentials in a given time period. Let $X$ be the Poisson
random variable that generated this result. Test the
hypothesis that $\lambda > 4 events per second$. You will
need to search the web or use the built-in R help to look or
functions that tell you about Poisson probabilities etc. You
ought to find an R function that will do the entre test for
you, and it would be wise to use this function to check your
work, but please perform all 5 steps as indicated in the
general instructions at the top of this homework.

## Problem 12

```{r, echo=F, fig.width=10}
n <- 2

mu_x_0 <- 5
sigma_x <- 2 / sqrt(n)
x_crit <- qnorm(0.95, mu_x_0, sigma_x, lower.tail=F)
mu_x_1 <- x_crit

x <- seq(mu_x_0 - 5*sigma_x, mu_x_1 + 5*sigma_x, 0.01)
fx0 <- dnorm(x, mu_x_0, sigma_x)
fx1 <- dnorm(x, mu_x_1, sigma_x)
d <- data.table(x, fx0, fx1)
d[x <= x_crit, region0 := 'I'] # confidence
d[x > x_crit, region0 := 'II'] # alpha
d[x <= x_crit, region1 := 'III'] # beta
d[x > x_crit, region1 := 'IV'] # power

ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx0)) +
  geom_line(aes(y=fx1)) +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d, aes(x=x, ymin=0, ymax=fx0, fill=region0), alpha=0.25) +
  geom_ribbon(data=d, aes(x=x, ymin=0, ymax=fx0, fill=region0), alpha=0.25) +
  geom_ribbon(data=d, aes(x=x, ymin=0, ymax=fx1, fill=region1), alpha=0.25) +
  geom_ribbon(data=d, aes(x=x, ymin=0, ymax=fx1, fill=region1), alpha=0.25) +
  scale_x_continuous(breaks=c(mu_x_1, mu_x_0), labels=c('H1', 'H0')) +
  ylab('Probability Density') +
  theme(legend.title = element_blank())
```

Please respond by assigning `"confidence"`, `"alpha"`,
`"beta"`, or `"power"` to the variables requested below.
Note that the dashed line in the above figure is the
critical value.

* What quantity does region I in the above plot correspond
to? Store your answer in a variable named `ans_12a`.

* What quantity does region II in the above plot correspond
to? Store your answer in a variable named `ans_12b`.

* What quantity does region III in the above plot correspond
to? Store your answer in a variable named `ans_12c`.

* What quantity does region IV in the above plot correspond
to? Store your answer in a variable named `ans_12d`.

* What is the numeric value (to two decimal places) of the
power in this example? Store your answer in a variable named
`ans_12e`.

* Assuming a type I error rate of 0.05, what is the numeric
value (to two decimal places) of confidence of this example?
Store your answer in a variable named `ans_12f`.

* Assuming that the illustrated distributions correspond to
the distribution of sample means, will increasing the sample
size increase the distance between the mean of H1 and H0
distributions? Store your answer using `ans_12g <- "YES"` or
`ans_12g <- "NO"`.

