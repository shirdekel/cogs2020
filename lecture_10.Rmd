---
title: "Lecture 10 - Repeated measures and mixed-design ANOVA"
author: "Author: Matthew J. Cossley"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float:
        collapsed: false
        smooth_scroll: true
    toc_depth: 2
    fig_caption: yes
    number_sections: false
    theme: cosmo
fontsize: 14pt
editor_options:
  chunk_output_type: console
---

# Repeated measures ANOVA
- A repeated measures design is one in which at least one of
the factors consists of repeated measurements on the same
experiment unit -- this usually corresponds to multiple
measurements from the same subjects.

- It is fair to view this as an extension of the
paired-samples t-test, just as it is fair to view factorial
ANOVA as an extension of the independent samples t-test.

- Advantage: individual differences possibly reduced as a
source of between-group differences.

- Advantage: sample size is not divided between conditions
so can require fewer subjects.

- Disadvantage: fewer subjects means smaller degrees of
freedom (we will see below the relevant $df$ term shrinks
from $n_{observations} - k$ to $(k - 1)(n_{subject} - 1)$.
The more degrees of freedom we have, in general, the less
extreme observed outcome we need to reject the null (because
of its effect on the shape of the sampling distribution of
our test statistic).

## Intuition
- The intuition for a repeated measures ANOVA is the same as
that for a factorial ANOVA.

- E.g., if the population means in the levels of some factor
(e.g., the mean effect of different doses of a medicine) are
different, then between-level variability should be greater
than within-level variability.

- However, the repeated measures aspect introduces one
important difference.

- Between-level variability will inherently be smaller in a
repeated measures design than in an independent samples
design (e.g., because the same subjects give measurements
for each level, and subjects tend to be similar to
themselves).

- This means that, to decide that there are true
differences, we should require less between-level
differences in variability for repeated measures designs
than for independent samples designs.

- Recall that for a factorial ANOVA, the $F$-test that we
use is a ratio of between-level variability to within-level
variability.

  $$F = \frac{MS_{between-levels}}{MS_{within-levels}}$$

- In a repeated measures ANOVA, the $F$-test that we use is
the ratio

  $$F = \frac{MS_{between-levels}}{MS_{within-levels} - MS_{between-subjects}}$$

## Formal treatment

- $k$ is the number of factor levels
- $n$ is the number of subjects
- $x_{ij}$ is observation from factor level $i$ and subject $j$

\begin{align}
SS_{between-levels} &= n \sum_{i=1}^k (\bar{x_{i \bullet}} - \bar{x_{\bullet \bullet}})^2 \\
SS_{within-levels} &= \sum_{i=1}^k \sum_{j=1}^n (x_{ij} - \bar{x_{i \bullet}})^2 \\
SS_{between-subject} &= k \sum_{j=1}^n (\bar{x_{\bullet j}} - \bar{x_{\bullet \bullet}})^2 \\
SS_{error} &= SS_{within-levels} - SS_{between-subject} \\
\end{align}

- The nomenclature $SS_{error}$ will make more sense in the coming lectures.

- This leads to the ANOVA table:

| $Df$       | $SS$      | $MS$                  | $F$                                      | $P(>F)$   |
| :------:   | :------:  | :-------:             | :-------:                                | :-------: |
| k-1        | see above | $SS_{between-levels}$ | $\frac{MS_{between-levels}}{MS_{error}}$ |           |
| (k-1)(n-1) | see above | $SS_{error}$          |                                          |           |

## Repeated measures ANOVA in R 

### toy example
```{r, message=F, warning=F, echo=F}

## important libraries
library(data.table)
library(ggplot2)
library(ez)

## clean slate
rm(list=ls())

## Simulate repeated measures data
n <- 5
k <- 3
set.seed(0)
level1 <- rnorm(n, 10, 1)
level2 <- rnorm(n, 20, 1)
level3 <- rnorm(n, 30, 1)

## build a data.table in a suitable format
d <- data.table(level=factor(rep(1:3, each=n)),
                subject=factor(rep(1:n, 3)),
                score=c(level1, level2, level3))

d
```

* Notice in the above data that each subject gives multiple
measurements (one per factor level).

```{r}
level <- rep(1:3, each=5)
subject <- rep(1:5, 3)
score <- c(11.262954, 9.673767, 11.329799, 11.272429, 10.414641, 
           18.460050, 19.071433, 19.705280, 19.994233, 22.404653, 
           30.763593, 29.200991, 28.852343, 29.710538, 29.700785
)
d <- data.table(level, subject, score)
  
k <- d[, length(unique(level))] # n factor levels
n <- d[, length(unique(subject))] # n subs

## do it by hand
ss_between_levels <- 0
for(i in 1:k) {
  ss_between_levels <- ss_between_levels + 
    (d[level==i, mean(score)] - d[, mean(score)])^2
}
ss_between_levels <- n * ss_between_levels

ss_between_subject <- 0
for(j in 1:n) {
  ss_between_subject <- ss_between_subject + 
    (d[subject==j, mean(score)] - d[, mean(score)])^2
}
ss_between_subject <- k * ss_between_subject

ss_within_levels <- 0
for(i in 1:k) {
  for(j in 1:n) {
    ss_within_levels <- ss_within_levels + 
      (d[level==i & subject==j, score] - d[level==i, mean(score)])^2
  }
}

ss_error <- ss_within_levels - ss_between_subject

df_between_levels <- k - 1
df_error <- (k-1)*(n-1)

ms_between_levels <- ss_between_levels / df_between_levels
ms_error <- ss_error / df_error

fobs <- ms_between_levels / ms_error

p_val <- pf(fobs, df_between_levels, df_error, lower.tail=F)

## Use the function `ezANOVA()` from the `ez` package to
## perform a repeated measures ANOVA
d[, subject := factor(subject)]
d[, level := factor(level)]
ezANOVA(
  data=d, ## where the data is located
  dv=score, ## the dependent variable
  wid=subject, ## the repeated measure indicator column
  within = .(level), ## a list of repeated measures factors
  type = 3 ## type of sums of squares desired
)

```

### Real data
```{r}
## Consider the MIS data
d <- fread('https://crossley.github.io/cogs2020/data/mis/mis_data.csv')

## We will answer this question:

## Are there significant differences in the mean error per
## subject across phases? Note that this question ignores
## differences between conditions

## First, fix the annoying bug that different subjects in
## different groups have the same number.
d[group==1, subject := subject+10]

## compute mean error per subject
dd <- d[order(subject, phase), mean(error, na.rm=TRUE), .(subject, phase)]

## It's important to code factors as factors
dd[, subject := factor(subject)]
dd[, phase := factor(phase)]

## do it by hand
n <- d[, length(unique(subject))]
k <- d[, length(unique(phase))]

ss_between_phases <- 0
for(i in d[, unique(phase)]) {
  ss_between_phases <- ss_between_phases + 
    (dd[phase==i, mean(V1)] - dd[, mean(V1)])^2
}
ss_between_phases <- n * ss_between_phases

ss_between_subject <- 0
for(j in d[, unique(subject)]) {
  ss_between_subject <- ss_between_subject + 
    (dd[subject==j, mean(V1)] - dd[, mean(V1)])^2
}
ss_between_subject <- k * ss_between_subject

ss_within_phases <- 0
for(i in d[, unique(phase)]) {
  for(j in d[, unique(subject)]) {
    ss_within_phases <- ss_within_phases + 
      (dd[phase==i & subject==j, V1] - dd[phase==i, mean(V1)])^2
  }
}

ss_error <- ss_within_phases - ss_between_subject

df_between_phases <- k - 1
df_error <- (k-1)*(n-1)

ms_between_phases <- ss_between_phases / df_between_phases
ms_error <- ss_error / df_error

fobs <- ms_between_phases / ms_error

p_val <- pf(fobs, df_between_levels, df_error, lower.tail=F)

## Do it with ezANOVA()
ezANOVA(
  data=dd,
  dv=V1,
  wid=subject,
  within=.(phase),
  type=3
  )
```

### Making sense of `ezANOVA` output
What is `Mauchly's Test for Sphericity` and `Sphericity
Corrections`? Both have to do with the underlying
assumptions being made by a repeated measures ANOVA. Time
permitting, we will return to this as we review the course
material in preparation for the final exam.

### Quick note on balanced versus unbalanced data
The formulas I wrote in previous sections for computing the
various sums of squares all assumed that we had a perfectly
balanced design. Just as with factorial ANOVA, everything
gets a little wonky with an unbalanced design. The details
of this aren't really suitable for this class. The important
thing to know is that `ezANOVA()` will handle it all for
you.


# Mixed-design ANOVA
We have covered factorial ANOVA (one-way, two-way, etc.) and
repeated measures ANOVA. A **mixed-design ANOVA** is what
you get if some of your experimental factors are derived
from independent samples (e.g., different subjects) and some
are derived from repeated measures (i.e., the same subjects
at different times). Mixed situations are very common, and
are actually characteristic of nearly every set of real data
we have used in this class to date. The good news is that
there isn't really a lot of work to do for us to learn to
perform a mixed-design ANOVA. The maths involved are
understandably more complex than what we have seen in the
previous section but the basic logic and intuitions we have
hopefully developed still holds.

- We are still asking an omnibus question about differences
in means

- Differences in means still comes down to between-level
variation being greater than within-level variation.

- Repeated measures introduces the same familiar problem of
non-independent observations between-levels, and is dealt
with in a similar way to what we saw in the last lecture
(i.e., "correcting" the denominator of the F-ratio).

With that, lets just dive into some practice.

**Important notice about mixed-design ANOVA:**
I will not ask you to compute the relevant sums of squares
by hand for a mixed design ANOVA, but you will need to know
how to use `ezANOVA()` to perform one.

## Toy example
```{r, message=F, warning=F, echo=F}
## important libraries
library(data.table)
library(ggplot2)
library(ez)

## clean slate
rm(list=ls())

## Simulate repeated measures data
n <- 5
k <- 3
set.seed(0)
level1 <- rnorm(n, 10, 1)
level2 <- rnorm(n, 20, 1)
level3 <- rnorm(n, 30, 1)

## build a data.table in a suitable format
d <- data.table(level1=factor(rep(1:3, each=n)),
                subject=factor(rep(1:n, 3)),
                score=c(level1, level2, level3))
d[, level2 := as.integer(subject%in%c(1, 2, 3))]
d <- d[, .(subject, level1, level2, score)]
d
```

```{r}
subject <- rep(1:5, 3)
level1 <- rep(1:3, each=5)
level2 <- rep(c(1, 1, 1, 0, 0), 3)
score <- c(11.262954, 9.673767, 11.329799, 11.272429, 10.414641, 
           18.460050, 19.071433, 19.705280, 19.994233, 22.404653, 
           30.763593, 29.200991, 28.852343, 29.710538, 29.700785 )
d <- data.table(subject, level1, level2, score)

# Be sure to diagnose what factors are repeated
# (within-subject) and which are between-subject.

# same subjects present at each level of level1
# level 1 is a within-subjects factor
d[, unique(subject), .(level1)]

# different subjects present in different levels of level 2
# level 2 is a between-subjects factor
d[, unique(subject), .(level2)]

## Differences between means of level1 and level2 main
## effects or interaction?
d[, subject := factor(subject)]
d[, level1 := factor(level1)]
d[, level2 := factor(level2)]
ezANOVA(
  data=d,
  dv=score,
  wid=subject,
  within=.(level1),
  between=.(level2),
  type=3
)
```

## Real data
```{r}
## important libraries
library(data.table)
library(ggplot2)
library(ez)

## clean slate
rm(list=ls())

d <- fread('https://crossley.github.io/cogs2020/data/mis/mis_data.csv')

## We will answer this question:
## Are there significant differences in the mean error per
## subject across phases and between groups?

## First, fix the annoying bug that different subjects in
## different groups have the same number.
d[group==1, subject := subject+10]

## compute mean error per subject per group
dd <- d[order(subject, phase), mean(error, na.rm=TRUE), .(subject, phase, group)]

## It's important to code factors as factors
dd[, subject := factor(subject)]
dd[, phase := factor(phase)]
dd[, group := factor(group)]

ggplot(dd, aes(group, V1, colour=phase)) +
  geom_boxplot()

## Do it with ezANOVA()
ezANOVA(
  data=dd,
  dv=V1,
  wid=subject,
  within=.(phase),
  between=.(group),
  type=3
)
```