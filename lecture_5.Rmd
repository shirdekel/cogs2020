---
title: "Lecture 5 - Hypothesis testing"
author: "Author: Matthew J. Cossley"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float:
        collapsed: false
        smooth_scroll: true
    toc_depth: 2
    fig_caption: yes
    number_sections: false
    theme: cosmo
fontsize: 14pt
editor_options:
  chunk_output_type: console
---

<!-- TODO: -->

<!-- Change Binom to rephrase H0? -->

<!-- Comment on how a normal distribution is a good model for rat -->
<!-- maze times -->

<!-- Explain / write equations showing how (X - mu ) / sig ~ Z -->
<!-- (this is currently in the t-test section... so just move it -->
<!-- up). -->

<!-- Add comment that the t-distribution in the NHST example has -->
<!-- a normal overlaid as well for comparison. A comment below -->
<!-- says this is in black but I don't think it is. Fix this. -->

```{r setup, include=F, message=F}
knitr::opts_chunk$set(echo = T, collapse=T)
```

```{r, echo=F}
library(data.table)
library(ggplot2)
library(ggpubr)
rm(list=ls())
```

## Learning objectives

* Define and understand null hypothesis significance testing.

* Define and understand **p-value**.

* Define and understand **more extreme outcomes** in the context
  of hypothesis testing.

* Define and understand **critical value**.

* Define and understand **rejection region**.

<!-- * Define and understand **$1-\alpha\%$ confidence interval**. -->

* Be able to perform hypothesis binomial tests and t-tests
  manually (i.e., run through the 5 steps).

* Understand when and how to use `binom.test()`.

* Understand when and how to use `t.test()`.

## Null Hypothesis Significance Testing

We will unpack each of these 5 steps in the examples that
follow. They are listed here for reference.

1. Specify the null and alternative hypotheses ($H_0$ and
   $H_1$) in terms of a population parameter $\theta$.

2. Specify the type I error rate -- denoted by the symbol
   $\alpha$ -- you are willing to tolerate.

3. Specify the sample statistic $\widehat{\theta}$ that you will use to
   estimate the population parameter $\theta$ in step 1 and state how
   it is distributed under the assumption that $H_0$ is
   true.

4. Obtain a random sample and use it to compute the sample
   statistic from step 3. Call this value
   $\widehat{\theta}_{\text{obs}}$.

5. If $\widehat{\theta}_{\text{obs}}$ or a **more extreme outcome 
   is very unlikely to occur under the assumption that $H_0$ is true, then
   reject $H_0$. Otherwise, do not reject $H_0$.

## Binomial Test

Consider an experiment in which researchers are trying to
determine if a particular rat has learned to press one of
two levers whenever they are placed inside of an
experimental apparatus. Intuitively, answering this
questions would be as simple as just watching the rat and
taking note of whether it pressed the correct lever or not.
The trouble is that rat behaviour is somewhat random: it
sometimes pressed the correct lever and sometimes presses
the incorrect lever.

Suppose that the experiment contained $n=100$
trials and the number of trials in which the rat pressed the
lever was $n_{\text{pressed}}=61$. Did the rat learn or not?
We turn to **Null Hypothesis Significance Testing (NHST)**
to answer this question.

<h4>1\. Specify the null and alternative hypotheses ($H_0$ and
   $H_1$) in terms of a distribution and population
   parameter.</h4>

First, decide on a statistical model for the measurements
you are trying to reason about. That is, think of your data
as being samples from a random variable. What distribution
does that random variable have? We have a rat that either
presses the correct lever or doesn't. This is a dichotomous
outcome, so we know that Binomial distribution is a good
model. Recall that the binomial has two parameters, (1) the
number of trials $n$ and (2) the probability of success $p$.
The number of trials is fixed for us by our experiment
($n=100$), and $p$ is the very thing we are trying to reason
about. If $p=0.5$ then the rat is just guessing at the
levers. This corresponds to a null result, and therefore
to the null hypothesis $H_0$. If $p>0.5$ then the rat is doing better than
guessing and we would say that it has learned. Therefore, 
this corresponds to the alternative hypothesis $H_1$.
Let $X$ be the binomial random variable that generates
the results in the above experiment. Then, 

$$
X \sim binomial(n=100, p) \\
H_0: p = 0.5 \\
H_1: p > 0.5
$$

<h4>2\. Specify the type I error rate -- denoted by the symbol
   $\alpha$ -- you are willing to tolerate.</h4>

It is traditional in psychology and neuroscience to tolerate
a $5\%$ type I error rate in our inference procedure. This
means that 5 out of every 100 times we think the rat has
learned, we will have made an incorrect conclusion.

$$
\alpha=0.05
$$

<h4>3\. Specify the sample statistic that you will use to
   estimate the population parameter in step 1 and state how
   it is distributed under the assumption that $H_0$ is
   true.</h4>

We are trying to make an inference about the population
parameter $p$. An intuitive estimate of this parameter is 
the proportion of successful trials relative to all trials 
performed. Let $x$ be the be the count of successful trials.
Then, 

$$
\widehat{p}=\frac{x}{n}
$$

We know that $X$ is Binomial, but what about $\widehat{p}$?
With some careful thinking, we can see that $\widehat{p}$ is
also Binomial. The possible outcomes of $X$ just need to be
normalised by the total number of observations $n$. The
probabilities associated with each outcome are the same
as those that corresponded to the $X$ distribution. 
Another approach (not taken here) is to
rephrase the hypotheses to be in terms of counts instead of
proportions (e.g., $H_0: np = 50; H_0: np > 50$). 
The **sampling distribution** of $\widehat{p}$ under the null 
is as follows:

$$
\widehat{p} = \frac{X}{n} \sim binomial(n=100, p=0.5), x \rightarrow \frac{x}{n}
$$

```{r, echo=F}
n <- 100
p <- 0.5
x <- 0:n
fx <- dbinom(x, n, p)
x <- x/n
xobs <- 61/n
d <- data.table(x,
                fx,
                region=factor(x>=xobs, labels=c('x < xobs', 'x >= xobs')))
ggplot(d, aes(x, fx, colour=region)) +
  geom_point() +
  geom_segment(aes(x=xobs, xend=xobs, y=0, yend=d[x==xobs, fx]),
               colour='black',
               linetype=2) +
  annotate('text', x=xobs+0.2, y=d[x==xobs, fx],
           label='xobs or more extreme',
           colour='#00AFBB') +
  scale_x_continuous(breaks=seq(0,n,10)/n) +
  ylab('P(X=x)') +
  xlab('x_successes / n_total') +
  theme(legend.title = element_blank())
```

<h4>4\. Obtain a random sample and use it to compute the
   sample statistic from step 3. Call this value
   $\widehat{\theta}_{\text{obs}}$</h4>

This was given to us in the formulation of the example. The
key is to understand that in this example,
$\widehat{\theta}_{\text{obs}} = \widehat{p}_{\text{obs}}$.

$$
\widehat{p}_{\text{obs}}=\frac{61}{100}
$$

<h4>5\. If $\widehat{\theta}_{\text{obs}}$ is very unlikely to
   occur under the assumption that $H_0$ is true, then
   reject $H_0$. Otherwise, do not reject $H_0$.</h4>

Compute the probability of $\widehat{\theta}_{\text{obs}}$
**or a more extreme outcome** occurring under the
assumption that $H_0$ is true. This value is called the
**p-value** in NHST. If the p-value is very small (less than \alpha) 
then reject $H_0$. Otherwise, fail to reject $H_0$. Formally, we write
our decision rule as follows:

$$
\text{if } P(\widehat{\theta} \geq \widehat{\theta}_{\text{obs}} | H_0) \leq \alpha \rightarrow \text{Reject } H_0\\
\text{otherwise fail to reject } H_0
$$

**Notice that our logic only allows to reject or fail to
reject the null hypothesis.** We can't make any inference
about the alternative hypothesis (e.g., we can't accept the
alternative). We are only assessing the evidence for the
null. The purpose of including an alternative hypothesis is
to give meaning to what sorts of outcomes correspond to more
extreme than what we observed.

To proceed, note again that in this example 
$\widehat{\theta}_{\text{obs}} = \widehat{p}_{\text{obs}}$.
We therefore need to compute 

$$
P(\widehat{p} \geq \widehat{p}_{\text{obs}} | H_0)
$$

When actually computing the p-value, we will turn to
`pbinom()`. From the plot above, and from reasoning about
the alternative hypothesis, we see that we need
`lower.tail=FALSE`.

```{r}
n <- 100
xobs <- 61 # observed count
phatobs <- xobs / n # observed proporion

## `xobs-1` because `lower.tail=FALSE` give P(X > x)
## but we want P(X >= x)
pval <- pbinom(xobs-1, n, p, lower.tail=FALSE)
pval

## Notice that in the above we used `xobs` and not `phatobs`.
## This is becuase `pbinom()` needs counts, not proportions 
## to functino properly. The probability it returns for `xobs`
## will perfectly correspond to the probabiliy of `phatobs` 
## because of the notes discussed above in step 3
```

The decision rule above can also be expressed in terms of a
**critical value**. The critical value $\widehat{\theta}_{\text{crit}}$ in NHST is the outcome
such that $P(\widehat{\theta} \geq \widehat{\theta}_{\text{crit}} | H_0) \leq \alpha$.

As such, any observed outcome equal to the critical value or
more extreme than the critical value will lead to the
rejection of the null. For this reason, values more extreme
than the critical value are said reside in the **rejection
region**.

To obtain critical values, we need to get an outcome that
corresponds to a specific probability. This is given to us
by the `qbinom()` function, again with `lower.tail=FALSE`.

```{r}
n <- 100
p <- 0.5
xcrit <- qbinom(0.05, n, p, lower.tail=FALSE)

## Have to divide by n because we are working with
## proportions not counts
phatcrit <- xcrit / n

xcrit
phatcrit
```

```{r, echo=F}
d <- data.table(x,
                fx,
                pregion=factor(x>=phatcrit, labels=c('x < xobs', 'x >= xobs')),
                rregion=factor(x>=phatcrit, labels=c('x < xcrit', 'x >= xcrit'))
                )
ggplot(d, aes(x, fx, colour=rregion)) +
  geom_point() +
  geom_segment(aes(x=phatobs, xend=phatobs, y=0, yend=d[x==phatobs, fx]),
               colour='black',
               linetype=2) +
  geom_segment(aes(x=phatcrit, xend=phatcrit, y=0, yend=d[x==phatcrit, fx]),
               linetype=2) +
  annotate('text', x=phatobs+0.01, y=d[x==phatobs, fx],
           label='observed outcome',
           colour='black',
           hjust=0) +
  annotate('text', x=phatcrit+0.01, y=d[x==phatcrit, fx],
           label='critical value (rejection region)',
           colour='#00AFBB',
           hjust=0) +
  scale_x_continuous(breaks=seq(0,n,10)/n) +
  ylab('P(X=x)') +
  xlab('x_successes / n_total') +
  theme(legend.title = element_blank())
```

<!-- Yet another way of stating the decision rule above is -->
<!-- through the use of **95% confidence intervals**. A -->
<!-- confidence interval is a range of outcomes within which the -->
<!-- probability that the true parameter value is within the -->
<!-- interval is 95%. If the null falls outside of a test 95% -->
<!-- confidence interval, then we reject the null. Learning how -->
<!-- to compute confidence intervals for a binomial test is a bit -->
<!-- beyond the scope of this unit, but later on we will learn -->
<!-- how to compute them for other sampling distributions. For -->
<!-- now, it is enough to simply be aware that they exist and -->
<!-- have a vague understanding of what they are and how to -->
<!-- use them. A full treatment of confidence intervals will -->
<!-- come in a later lecture. -->

The five steps outlined above are the core of
null-hypothesis significance testing, and working through
each step in the longhand format that we just did is
important to be able to do. This is because these five steps
are completely general and will perfectly apply to any
situation you might find yourself in. That said, in many
situations, `R` has built-in functions to handle all five
steps in one line of code. When making inferences from a
binomial distribution, you can use the `binom.test()`
function.

```{r}
xobs <- 61       ## number of observed successes
n <- 100         ## n parameter of H0
p <- 0.5         ## p parameter of H0
alpha <- 0.05    ## error rate tolerance
alt <- 'greater' ## test direction

## Use built in `binom.test()`
binom.test(xobs,
           n,
           p,
           alternative = alt,
           conf.level = 1 - alpha
           )
```

## Normal test

Consider an experiment in which a rat is placed into a maze
and given the chance to search for a bit of cheese hidden
somewhere in the maze. After much training, the researchers
are interested in assessing whether or not the animal has
learned where the cheese is hidden. The researchers also
know that rats without any training whatsoever find the
cheese on average in 90 seconds with a standard deviation
of 20 seconds. They perform 15 trials and measure the time
to cheese on each trial. The data are as follows:

```{r, echo=F}
set.seed(0)
n <- 15
mu_x <- 80
sig_x <- 20
x_obs <- rnorm(n, mu_x, sig_x)
d <- data.table(x_obs)
ggplot(d, aes(x_obs)) +
  geom_histogram(bins=6)
```

<h4>1\. Specify the null and alternative hypotheses ($H_0$ and
   $H_1$) in terms of a distribution and population
   parameter.</h4>

If the rat has learned something about where to find the cheese,
then we expect it's time to be less than that of naive rats,
which we are told is 90 seconds. This leads to the following hypotheses.

$$
H_0: \mu = 90 \\
H_1: \mu < 90
$$

<h4>2\. Specify the type I error rate -- denoted by the symbol
   $\alpha$ -- you are willing to tolerate.</h4>

$$
\alpha = 0.05
$$

<h4>3\. Specify the sample statistic that you will use to
   estimate the population parameter in step 1 and state how
   it is distributed under the assumption that $H_0$ is
   true.</h4>

$$
\widehat{\mu} = \bar{x} \\
\bar{x} \sim \mathcal{N}(\mu_{\bar{x}}, \sigma_{\bar{x}})
$$

Since we are the sample mean $\bar{x}$ to estimate $\mu$,
the sampling distribution of our test statistic is the
distribution of sample means. This is great news because
we know from previous lectures how the mean and variance
of the distribution of sample means $\bar{x}$ relates to
the mean and variance of our origin distribution $x$. In
particular, we know:

$$
\mu_{\bar{x}} = \mu_{x} \\
\sigma_{\bar{x}} = \frac{\sigma_{x}}{\sqrt{n}}
$$

We can now inspect the sampling distribution
under the assumption that $H_0$ is true, and inspect how likely the observed 
$\bar{x}$ is to be sampled from this distribution.

```{r, echo=F, fig.width=10}
x_bar_obs <- mean(x_obs)
mu_x_bar <- 90
sig_x_bar <- sig_x / sqrt(n)
x_bar_crit <- qnorm(0.05, mu_x_bar, sig_x_bar, lower.tail=TRUE)
x <- seq(mu_x_bar-4*sig_x_bar, mu_x_bar+4*sig_x_bar, 0.01)
fx <- dnorm(x, mu_x_bar, sig_x_bar)
d <- data.table(x,
                fx,
                region=factor(x<=x_bar_obs, labels=c('x > x_bar_obs', 'x <= x_bar_obs')),
                cregion=factor(x<=x_bar_crit, labels=c('x > x_bar_crit', 'x <= x_bar_crit')))

g1 <- ggplot(d, aes(x, fx, colour=region)) +
  geom_line() +
  geom_vline(xintercept=x_bar_obs, colour='black', linetype=2) +
  geom_ribbon(aes(ymin=0, ymax=dnorm(x, mu_x_bar, sig_x_bar), fill=region), alpha=0.5) +
  annotate('text', x=x_bar_obs-0.1, y=dnorm(x_bar_obs, mu_x_bar, sig_x_bar),
           label='x_obs or \n more extreme',
           colour='#00AFBB',
           hjust=1) +
  ylab('f(xbar)') +
  xlab('xbar') +
  ggtitle('p-value approach') +
  theme(legend.position = "none")

g2 <- ggplot(d, aes(x, fx, colour=cregion)) +
  geom_line() +
  geom_vline(xintercept=x_bar_obs, colour='black', linetype=2) +
  geom_vline(xintercept=x_bar_crit, colour='black', linetype=2) +
  geom_ribbon(aes(ymin=0, ymax=dnorm(x, mu_x_bar, sig_x_bar), fill=cregion), alpha=0.5) +
  annotate('text', x=x_bar_crit-0.1, y=dnorm(x_bar_crit, mu_x_bar, sig_x_bar),
           label='critical value\n(rejection region)',
           colour='#00AFBB',
           hjust=1) +
  annotate('text', x=x_bar_obs+0.1, y=dnorm(x_bar_obs, mu_x_bar, sig_x_bar),
           label='observed outcome',
           colour='black',
           hjust=0) +
  ylab('f(xbar)') +
  xlab('xbar') +
  ggtitle('Critical value approach') +
  theme(legend.position = "none")

ggarrange(g1, g2, ncol=2)
```

<h4>4\. Obtain a random sample and use it to compute the sample
   statistic from step 3. Call this value
   $\widehat{\theta}_{\text{obs}}$</h4>

In this example, $\widehat{\theta}_{\text{obs}}$ corresponds to the
observed sample mean. The data, sampled from the random variable $X$ is 
given by the following times:

```{r} 
x_obs
```

Thus, the observed sample mean $\bar{x}_{\text{obs}}$, sampled from
the random variable $\bar{X}$, is obtained as follows:

```{r} 
x_bar_obs <- mean(x_obs)
```

* Therefore, $\bar{x}_{\text{obs}}=$ `r x_bar_obs`

<h4>5\. If $\widehat{\theta}_{\text{obs}}$ is very unlikely to
   occur under the assumption that $H_0$ is true, then
   reject $H_0$. Otherwise, do not reject $H_0$.</h4>

When computing the p-value, we will turn to `pnorm()`. From the
plot above, and from reasoning about the alternative hypothesis,
we see that we need `lower.tail=TRUE`.

```{r}
mu_x <- 90
sig_x <- 20

mu_x_bar <- mu_x
sig_x_bar <- sig_x / sqrt(n)

## p-value
pval <- pnorm(x_bar_obs, mu_x_bar, sig_x_bar, lower.tail=TRUE)
pval

## critical value
x_bar_crit <- qnorm(0.05, mu_x_bar, sig_x_bar, lower.tail=TRUE)
x_bar_crit
```

It is easy to decide whether or not to reject $H_0$ based on
the p-value or the critical region, but it sure would be nice
if R gave us a one liner like `binom.test()`. Unfortunately,
in the case of a normal sampling distribution, no such R function
exists. The reason for this is that to have a normal $\bar{x}$
sampling distribution, you have to know both the mean and the variance
of the $H_0$ distribution. The mean is specified by $H_0$ so is
no issue, but we rarely are in a situation to know the population
variance of X, and we therefore have to estimate it. This leads
us to the famous t-test.

## The z-test

- Why is the $Z$ distribution so important?

- To be candid, for the privileged `R` users of the world,
it really isn't that essential to our day to day statistical
adventures.

- This is because, if we want to know what $P(X < x)$ for $X
\sim N(\mu_X, \sigma_x)$ then we just type `pnorm(x, mu_x,
sigma_x, lower.tail=TRUE)` and let R do all the hard work.

- But what is R doing under the hood?

- Recall that $P(X < x)$ for $X \sim N(\mu_X, \sigma_x)$
corresponds to the area under the probability density
function in the interval $[-\infty, x]$.

- Further recall that the equation that describes the PDF of
a normal distribution is given by:

$$
f(x) = \frac{1}{\sigma_X \sqrt(2 \pi)} e^{ -\frac{1}{2} (\frac{x - \mu_X}{\sigma_X})^2 }
$$

- This means that $P(X < x)$ is given by:

$$
P(X < a) = \int_{\infty}^{a}
\frac{1}{\sigma_X \sqrt(2 \pi)} e^{ -\frac{1}{2} (\frac{x - \mu_X}{\sigma_X})^2 }
$$

- It turns out that without awesome computer programs like
R, evaluating this integral is challenging.

- The old school solution to this challenge was to evaluate
this integral for the standard normal ($Z$) and put that
solution in the back of standard statistics text books in
the form of big giant tables.

- The $Z$ distribution has other uses in statistics and
machine learning, but most of them are beyond the scope of
this unit.

- We will see that the overarching idea of the z-transform
-- i.e., standardising data to mean zero and variance one --
comes up when we get to the t-test (later this lecture!).

```{r}
# Using Normal distribution
x_bar_obs <- mean(x_obs)
sig_x_bar <- sig_x / sqrt(n)
px <- pnorm(x_bar_obs, mu_x_bar, sig_x_bar, lower.tail=TRUE)

## Using Z distribution
z_obs <- (x_bar_obs - 90) / sig_x_bar
pz <- pnorm(z_obs, 0, 1, lower.tail=TRUE)

## Compare p-values from different methods
px
pz
```

- How are the p-values the same?

- The plot below is the X distribution on the left and the Z
distribution on the right. - Notice that the Z is just a
scaled version of X, and the that scaling also applied to
the observed value.

- This means that all probabilities (area under the curve)
are the same.
```{r, echo=F, fig.width=10}
## How are the p-values the same?
x <- seq(90 - 4*sig_x_bar, 90 + 4*sig_x_bar, 0.001)
fx <- dnorm(x, 90, sig_x_bar)

z <- seq(0 - 4*1, 0 + 4*1, 0.001)
fz <- dnorm(z, 0, 1)

d <- data.table(
  x=c(x, z),
  fx=c(fx, fz),
  rv=rep(c('x', 'z'), c(length(x), length(z))),
  obs=rep(c(x_bar_obs, z_obs), c(length(x), length(z)))
)

## We can see that the observed values and full
## distributions get transformed in proportion, so we get
## the same p-values
ggplot(data=d, aes(x, fx)) +
  geom_line() +
  geom_vline(aes(xintercept=obs), linetype=2) +
  facet_wrap(~rv, scales='free')

```

## t-test

So far, everything that we have done we have been lucky
enough to know both the mean and the variance of the
sampling distribution in our hypothesis tests. The mean is
specified in $H_0$ and $H_1$ and the variance has either
fallen out luckily (e.g., as with the Binomial test), or I
have just given you a number and told you pretend that we
just know it to be true (e.g., previous cheese maze
example). Of course, in most real world scenarios, we will
not know the variance of the sampling distribution, and this
means that the approaches we have developed so far aren't
quite appropriate. Here is what we do instead:

Let $X_1, X_2, \ldots, X_n$ be independent and identically
distributed as

$$
X_i \sim N(\mu_X, \sigma_X)
$$

and define two random variables $\bar{X}$ and $S^2$ as

$$
\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i
$$

$$
S^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2
$$

then the random variable

$$
\frac{\bar{X} - \mu_X}{\frac{\sigma_X}{\sqrt{n}}} \sim N(0, 1) = Z
$$

and

$$
\frac{\bar{X} - \mu_X}{\frac{S}{\sqrt{n}}} \sim t(n-1)
$$

where $t$ is a t-distribution, which is completely defined
by one parameter called the *degrees of freedom* given by
$n-1$. This all means that the mathematical formulation for
how our sampling distribution is defined is different
depending on whether or not we know $\sigma_X$.
Lets examine how this pans out using our previous cheese
example, but without assuming known variance.

<h4>1\. Specify the null and alternative hypotheses ($H_0$ and
   $H_1$) in terms of a distribution and population
   parameter.</h4>

$$
H_0: \mu = 90 \\
H_1: \mu < 90
$$

<h4>2\. Specify the type I error rate -- denoted by the symbol
   $\alpha$ -- you are willing to tolerate.</h4>

$$
\alpha = 0.05
$$

<h4>3\. Specify the sample statistic that you will use to
   estimate the population parameter in step 1 and state how
   it is distributed under the assumption that $H_0$ is
   true.</h4>

In this example we do not know $\sigma_{x}$, and so we
must estimate it. This means that we do not want to reason using
the observed $\bar{x}$ value and corresponding sampling distribution,
but instead want to reason using an observed $t$ value and corresponding
t-distribution.

$$
t_{obs} = \frac{\bar{x} - \mu_x}{\frac{s_x}{\sqrt{n}}} \sim t(n-1)
$$

```{r, echo=F, fig.width=10}
t_obs <- (mean(x_obs)-90)/(sd(x_obs)/sqrt(n))
t_crit <- qt(0.05, n-1, lower.tail=TRUE)
mu <- 0
t <- seq(-4, 4, 0.01)
ft <- dt(t, n-1)

x_bar_obs <- mean(x_obs)
muxbar <- 90
sig_x_bar <- sig_x / sqrt(n)
z_obs <- (x_bar_obs - 90) / sig_x_bar
z <- seq(-4, 4, 0.01)
fz <- dnorm(z, 0, 1)

d <- data.table(t,
                ft,
                z, 
                fz,
                region=factor(t<=t_obs, labels=c('t > t_obs', 't <= t_obs')),
                cregion=factor(t<=t_crit, labels=c('t > t_crit', 't <= t_crit')))

g1 <- ggplot(d, aes(t, ft, colour=region)) +
  geom_line() +
  geom_vline(xintercept=t_obs, colour='black', linetype=2) +
  geom_ribbon(aes(ymin=0, ymax=dnorm(t, 0, 1), fill=region), 
              colour='black', alpha=0.5) +
  annotate('text', x=t_obs-0.1, y=dnorm(t_obs, 0, 1),
           label='t_obs or \n more extreme',
           colour='#00AFBB',
           hjust=1) +
  ylab('f(t)') +
  xlab('t') +
  ggtitle('p-value approach') +
  theme(legend.position = "none")

g2 <- ggplot(d, aes(t, ft, colour=cregion)) +
  geom_line() +
  geom_vline(xintercept=t_obs, colour='black', linetype=2) +
  geom_vline(xintercept=t_crit, colour='black', linetype=2) +
  geom_ribbon(aes(ymin=0, ymax=dnorm(t, 0, 1), fill=cregion), 
              colour='black', alpha=0.5) +
  annotate('text', x=t_crit-0.1, y=dnorm(t_crit, 0, 1),
           label='critical value\n(rejection region)',
           colour='#00AFBB',
           hjust=1) +
  annotate('text', x=t_obs+0.1, y=dnorm(t_obs, 0, 1),
           label='t observed',
           colour='black',
           hjust=0) +
  ylab('f(t)') +
  xlab('t') +
  ggtitle('Critical value approach') +
  theme(legend.position = "none")

ggarrange(g1, g2, ncol=2)
```

<h4>4\. Obtain a random sample and use it to compute the sample
   statistic from step 3. Call this value
   $\widehat{\theta}_{\text{obs}}$</h4>

For our data, the following is true:

* $n=15$,
* $\bar{x} =$ `r mean(x_obs)`,
* $s_x =$ `r sd(x_obs)`
* $t_{obs} =$ `r (mean(x_obs)-90)/(sd(x_obs)/sqrt(n))`.

The above plot shows the $t(n-1)$ sampling distribution in
colour, and the $Z\sim\mathcal{N}(0,1)$ in black. The $t$
has higher tails than the $Z$. This is because the t-value
is the result of two random variables (sample mean and
sample variance), while the z-value is only a product of
only one random variable (the sample mean). However, it is
easy to see that the difference between $t$ and $Z$ is
reduces as $n$ increases.

```{r, echo=F}
z <- seq(-5, 5, 0.001)
fz <- dnorm(z, 0, 1)
ft1 <- dt(z, 1)
ft3 <- dt(z, 3)
ft10 <- dt(z, 10)

d <- data.table(z, fz, ft1, ft3, ft10)

dd <- melt(d, measure.vars=c('fz', 'ft1', 'ft3', 'ft10'))

ggplot(dd, aes(z, value, colour=variable)) +
  geom_line() +
  xlab('x') +
  ylab('f(x)')
```

<h4>5\. If $\widehat{\theta}_{\text{obs}}$ is very unlikely to
   occur under the assumption that $H_0$ is true, then
   reject $H_0$. Otherwise, do not reject $H_0$.</h4>

When computing the p-value, we will turn to `pt()`. From the
plot above, and from reasoning about the alternative hypothesis,
we see that we need `lower.tail=TRUE`.

```{r}
## p-value
p_val <- pt(t_obs, n-1, lower.tail=TRUE)
p_val

## critical value
t_crit <- qt(0.05, n-1, lower.tail=TRUE)
t_crit
```

Finally, there is a built in function called `t.test()` that
will do all of this for you.
```{r}
t.test(x_obs, mu=90, alternative='less')
```

## Type I versus Type II errors

### Definitions

- Hypothesis testing is about using noisy data to make decisions about what we
  think the true state of the universe is.

- Sometimes, our procedure for making these decisions will lead us to make the
  correct decision, but sometimes we will make the wrong decision.

- In the context of hypothesis testing, there are two ways to make a correct
  decision and two ways to make an incorrect decision. These are summarised in
  the following table.

|                   | H0 True                 | H1 True                 |
|:------------------|:------------------------|:------------------------|
| Reject H0         | Type I error ($\alpha$) | Power ($1-\beta$)        |
| Fail to reject H0 | Confidence ($1-\alpha$) | Type II error ($\beta$) |


- Here, we have introduced the concept of **power**.

- Power is the probability of correctly rejecting $H_0$.

- Power can only be computed with respect to some fully specified $H_1$.

- This is best seen with a visualisation.

```{r, echo=F}
mu_x_0 <- 2
mu_x_1 <- 7

sigma_x <- 2

n <- 10
x_crit <- qnorm(0.95, mu_x_0, sigma_x, lower.tail=TRUE)

x <- seq(mu_x_0 - 5*sigma_x, mu_x_1 + 5*sigma_x, 0.01)
fx0 <- dnorm(x, mu_x_0, sigma_x)
fx1 <- dnorm(x, mu_x_1, sigma_x)

d <- data.table(x, fx0, fx1)

g1 <- ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx0), colour='red') +
  geom_line(aes(y=fx1), colour='blue') +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d[x<x_crit], aes(x=x, ymin=0, ymax=fx0), fill='red', alpha=0.5) +
  scale_x_continuous(breaks=c(mu_x_0, mu_x_1)) +
  ylab('Probability Density') +
  ggtitle(expression(1-alpha)) +
  theme(plot.title = element_text(size=36))

g2 <- ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx0), colour='red') +
  geom_line(aes(y=fx1), colour='blue') +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d[x>x_crit], aes(x=x, ymin=0, ymax=fx0), fill='red', alpha=0.5) +
  scale_x_continuous(breaks=c(mu_x_0, mu_x_1)) +
  ylab('Probability Density') +
  ggtitle(expression(alpha)) +
  theme(plot.title = element_text(size=36))


g3 <- ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx0), colour='red') +
  geom_line(aes(y=fx1), colour='blue') +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d[x<x_crit], aes(x=x, ymin=0, ymax=fx1), fill='blue', alpha=0.5) +
  scale_x_continuous(breaks=c(mu_x_0, mu_x_1)) +
  ylab('Probability Density') +
  ggtitle(expression(beta)) +
  theme(plot.title = element_text(size=36))


g4 <- ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx0), colour='red') +
  geom_line(aes(y=fx1), colour='blue') +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d[x>x_crit], aes(x=x, ymin=0, ymax=fx1), fill='blue', alpha=0.5) +
  scale_x_continuous(breaks=c(mu_x_0, mu_x_1)) +
  ylab('Probability Density') +
  ggtitle(expression(1 - beta)) +
  theme(plot.title = element_text(size=36))

## ggarrange(g2, g4, g1, g3, ncol=2, nrow=2)
```
<div class = "row">
<div class = "col-md-6">

```{r echo=F}
g2
```

- Type I error is given by $\alpha$
- The probability of a type I error is given by the area under the $H_0$ curve
  in the rejection region.
- A type I error is the probability of incorrectly rejecting $H_0$
- Type I error is completely determined by $H_0$
- Type I error does not depend on $H_1$

</div>
<div class = "col-md-6">

```{r echo=F}
g4
```

- Power is given by $1 - \beta$
- Power is given by the area under the $H_1$ curve in the rejection region.
- Power can depends on both $H_0$ and $H_1$
-
-

</div>
<div class = "col-md-6">

```{r echo=F}
g1
```

- Confidence is given by $1 - \alpha$
- Confidence is given by the area under the $H_0$ curve outside the rejection region.
- Confidence depends only on $H_0$
-
-
-

</div>
<div class = "col-md-6">

```{r echo=F}
g3
```

- Type II error is given by $\beta$
- The probability of a type II error is given by the area under the $H_1$ curve
  outside the rejection region.
- A type II error is the probability of incorrectly failing to reject $H_0$
- Type II error depends on both $H_0$ and $H1$
-


</div>
</div>

## power

```{r, echo=F}
mu_x_0 <- 2
mu_x_1 <- 7

sigma_x <- 2

n <- 10
x_crit <- qnorm(0.95, mu_x_0, sigma_x, lower.tail=TRUE)

x <- seq(mu_x_0 - 5*sigma_x, mu_x_1 + 5*sigma_x, 0.01)
fx0 <- dnorm(x, mu_x_0, sigma_x)
fx1 <- dnorm(x, mu_x_1, sigma_x)

d <- data.table(x, fx0, fx1)

ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx0), colour='red') +
  geom_line(aes(y=fx1), colour='blue') +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d[x>x_crit], aes(x=x, ymin=0, ymax=fx1), fill='blue', alpha=0.5) +
  scale_x_continuous(breaks=c(mu_x_0, mu_x_1)) +
  ylab('Probability Density') +
  ggtitle(expression(1-beta)) +
  theme(plot.title = element_text(size=36))
```

- In general, we want as much power as possible.
- This is because, if $H_0$ isn't true, then we'd really like to reject it.
- How can we increase power?

### Increase the distance between $H_0$ and $H_1$

```{r, echo=F}
mu_x_0 <- 2
mu_x_1 <- c(7, 10)

sigma_x <- 2

n <- 10
x_crit <- qnorm(0.95, mu_x_0, sigma_x, lower.tail=TRUE)

x <- seq(mu_x_0 - 4*sigma_x, mu_x_1[2] + 4*sigma_x, 0.01)
fx0 <- dnorm(x, mu_x_0, sigma_x)
fx1 <- dnorm(x, mu_x_1[1], sigma_x)
fx2 <- dnorm(x, mu_x_1[2], sigma_x)

d <- data.table(x, fx0, fx1, fx2)

g1 <- ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx0), colour='red') +
  geom_line(aes(y=fx1), colour='blue') +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d[x>x_crit], aes(x=x, ymin=0, ymax=fx1), fill='blue', alpha=0.5) +
  scale_x_continuous(breaks=c(mu_x_0, mu_x_1)) +
  ylab('Probability Density') +
  ggtitle(expression(1-beta)) +
  theme(
    aspect.ratio=0.85,
    plot.title = element_text(size=30))

g2 <- ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx0), colour='red') +
  geom_line(aes(y=fx2), colour='blue') +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d[x>x_crit], aes(x=x, ymin=0, ymax=fx2), fill='blue', alpha=0.5) +
  scale_x_continuous(breaks=c(mu_x_0, mu_x_1)) +
  ylab('Probability Density') +
  ggtitle(expression(1-beta)) +
  theme(
    aspect.ratio=0.85,
    plot.title = element_text(size=30))

ggarrange(g1, g2, ncol=2)
```

- The closer together the $H_0$ and $H_1$ distributions, the less power we have.


### Decrease the variance of $H_0$ and $H_1$

```{r, echo=F}
mu_x_0 <- 2
mu_x_1 <- 10

sigma_x <- c(2, 20)

n <- 10
x_crit_1 <- qnorm(0.95, mu_x_0, sigma_x[1], lower.tail=TRUE)
x_crit_2 <- qnorm(0.95, mu_x_0, sigma_x[2], lower.tail=TRUE)

x <- seq(mu_x_0 - 4*sigma_x[2], mu_x_1 + 4*sigma_x[2], 0.01)
fx01 <- dnorm(x, mu_x_0, sigma_x[1])
fx11 <- dnorm(x, mu_x_1, sigma_x[1])
fx02 <- dnorm(x, mu_x_0, sigma_x[2])
fx12 <- dnorm(x, mu_x_1, sigma_x[2])


d <- data.table(x, fx01, fx11, fx02, fx12)

g1 <- ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx01), colour='red') +
  geom_line(aes(y=fx11), colour='blue') +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d[x>x_crit], aes(x=x, ymin=0, ymax=fx11), fill='blue', alpha=0.5) +
  scale_x_continuous(breaks=c(mu_x_0, mu_x_1)) +
  ylab('Probability Density') +
  ylim(0, 0.2) +
  ggtitle(expression(1-beta)) +
  theme(
    aspect.ratio=0.85,
    plot.title = element_text(size=30))

g2 <- ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx02), colour='red') +
  geom_line(aes(y=fx12), colour='blue') +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d[x>x_crit], aes(x=x, ymin=0, ymax=fx12), fill='blue', alpha=0.5) +
  scale_x_continuous(breaks=c(mu_x_0, mu_x_1)) +
  ylab('Probability Density') +
  ylim(0, 0.2) +
  ggtitle(expression(1-beta)) +
  theme(
    aspect.ratio=0.85,
    plot.title = element_text(size=30))

ggarrange(g1, g2, ncol=2)
```

- The smaller the variance of $H_0$ and $H_1$, the greater power we have


### Power as a function of sample size

- The key point here is that, when dealing with the distribution of sample
  means, variance is inversely proportional to sample size.

- $\sigma_\bar{X} = \frac{\sigma_X}{\sqrt{n}}$

- This means that power increases with increased sample size.

```{r, echo=F}
mu_x_0 <- 20
mu_x_1 <- 70

sigma_x <- 30

n <- 1:10

mu_x_bar_0 <- mu_x_0
mu_x_bar_1 <- mu_x_1

sigma_x_bar <- sigma_x / sqrt(n)

x_crit <- qnorm(0.95, mu_x_bar_0, sigma_x_bar, lower.tail=TRUE)
power <- pnorm(x_crit, mu_x_bar_1, sigma_x_bar, lower.tail=FALSE)

d <- data.table(n, power)

ggplot(d, aes(n, power)) +
  geom_line()
```


## Summary

* The **p-value** is the probability of the outcome or **a
more extreme outcome** occurring under the assumption that
$H_0$ is true.
  
* What counts as a more extreme outcome is determined by $H_1$.

* The **critical value** is the observed value for which
  more extreme outcomes would lead us to reject $H_0$.

* The **rejection region** is the set of all outcomes that
  would lead us to reject $H_0$.

<!-- * A **$1-\alpha\%$ confidence interval** is a range of -->
<!--   values in which the true value of the parameter is -->
<!--   $1-\alpha\%$ likely to reside. -->

* `binom.test()` can be used to efficiently perform a
  binomial test without manually labouring through the
  5 steps.

* Must do everything long-form if dealing with a Normal
  sampling distribution (known variance).
  
* If the standard deviation of the original sampling distribution 
  is not known, then it must be estimated, and the appropriate 
  test to use is a t-test.
  
* A $t$ distribution has higher tails than a $Z$ distribution
  but is otherwise very similar.
  
* `t.test()` can be used to efficiently perform a t-test
  without manually labouring through the 5 steps.
