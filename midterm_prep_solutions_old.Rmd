---
title: "Midterm Prep Key"
author: "Author: Matthew J. Crossley"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float:
        collapsed: false
        smooth_scroll: true
    toc_depth: 3
    fig_caption: yes
    number_sections: false
    theme: cosmo
fontsize: 18pt
---

```{r echo=FALSE}
library(data.table)
library(ggplot2)

rm(list = ls())

source("util_funcs.R")
set.seed(0)
```

## PDF

### Practice 1

```{r echo=FALSE}
l <- generate_pdf_continuous()
g <- l[[1]]
d <- l[[2]]

g

## The values in the graph are given by the following:
x1 <- d[, x][1]
x2 <- d[, x][2]
x3 <- d[, x][3]
x4 <- d[, x][4]
x5 <- d[, xend][4]

y1 <- 0
y2 <- d[, sort(density)][1]
y3 <- d[, sort(density)][2]
y4 <- d[, sort(density)][3]
y5 <- d[, sort(density)][4]
```

- Is $X$ continuous or discrete?

```{r}

## We can tell that X is continuous because it is defined for every value
## between some interval [a, b]. In this case, a=67 and b=366.

```

- Compute $P(X < `r x3`)$

```{r}

## Probability is given by the area under a PDF. Thus, this probability is given
## by the area under the PDF for which x < 181. We can see that this area is
## composed of two rectangles. Further we know that the area of a rectangle is
## just the product of the width and the height.
width_r1 <- 158 - 67
height_r1 <- 0.005

width_r2 <- 181 - 158
height_r2 <- 0.0016

p <- width_r1*height_r1 + width_r2*height_r2

```

- Compute $P(X > `r x3`)$

```{r}

## Same logic as previous problem
width_r1 <- 271 - 181
height_r1 <- 0.0022

width_r2 <- 366 - 271
height_r2 <- 0.0033

p <- width_r1*height_r1 + width_r2*height_r2

```

- Compute $P(`r x1` \leq X < `r x3`)$

```{r}

## Same logic as previous problem
width_r1 <- 158 - 67
height_r1 <- 0.005

width_r2 <- 181 - 158
height_r2 <- 0.0016

p <- width_r1*height_r1 + width_r2*height_r2

```

- Find $x$ such that $P(X < x) = 0.05$

```{r}

## We know the smallest value x can take is 67. We also know that up to x=158,
## the height of the pdf is 0.005. Since area under the pdf is probability, and
## area is also width times height, we can do a little algebra to solve for
## width:
## p = w x h
## w = p / h
w <- 0.05 / 0.005
x <- 67 + w

```

- Find $x$ such that $P(X > x) = 0.05$

```{r}

## Same logic as above but this time we come in from the right side.
## p = w x h
## w = p / h
w <- 0.05 / 0.0033
x <- 366 - w

```

### Practice 2

```{r echo=FALSE}

l <- generate_pdf_discrete()
g <- l[[1]]
d <- l[[2]]

g

## The values in the graph are given by the following:
x1 <- d[, x][1]
x2 <- d[, x][2]
x3 <- d[, x][3]
x4 <- d[, x][4]

y1 <- d[, sort(mass)][1]
y2 <- d[, sort(mass)][2]
y3 <- d[, sort(mass)][3]
y4 <- d[, sort(mass)][4]

```

- Is $X$ continuous or discrete?

```{r}

## X is discrete because it is defined for only a few single x values.

```

- Compute $P(X < `r x3`)$

```{r}

## Given a probability mass function, probabilities of a given outcome are given
## by the height of the pmf at that value. The x values 51 and 40 are the only
## x values that satisfy X<73
p <- 0.5517 + 0.0776

```

- Compute $P(X > `r x3`)$

```{r}

## Same logic as above
p <- 0.1724

```

- Compute $P(`r x1` \leq X < `r x3`)$

```{r}

## Same logic as above
p <- 0.5517 + 0.0776

```

- Compute the mean and variance of $X$

```{r}

mux <- 0.5517*40 + 0.0776*51 + 0.1983*70 + 0.1724*78
sigxsq <- 0.5517*( 40 - mux )^2 + 0.0776*( 51 - mux )^2 + 0.1983*( 70 - mux )^2 + 0.1724*( 78 - mux)^2

```

## CDF

### Practice 1

For the random variable $X \sim N(\mu_X=10, \sigma_X=2)$

```{r}

mux <- 10
sigx <- 2

```

- Compute $P(X < 12)$

```{r}

## Recall that for continuous distributions P(X<x) = P(X<=x), which keeps our
## use of `lower.tail` pretty straight forward.
pnorm(12, mux, sigx, lower.tail=TRUE)

```

- Compute $P(X > 8)$

```{r}

pnorm(8, mux, sigx, lower.tail=FALSE)

```

- Compute $P(8 \leq X < 12)$

```{r}

pnorm(12, mux, sigx, lower.tail=TRUE) - pnorm(8, mux, sigx, lower.tail=TRUE)

```

### Practice 2

```{r echo=FALSE}

l <- generate_cdf_discrete()

g <- l[[1]]
d <- l[[2]]

g

## The values in the graph are given by the following:
x1 <- d[, x][1]
x2 <- d[, x][2]
x3 <- d[, x][3]
x4 <- d[, x][4]

y1 <- d[, sort(probability)][1]
y2 <- d[, sort(probability)][2]
y3 <- d[, sort(probability)][3]
y4 <- d[, sort(probability)][4]

```

```{r}

## For all the following problems, just read off the graph.

## The key point to notice is that this cdf corresponds to a
## discrete random variable.

## X has non-zero probability only for x=29, x=123, x=147, x=213, x=228

```

- Compute $P(X < `r x2`)$

```{r}

## P(X<123) = 0.1379

```

- Compute $P(X \geq `r x2`)$

```{r}

## P(X>=123) = 1 - 0.1379

```

- Compute $P(`r x2` \leq X < `r x3`)$

```{r}

## P(123â‰¤X<147) = 0.2517

```

## Quantile Function

### Practice 1

For the random variable $X \sim N(\mu_x = 5, \sigma_x=3)$

- Find $x$ such that $P(X < x) = 0.5$

```{r}

qnorm(0.5, 5, 3, lower.tail=TRUE)

```

- Find $x$ such that $P(X < x) = 0.05$

```{r}

qnorm(0.05, 5, 3, lower.tail=TRUE)

```

- Find $x$ such that $P(X \geq x) = 0.05$

```{r}

qnorm(0.05, 5, 3, lower.tail=FALSE)

```

### Practice 2

For the random variable $X \sim Poisson(\lambda=4)$

- Find $x$ such that $P(X \leq x) = 0.5$

```{r}

p <- seq(0,0.99,0.01)
qf <- qpois(p, 4, lower.tail=TRUE)
d <- data.table(p, qf)
ggplot(d, aes(x=p, y=qf)) +
  geom_point() +
  scale_y_continuous(breaks=qf) +
  xlab('P(X <= x)') +
  ylab('x') +
  geom_vline(xintercept=0.5, linetype=2)

qpois(0.5, 4, lower.tail=TRUE)

```

- Find $x$ such that $P(X < x) = 0.05$

```{r}

p <- seq(0,0.99,0.01)
qf <- qpois(p, 4, lower.tail=TRUE)
d <- data.table(p, qf)
ggplot(d, aes(x=p, y=qf)) +
  geom_point() +
  scale_y_continuous(breaks=qf) +
  xlab('P(X <= x)') +
  ylab('x') +
  geom_vline(xintercept=0.05, linetype=2)

qpois(0.05, 4, lower.tail=TRUE) + 1

```

- Find $x$ such that $P(X > x) = 0.05$

```{r}

p <- seq(0,0.99,0.01)
qf <- qpois(p, 4, lower.tail=FALSE)
d <- data.table(p, qf)
ggplot(d, aes(x=p, y=qf)) +
  geom_point() +
  scale_y_continuous(breaks=qf) +
  xlab('P(X > x)') +
  ylab('x') +
  geom_vline(xintercept=0.05, linetype=2)

qpois(0.05, 4, lower.tail=FALSE)

```

## NHST

### Practice 1

Consider an experiment in which participants first learn novel categories, and
are subsequently asked to perform some other task. The question the researchers
are interested in is how initial learning affects the subsequent task, and it is
therefore critical that participants that didn't learn anything be excluded from
the final analysis. An exclusion criteria is set such that any participant that
fails to reach at least $60\%$ accuracy by the end of training is excluded.
Suppose that data is collected from 30 participants, and of these, 5 are
excluded. 

* Test the hypothesis that the true probability of meeting
the exclusion criteria is greater than 01.

```{r}

## 1
## Let X = number excluded
## X ~ Binomial(n, p)

## H0: p = 0.1
## H1: p > 0.1

## 2
# alpha = 0.05

## 3
## p_hat = n_success / n_total
## p_hat ~ Binomial(n, p); x -> x/n_total 
## "x/n_total" is to convert between count and proportion

## 4
n <- 30
p_hat_obs <- 5 / n

## 5
p_val <- pbinom(p_hat_obs*n-1, n, 0.1, lower.tail=FALSE)
p_hat_crit <- qbinom(0.05, n, 0.1, lower.tail=FALSE)/n

p_val
p_hat_crit

x <- 0:n
fx <- dbinom(x, n, 0.1)
x <- x / n
d <- data.table(x, fx)
ggplot(d, aes(x, fx)) +
  geom_point() +
  geom_segment(aes(x=p_hat_obs,  
                   xend=p_hat_obs,  
                   y=0,  
                   yend=max(fx)),
               linetype=2, 
               colour='grey') +
  geom_segment(aes(x=p_hat_crit,  
                   xend=p_hat_crit,  
                   y=0,  
                   yend=max(fx)),
               linetype=2, 
               colour='red')

if(p_val < 0.05) {
  print('reject H0')
} else {
  print('fail to reject H0')
}

if(p_hat_obs > p_hat_crit) {
  print('reject H0')
} else {
  print('fail to reject H0')
}

## NOTE: I haven't shown how to compute confidence interval
## estimates from Binomial random variables (or any discrete
## random variable for that matter). It follows the same
## principles in most ways as our approach for continuous
## random variables, but it turns out to be a bit more
## involved. Because of that, I will not expect the CI
## intervals that you compute by hand to exactly match those
## from R. You will, however, need to be careful to properly
## deal with lower.tail=F issues.

```

- Is there a built in function you can use? If so, use it.

```{r}

binom.test(p_hat_obs*n, n=n, p=0.1, alternative='greater', conf.level=0.95)

```

- Report a point estimates for $\hat{p}$

```{r}

p_hat_obs

```

- Report the critical value for $\alpha=0.05$

```{r}

x <- 0:n
fx <- pbinom(x, n, 0.1, lower.tail=FALSE)
x <- x/n
d <- data.table(x, fx)
ggplot(d, aes(x, fx)) +
  geom_point() +
  geom_hline(yintercept=0.05) +
  geom_segment(aes(x=x, xend=x+1/n, y=fx, yend=fx)) +
  scale_x_continuous(breaks=round(x, 1)) +
  ylab('P(X > x)') +
  xlab('x')

p_hat_crit
```

- Compute power against the alternative $H_1$ that $p$ is .3

```{r}
x <- 0:30
pmf_h0 <- dbinom(x, n, 0.1)
pmf_h1 <- dbinom(x, n, 0.3)
x <- x/n
d <- data.table(x, pmf_h0, pmf_h1)
dd <- melt(d, 
          id.vars='x', 
          measure.vars=c('pmf_h0', 'pmf_h1'),
          variable.name='h',
          value.name='fx')
ggplot(dd) +
  geom_point(aes(x, fx, colour=h)) +
  geom_vline(xintercept=p_hat_crit, linetype=2)

## power is the probability in the rejection region
type_I <- d[x > p_hat_crit, sum(pmf_h0)]
type_II <- d[x <= p_hat_crit, sum(pmf_h1)]
power <- d[x > p_hat_crit, sum(pmf_h1)]
```


### Practice 2

```{r echo=FALSE}

n <- 10
mu_x <- 0.6
sigma_x <- 0.15
x <- rnorm(n, mu_x, sigma_x)
x[x > 1] <- 1

```

Consider an experiment in which participants study a list of words and are
subsequently asked to recall as many of the studied words as possible. Suppose
that data is collected from `r n` participants and the percentage of words
recalled by each participant is given by

\begin{equation}
x = ( `r round(x, 2)` )
\end{equation}

Let $X$ be the random variable that each participants performance is drawn from,
and suppose we know $\sigma_x = `r sigma_x`$.

Is $\mu_x$ significantly greater than $0.6$?

```{r}

## 1
## H0: mux = .6
## H1: mux > .6

## 2
# alpha <- 0.05

## 3
## mu_x_hat = xbar 
## x_bar ~ N(mu_x_bar, sig_x_bar)
## mu_x_bar = mu_x
## sig_x_bar = sig_x/sqrt(n)

n <- length(x)
sig_x <- 0.15
mu_x_bar <- 0.60
sig_x_bar <- sig_x/sqrt(n)

## 4
x_bar_obs <- mean(x)

## 5
p_val <- pnorm(x_bar_obs, mu_x_bar, sig_x_bar, lower.tail=FALSE)
x_bar_crit <- qnorm(0.05, mu_x_bar, sig_x_bar, lower.tail=FALSE)

```

- Is there a built-in function you can use? If so, use it.

```{r}
## Unfortunately, there is not.
```

- Report point and interval estimates for $\hat{\mu_x}$

```{r}
# point estimate
x_bar_obs

# interval estimate (90% interval)
ci_width <- qnorm(0.95, mu_x_bar, sig_x_bar) - qnorm(0.05, mu_x_bar, sig_x_bar)
ci_lower <- x_bar_obs - ci_width / 2
ci_upper <- x_bar_obs + ci_width / 2


# interval estimate (95% interval)
ci_width <- qnorm(0.975, mu_x_bar, sig_x_bar) - qnorm(0.025, mu_x_bar, sig_x_bar)
ci_lower <- x_bar_obs - ci_width / 2
ci_upper <- x_bar_obs + ci_width / 2
```

- Report the critical value for $\alpha=0.05$

```{r}
x_bar_crit
```

- Compute power against the alternative $H_1$ that $\mu_x$ is 0.62

```{r}
x <- seq(mu_x_bar-sig_x_bar*4, mu_x_bar+sig_x_bar*4, 0.001)
pdf_h0 <- dnorm(x, 0.60, sig_x_bar)
pdf_h1 <- dnorm(x, 0.62, sig_x_bar)
d <- data.table(x, pdf_h0, pdf_h1)
dd <- melt(d, 
          id.vars='x', 
          measure.vars=c('pdf_h0', 'pdf_h1'),
          variable.name='h',
          value.name='fx')
ggplot(dd) +
  geom_line(aes(x, fx, colour=h)) +
  geom_vline(xintercept=x_bar_crit, linetype=2)

typeI <- pnorm(x_bar_crit, 0.60, sig_x_bar, lower.tail=FALSE)
typeII <- pnorm(x_bar_crit, 0.62, sig_x_bar, lower.tail=TRUE)
power <- pnorm(x_bar_crit, 0.62, sig_x_bar, lower.tail=FALSE)
```

### Practice 3

```{r echo=FALSE}

n <- 15

mu_x <- 0
sigma_x <- 15
x <- rnorm(n, mu_x, sigma_x)

```

Consider an experiment in which participants attempt to toss
a ping pong ball as accurately as possible to a target.
Suppose that data is collected from `r n` participants and
that the mean reach error per participant is given by

\begin{equation}
x = ( `r round(x, 2)` )
\end{equation}

Let $X$ be the random variable that each participants performance is drawn from.

Is $\mu_x$ significantly different from $0$?

```{r}

## 1
## H0: mux = 0
## H1: mux != 0

## 2
# alpha <- 0.05

## 3
## mu_x_hat = x_bar ~ N(mu_x_bar, sig_x_bar)
## mu_x_bar = mu_x
## sig_x_bar = sig_x / sqrt(n)
## Don't know sig_x so we need to estimate it
## This means we need to transform everything for a t-test
## t_x_bar = (x_bar - mu_x_bar) / sig_x_bar 
## t_x_bar ~ t(n-1)

n <- length(x)
df <- n - 1
mu_x_bar <- 0

sig_x <- sd(x)
sig_x_bar <- sig_x/sqrt(n)

## 4
x_bar_obs <- mean(x)
t_obs <- (x_bar_obs - mu_x_bar) / sig_x_bar

## 5
p_val_lower <- pt(-abs(t_obs), n-1, lower.tail = TRUE)
p_val_upper <- pt(abs(t_obs), n-1, lower.tail = FALSE)
p_val <- p_val_lower + p_val_upper

t_crit_lower <- qt(0.025, n-1)
t_crit_upper <- qt(0.975, n-1)

```

- Is there a built in function you can use? If so, use it.

```{r}

t.test(x, mu=0, alternative='two.sided')

```

- Report point and interval estimates for $\hat{\mu_x}$

```{r}
# The insight you have to have here is that to convert
# between t which has mean 0 and (approximately) sd 1, you
# have to scale the interval you get from the t by sig_x_bar.
ci_width <- ( t_crit_upper - t_crit_lower) * sig_x_bar
ci_lower <- x_bar_obs - ci_width / 2
ci_upper <- x_bar_obs + ci_width / 2

# print out stuff to compare with t.test()
p_val
ci_lower
ci_upper

```

- Report the critical values for $\alpha=0.05$

```{r}
t_crit_lower
t_crit_upper
```

- Compute power against the alternative $H_1$ that $\mu_x$
is 2 assuming that the population standard deviation is
equal to the sample standard deviation.

```{r}
# In x_bar units the mean of the null and the alternative
# are 2 units apart. However, since we have to estimate the
# population standard deviation from the data, we need to
# work with t-distributions.
# mu_h0 = 0 --> t
# mu_h1 = 2 --> t + 2/sig_x_bar
lower_t <- -4
upper_t <- 2/sig_x_bar + 4
t <- seq(lower_t, upper_t, 0.1)
pdf_h0 <- dt(t, n-1)
pdf_h1 <- dt(t-2/sig_x_bar, n-1)
d <- data.table(t, pdf_h0, pdf_h1)
dd <- melt(d, 
          id.vars='t', 
          measure.vars=c('pdf_h0', 'pdf_h1'),
          variable.name='h',
          value.name='ft')
ggplot(dd) +
  geom_line(aes(t, ft, colour=h)) +
  geom_vline(xintercept=t_crit_lower, linetype=2) +
  geom_vline(xintercept=t_crit_upper, linetype=2)

# contribution from upper tail
typeI <- pt(t_crit_upper, n-1, lower.tail=FALSE)
typeII <- pt(t_crit_upper-2 / sig_x_bar, n-1, lower.tail=TRUE)
power <- pt(t_crit_upper-2 / sig_x_bar, n-1, lower.tail=FALSE)

# contribution from lower tail
# TODO:...
```

## Data Wrangling

```{r echo=FALSE}

## Make data up -- Rats running a maze -- simple wide format
mu_x <- runif(1, 100, 100)
sigma_x <- runif(1, 100, 100)

maze_time_rat_1 <- rnorm(5, mu_x, sigma_x)
maze_time_rat_2 <- rnorm(5, mu_x, sigma_x)
maze_time_rat_3 <- rnorm(5, mu_x, sigma_x)
maze_time_rat_4 <- rnorm(5, mu_x, sigma_x)
maze_time_rat_5 <- rnorm(5, mu_x, sigma_x)

d_wide <- data.table(
  maze_time_rat_1,
  maze_time_rat_2,
  maze_time_rat_3,
  maze_time_rat_4,
  maze_time_rat_5
)

d_wide[, run := 1:.N]

## Make data up -- Rats running a maze -- simple long format
n_rats <- 5
n_runs_per_rat <- 5
n_rows <- n_rats*n_runs_per_rat
rat <- rep(1:n_rats, n_runs_per_rat)

run <- rep(1:n_runs_per_rat, each=n_rats)

mu_x <- runif(1, 100, 100)
sigma_x <- runif(1, 100, 100)
maze_time <- rnorm(n_rows,
                   mu_x,
                   sigma_x)

d_long <- data.table(run,
                     rat,
                     maze_time)

fwrite(d_wide, '../live_course_material/data/maze/d_wide.csv')
fwrite(d_long, '../live_course_material/data/maze/d_long.csv')

```

### Practice 1

- Convert the following `data.table` to long format
- The `data.table` can be found here:
https://crossley.github.io/cogs2020/data/maze/d_wide.csv

```{r echo=FALSE}

d_wide

```

```{r}

d <- fread('https://crossley.github.io/cogs2020/data/maze/d_wide.csv')
melt(d, id.vars=c('run'))

```

### Practice 2

- Convert the following `data.table` to wide format
- The `data.table` can be found here:
https://crossley.github.io/cogs2020/data/maze/d_long.csv


```{r echo=FALSE}

d_long

```

```{r}

d <- fread('https://crossley.github.io/cogs2020/data/maze/d_long.csv')
dcast(d, run ~ rat, value.var='maze_time')

```

### Practice 3


```{r echo=FALSE}

## Make data up -- Rats running a maze -- more complicated long format
n_rats <- round(runif(1, 20, 50))
n_mazes <- round(runif(1, 4, 10))
n_runs <- round(runif(1, 5, 20))
n_rows <- n_rats*n_mazes*n_runs

rat <- rep(1:n_rats, each=n_mazes*n_runs)
maze <- rep(1:n_mazes, times=n_rats, each=n_runs)
run <- rep(1:n_runs, times=n_rats*n_mazes)

time <- c()
for(i in 1:n_rats) {
  mu_x <- runif(1, 50, 200)
  sigma_x <- runif(1, 25, 50)
  for(j in 1:n_mazes) {
    mu_x <- mu_x + runif(1, -20, 20)
    sigma_x <- sigma_x + runif(1, -10, 10)
    time <- c(time, rnorm(n_runs, mu_x, sigma_x))
  }
}

d <- data.table(rat,
                maze,
                run,
                time)

```

```{r echo=FALSE}

fwrite(d, '../live_course_material/data/maze/maze.csv')

```

Consider an experiment in which a set of rats run through a set of mazes some
number of times each. The researchers running this experiment are interested in
how quickly rats can run these mazes, and so they record the time in seconds
each rat takes to complete each run of each maze. The data for the experiment
they performed can be found here:

https://crossley.github.io/cogs2020/data/maze/maze.csv

- Recreate the following visualisation of their results. Note that error bars
  represent standard error of the mean (SEM).

```{r echo=TRUE}

dd <- d[, .(mean(time), sd(time)/sqrt(length(unique(rat)))), .(maze, run)]

ggplot(dd, aes(run, V1)) +
  geom_pointrange(aes(ymin=V1-V2, ymax=V1+V2)) +
  xlab('run') +
  ylab('mean maze time') +
  facet_wrap(~factor(maze, labels=c('Maze ')))

```

- Recreate the following visualisation of their results. Note that error bars
  represent standard error of the mean (SEM). Also note that to get SEM here,
  first compute the mean `time` per `maze` for each `rat`. Then, compute the
  mean and standard error from these values.

```{r echo=TRUE}

dd <- d[, mean(time), .(rat, maze)]
ddd <- dd[, .(mean(V1), sd(V1)/sqrt(length(unique(rat)))), .(maze)]
ggplot(ddd, aes(maze, V1)) +
  geom_pointrange(aes(ymin=V1-V2, ymax=V1+V2)) +
  xlab('maze') +
  ylab('mean maze time')

```
