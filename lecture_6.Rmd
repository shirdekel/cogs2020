---
title: "Lecture 6 - Hypothesis testing II"
author: "Author: Matthew J. Cossley"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float:
        collapsed: false
        smooth_scroll: true
    toc_depth: 2
    fig_caption: yes
    number_sections: false
    theme: cosmo
fontsize: 14pt
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=F}
library(data.table)
library(ggplot2)
library(ggpubr)
rm(list=ls())
```

## Learning objectives

* Define and understand how to perform a **two-tailed**
hypothesis test.

* In the context of **two-tailed hypothesis tests**, define
and understand the following:

  * p-values
  
  * more extreme outcomes*
  
  * critical value
  
  * rejection region
  
  * $1-\alpha\%$ confidence interval

* Be able to perform two-tailed binomial tests, Normal, and
t-tests manually (i.e., run through the 5 steps).

* Understand when and how to use `binom.test()` to perform
two-tailed binomial tests.

* Understand when and how to use `t.test()` to perform
two-tailed t-tests.

* Define, understand, and reason about type I error, type II
error, and power.


## Two-tailed tests introduction

- Last lecture, we learned how to perform one-tailed
hypothesis tests. The hypothesis framing in those tests
was as follows:

$$
H_0: \theta = c \\
H_1: \theta < c \\
\text{or} \\
H_0: \theta = c \\
H_1: \theta > c
$$

- The hypotheses of a two-tailed test takes the following
form:

$$
H_0: \theta = c \\
H_1: \theta \neq c  \\
H_1: \theta < c \text{ or } \theta > c
$$

- As we will see in the following examples, the main
difference between one-tailed and two-tailed tests is that
the p-value gets contributions from both the lower and the
upper tail of the sampling distribution.

## Two-tailed Binomial test

In the binomial test example from last lecture  we
considered an experiment in which researchers are trying to
determine if a particular rat has learned to press one of
two levers whenever they are placed inside of an
experimental apparatus. Suppose that the experiment
contained $n=100$ trials and the number of trials in which
the rat pressed the correct lever was
$n_{\text{pressed}}=61$. 

Did the rat learn or not? 

In answering this question, suppose that we are concerned
that the rat may have learned to press the wrong lever
(perhaps through experimenter error). This puts us into a
two-tailed test scenario.

<b>1\. Specify the null and alternative hypotheses ($H_0$ and
   $H_1$) in terms of a distribution and population
   parameter.</b>
   
$$
X \sim binomial(n=100, p) \\
H_0: p = 0.5 \\
H_1: p \neq 0.5
$$

<b>2\. Specify the type I error rate -- denoted by the symbol
   $\alpha$ -- you are willing to tolerate.</b>
   
$$
\alpha = 0.05
$$
   
<b>3\. Specify the sample statistic that you will use to
   estimate the population parameter in step 1 and state how
   it is distributed under the assumption that $H_0$ is
   true.</b>
   
$$
\widehat{p} = \frac{X}{n} \sim binomial(n=100, p=0.5) \\ 
x \rightarrow \frac{x}{n}\\
$$

<b>4\. Obtain a random sample and use it to compute the
   sample statistic from step 3. Call this value
   $\widehat{\theta}_{\text{obs}}$</b>

$$   
\begin{align}
\widehat{\theta}_{\text{obs}} &= \widehat{p}_{\text{obs}} \\
                              &= \frac{x_{\text{obs}}}{n} \\
                              &= \frac{61}{100}
\end{align}
$$
   
<b>5\. If $\widehat{\theta}_{\text{obs}}$ is very unlikely to
   occur under the assumption that $H_0$ is true, then
   reject $H_0$. Otherwise, do not reject $H_0$.</b>
   
* Below, we compute the critical values and p-values and plot
them on the sampling distribution.

* Because this is a two-tailed test, we have critical values
above and below the mean.

* Similarly, in two-tailed tests, what counts as *more
extreme values* extends in both directions.

* To get the p-value from a two-tailed test, you need to
compute one p-value for the probability of getting more
extreme results in one direction, and also compute a
separate p-value for the probability of getting more extreme
results in the other direction.

* In the plot below, I have written `x_obs_lower` and
`x_obs_upper` to faciliate thinking about this, but keep in
mind that we only acutally observed `x_obs_upper`.

* `x_obs_lower` is obtained from `x_obs_upper` by reflecting
it around the mean of the sampling distribution.

* Critical values are obtained by splitting $\alpha$ in half
and using one half in the lower tail and one half in the
upper tail.
   
```{r}
n <- 100
p <- 0.5

xobs <- 61/n
xobs_upper <- xobs
xobs_lower <- -(xobs_upper - 0.5) + 0.5

x_crit_upper <- qbinom(0.05/2, n, p, lower.tail=FALSE) / n
x_crit_lower <- qbinom(0.05/2, n, p, lower.tail=TRUE) / n
```

```{r, echo=F, fig.width=8}
n <- 100
p <- 0.5

xobs_upper <- 61/n
xobs_lower <- -(xobs_upper - 0.5) + 0.5
x_crit_upper <- qbinom(0.05/2, n, p, lower.tail=FALSE) / n
x_crit_lower <- qbinom(0.05/2, n, p, lower.tail=TRUE) / n

x <- 0:n
fx <- dbinom(x, n, p)
x <- x/n
d <- data.table(x,
                fx,
                region=factor((x>x_crit_lower) & (x < x_crit_upper), 
                              labels=c('rejection region', 'failed rejection region')))
ggplot(d, aes(x, fx, colour=region)) +
  geom_point() +
  geom_segment(aes(x=xobs_lower, xend=xobs_lower, y=0, yend=d[, max(fx)]),
               colour='black',
               linetype=2) +
  geom_segment(aes(x=xobs_upper, xend=xobs_upper, y=0, yend=d[, max(fx)]),
               colour='black',
               linetype=2) +
  geom_segment(aes(x=x_crit_lower, xend=x_crit_lower, y=0, yend=d[, max(fx)]),
               colour='#CC6666',
               linetype=2) +
  geom_segment(aes(x=x_crit_upper, xend=x_crit_upper, y=0, yend=d[, max(fx)]),
               colour='#CC6666',
               linetype=2) +
  annotate('text', x=xobs_lower-0.09, y=d[x==xobs_lower, fx],
           label='xobs \n (xobs_lower)',
           colour='black') +
  annotate('text', x=xobs_upper+0.09, y=d[x==xobs_upper, fx],
           label='xobs \n (xobs_upper)',
           colour='black') +
  annotate('text', x=x_crit_lower, y=-0.0025,
           label='x_crit_lower',
           colour='#CC6666') +
  annotate('text', x=x_crit_upper, y=-0.0025,
           label='x_crit_upper',
           colour='#CC6666') +
  scale_x_continuous(breaks=seq(0,n,10)/n) +
  ylab('P(X=x)') +
  xlab('x_successes / n_total') +
  theme(legend.title = element_blank())
```

```{r}
# compute p-value by hand
pval_upper <- pbinom(xobs_upper*n-1, n, p, lower.tail=FALSE)
pval_lower <- pbinom(xobs_lower*n, n, p, lower.tail=TRUE)
pval <- pval_upper + pval_lower
pval

# check with binom.test
binom.test(xobs_upper*n, n, p, alternative = "two.sided")
```

## Two-tailed Normal test

Last lecture we considered an experiment in which a rat is
placed into a maze and given the chance to search for a bit
of cheese hidden somewhere in the maze. The researchers know
that rats without any training whatsoever find the cheese on
average in 90 seconds with a standard deviation of 20
seconds. After much training, the researchers are interested
in assessing whether or not the animal has learned where the
cheese is hidden or on the contrary, if the animal has
become frustrated and is taking longer than baseline.

<b>1\. Specify the null and alternative hypotheses ($H_0$ and
   $H_1$) in terms of a distribution and population
   parameter.</b>
   
$$
H_0: \mu = 90 \\
H_1: \mu \neq 90
$$

<b>2\. Specify the type I error rate -- denoted by the symbol
   $\alpha$ -- you are willing to tolerate.</b>
   
$$
\alpha = 0.05
$$
   
<b>3\. Specify the sample statistic that you will use to
   estimate the population parameter in step 1 and state how
   it is distributed under the assumption that $H_0$ is
   true.</b>
   
$$
\widehat{\mu} = \bar{x} \\
\bar{x} \sim \mathcal{N}(\mu_{\bar{x}}, \sigma_{\bar{x}}) \\
\mu_{\bar{x}} = \mu_{x} \\
\sigma_{\bar{x}} = \frac{\sigma_{x}}{\sqrt{n}}
$$

<b>4\. Obtain a random sample and use it to compute the
   sample statistic from step 3. Call this value
   $\widehat{\theta}_{\text{obs}}$</b>
   
The researchers perform 15 trials and measure the time to
cheese on each trial. The data are as follows:

```{r}
xobs <- c(105.25909, 73.47533, 106.59599, 105.44859, 88.29283,
          49.20100, 61.42866, 74.10559, 79.88466, 128.09307,
          95.27187 ,64.01982 ,57.04686 ,74.21077, 74.01570)
xbarobs <- mean(xobs)
```
   
<b>5\. If $\widehat{\theta}_{\text{obs}}$ is very unlikely to
   occur under the assumption that $H_0$ is true, then
   reject $H_0$. Otherwise, do not reject $H_0$.</b>
   
```{r}
n <- 15
mux <-90

sigx <- 20
muxbar <- 90
sigxbar <- sigx / sqrt(n)

xbarobs_upper <- -(xbarobs - mux) + mux
xbarobs_lower <- xbarobs

xbar_crit_upper <- qnorm(0.05/2, muxbar, sigxbar, lower.tail=FALSE)
xbar_crit_lower <- qnorm(0.05/2, muxbar, sigxbar, lower.tail=TRUE)
```

```{r, echo=F, fig.width=10}
x <- seq(muxbar-4*sigxbar, muxbar+4*sigxbar, 0.01)
fx <- dnorm(x, muxbar, sigxbar)
d <- data.table(x,
                fx,
                region=factor(
                  (x>xbar_crit_lower) & (x < xbar_crit_upper), 
                  labels=c('rejection region', 'failed rejection region'))
                )

ggplot(d, aes(x, fx)) +
  geom_line() +
  geom_segment(x=xbarobs_lower, xend=xbarobs_lower, y= 0, yend=d[, max(fx)], colour='black', linetype=2) +
  geom_segment(x=xbarobs_upper, xend=xbarobs_upper, y= 0, yend=d[, max(fx)], colour='black', linetype=2) +
  geom_segment(x=xbar_crit_lower, xend=xbar_crit_lower, y= 0, yend=d[, max(fx)], colour='red', linetype=2) +
  geom_segment(x=xbar_crit_upper, xend=xbar_crit_upper, y= 0, yend=d[, max(fx)], colour='red', linetype=2) +
  geom_ribbon(data=d[x<xbar_crit_lower], 
              aes(ymin=0, ymax=dnorm(x, muxbar, sigxbar), fill=region), alpha=0.5) +
  geom_ribbon(data=d[x>xbar_crit_lower & x<xbar_crit_upper], 
              aes(ymin=0, ymax=dnorm(x, muxbar, sigxbar), fill=region), alpha=0.5) +
  geom_ribbon(data=d[x>xbar_crit_upper], 
              aes(ymin=0, ymax=dnorm(x, muxbar, sigxbar), fill=region), alpha=0.5) +
  annotate('text', 
           x=xbarobs_lower, 
           y=dnorm(xbarobs_lower, muxbar, sigxbar),
           label='xobs lower',
           colour='black',
           hjust=0) +
  annotate('text', 
           x=xbarobs_upper, 
           y=dnorm(xbarobs_upper, muxbar, sigxbar),
           label='xobs upper',
           colour='black',
           hjust=1) +
  annotate('text', x=xbar_crit_lower, y=-0.005,
           label='xbar_crit_lower',
           colour='black',
           hjust=1) +
  annotate('text', x=xbar_crit_upper, y=-0.005,
           label='xbar_crit_upper',
           colour='black',
           hjust=0) +
  ylab('f(xbar)') +
  xlab('xbar')
```

```{r}
# compute p-value by hand
pval_upper <- pnorm(xbarobs_upper, muxbar, sigxbar, lower.tail=FALSE)
pval_lower <- pnorm(xbarobs_lower, muxbar, sigxbar, lower.tail=TRUE)
pval <- pval_upper + pval_lower
pval
```


## Two-tailed t-test

* t-test arises from the Normal test scenarios in which the
sample variance $\sigma_X$ is unknown.

<b>1\. Specify the null and alternative hypotheses ($H_0$ and
   $H_1$) in terms of a distribution and population
   parameter.</b>
   
$$
H_0: \mu = 90 \\
H_1: \mu \neq 90
$$

<b>2\. Specify the type I error rate -- denoted by the symbol
   $\alpha$ -- you are willing to tolerate.</b>
   
$$
\alpha = 0.05
$$
   
<b>3\. Specify the sample statistic that you will use to
   estimate the population parameter in step 1 and state how
   it is distributed under the assumption that $H_0$ is
   true.</b>
   
$$
\widehat{\mu} = \bar{x} \\
\bar{x} \sim \mathcal{N}(\mu_{\bar{x}}, \sigma_{\bar{x}}) \\
\mu_{\bar{x}} = \mu_{x} \\
\sigma_{\bar{x}} = \frac{\sigma_{x}}{\sqrt{n}} \rightarrow
\widehat{\sigma}_{\bar{x}} = \frac{s_{x}}{\sqrt{n}} \\
t = \frac{\bar{x} - \mu_x}{\frac{s_x}{\sqrt{n}}} \sim t(n-1)
$$

<b>4\. Obtain a random sample and use it to compute the
   sample statistic from step 3. Call this value
   $\widehat{\theta}_{\text{obs}}$</b>
   
The researchers perform 15 trials and measure the time to
cheese on each trial. The data are as follows:

```{r}
xobs <- c(105.25909, 73.47533, 106.59599, 105.44859, 88.29283,
          49.20100, 61.42866, 74.10559, 79.88466, 128.09307,
          95.27187 ,64.01982 ,57.04686 ,74.21077, 74.01570)

n <- length(xobs)
xbarobs <- mean(xobs)
sigxbarobs <- sd(xobs) / sqrt(n)

mux <-90
muxbar <- 90

tobs <- (xbarobs - muxbar) / sigxbarobs
```
   
<b>5\. If $\widehat{\theta}_{\text{obs}}$ is very unlikely to
   occur under the assumption that $H_0$ is true, then
   reject $H_0$. Otherwise, do not reject $H_0$.</b>
   
```{r}
tobs_upper <- -tobs
tobs_lower <- tobs

t_crit_upper <- qt(0.05/2, n-1, lower.tail=FALSE)
t_crit_lower <- qt(0.05/2, n-1, lower.tail=TRUE)
```

```{r, echo=F, fig.width=10}
x <- seq(-4, 4, 0.01)
fx <- dt(x, n-1)
d <- data.table(x,
                fx,
                region=factor(
                  (x>t_crit_lower) & (x < t_crit_upper), 
                  labels=c('rejection region', 'failed rejection region'))
                )

ggplot(d, aes(x, fx)) +
  geom_line() +
  geom_segment(x=tobs_lower, xend=tobs_lower, y=0, yend=d[, max(fx)], colour='black', linetype=2) +
  geom_segment(x=tobs_upper, xend=tobs_upper, y=0, yend=d[, max(fx)], colour='black', linetype=2) +
  geom_segment(x=t_crit_lower, xend=t_crit_lower, y=0, yend=d[, max(fx)], colour='red', linetype=2) +
  geom_segment(x=t_crit_upper, xend=t_crit_upper, y=0, yend=d[, max(fx)], colour='red', linetype=2) +
  geom_ribbon(data=d[x<t_crit_lower], 
              aes(ymin=0, ymax=dt(x, n-1), fill=region), alpha=0.5) +
  geom_ribbon(data=d[x>t_crit_lower & x<t_crit_upper], 
              aes(ymin=0, ymax=dt(x, n-1), fill=region), alpha=0.5) +
  geom_ribbon(data=d[x>t_crit_upper], 
              aes(ymin=0, ymax=dt(x, n-1), fill=region), alpha=0.5) +
  annotate('text', 
           x=tobs_lower, 
           y=dt(tobs_lower, n-1),
           label='tobs lower',
           colour='black',
           hjust=0) +
  annotate('text', 
           x=tobs_upper, 
           y=dt(tobs_upper, n-1),
           label='tobs upper',
           colour='black',
           hjust=1) +
  annotate('text', x=t_crit_lower, y=-0.005,
           label='t_crit_lower',
           colour='black',
           hjust=1) +
  annotate('text', x=t_crit_upper, y=-0.005,
           label='t_crit_upper',
           colour='black',
           hjust=0) +
  ylab('f(t)') +
  xlab('t')
```

```{r}
# compute p-value by hand
pval_upper <- pt(tobs_upper, n-1, lower.tail=FALSE)
pval_lower <- pt(tobs_lower, n-1, lower.tail=TRUE)
pval <- pval_upper + pval_lower
pval

# check with t.test
t.test(xobs, mu=mux, alternative='two.sided')
```


## Confidence intervals

- Way back in the beginning of the class we covered descriptive statistics, and
  more recently we have used some of these descriptive statistics as **estimates**
  of population parameters.

- For example, we have used the sample mean $\bar{x}$ as an estimate of the
  population mean $\mu_X$.

- This is an example of a **point estimate** of a population parameter, and it
  represents our single best guess about the true value of the parameter in
  question.

- An alternative approach to point estimation is to instead try to estimate a
  range of values in which the true value of the parameter is likely to reside.

- This is called **interval estimation** and it is accomplished by constructing
  a **confidence interval**.

- To construct a confidence interval for the population mean $\mu_X$ (assuming
  the population variance $\sigma_X^2$ is known), first compute the sample mean
  $\bar{x}$.

- Next, the width of the CI is the same as the width of the distribution of
  sample means required to cover $1-\alpha$ percent of it's probability. Call
  this width $w$.

- Then the confidence interval estimate of $\mu_X$ is given by $CI = \left[
  \bar{x}-w \text{, } \bar{x}+w \right]$

```{r, echo=F}
## Choose a confidence level
alph <- 0.05

## specify a hypothetical distribution for an experiment
mu_x <- 5
sigma_x <- 3

## specify the number of samples to take from X
n <- 10

## take n samples from x
sample_x <- rnorm(n, mu_x, sigma_x)

## compute the observed sample mean
x_bar_obs <- mean(sample_x)

## specify the distribution of sample means
mu_x_bar <- mu_x
sigma_x_bar <- sigma_x / sqrt(n)

## do some routine stuff to plot x-bar sitribution
x <- seq(mu_x_bar - 5*sigma_x_bar, mu_x_bar + 5*sigma_x_bar, 0.01)
fx <- dnorm(x, mu_x_bar, sigma_x_bar)
d <- data.table(x, fx)

## Compute 1-alpha width of the x-bar distribution
x_bar_lower <- qnorm(alph/2, mu_x_bar, sigma_x_bar)
x_bar_upper <- qnorm(1-alph/2, mu_x_bar, sigma_x_bar)

## Compute the width of the confidence interval
CI_half_width <- (x_bar_upper - x_bar_lower)/2

## Plot the distribution of sample means, and show with vertical lines the
## interval that capture 1 - alpha percent of the distribution.
ggplot(d, aes(x, fx)) +
  geom_line() +
  geom_vline(xintercept=x_bar_lower, linetype=2, colour='black') +
  geom_vline(xintercept=x_bar_upper, linetype=2, colour='black') +
  geom_vline(xintercept=x_bar_obs-CI_half_width, linetype=2, colour='red') +
  geom_vline(xintercept=x_bar_obs+CI_half_width, linetype=2, colour='red')
```

- In the above plot, red vertical lines mark the confidence
interval and black vertical lines mark the critical values.

- CI is always centred around the observed value.

- Critical values are always centred around the mean of the
null sampling distribution.

- This can all be written concisely as follows:

$$
CI_{1-\alpha} = \left[ \bar{x} - \Phi_{X}^{-1}(1-\frac{\alpha}{2}),
                \text{ }
                \bar{x} + \Phi_{X}^{-1}(1-\frac{\alpha}{2}) \right]
$$

- For example, the 95% confidence interval for the population mean of $X \sim
  N(\mu_X, \sigma_X)$ is given by:

\begin{align}
CI_{1-0.05} &= \left[ \bar{x} - \Phi_{X}^{-1}(1-\frac{0.05}{2}),
                \text{ }
                \bar{x} + \Phi_{X}^{-1}(1-\frac{0.05}{2}) \right]
\\ \\
CI_{.95}    &= \left[ \bar{x} - \Phi_{X}^{-1}(1-0.025),
                \text{ }
                \bar{x} + \Phi_{X}^{-1}(1-0.025) \right]
\\ \\
            &= \left[ \bar{x} - \Phi_{X}^{-1}(0.975),
                \text{ }
                \bar{x} + \Phi_{X}^{-1}(0.975) \right]
\end{align}

- Notice that just like the point estimate $\bar{x}$, a confidence interval is
  also a random variable, and so changes with every experiment performed.

- We can demonstrate this by repeating the procedure for three experiments in
  the following code chunk.

```{r, echo=F, fig.width=10}
## Choose a confidence level
alph <- 0.05

## specify a hypothetical distribution for an experiment
mu_x <- 5
sigma_x <- 3

## specify the number of samples to take from X
n <- 10

## take n samples from x
sample_x_1 <- rnorm(n, mu_x, sigma_x)
sample_x_2 <- rnorm(n, mu_x, sigma_x)
sample_x_3 <- rnorm(n, mu_x, sigma_x)

## compute the observed sample mean
x_bar_obs_1 <- mean(sample_x_1)
x_bar_obs_2 <- mean(sample_x_2)
x_bar_obs_3 <- mean(sample_x_3)

## specify the distribution of sample means
mu_x_bar <- mu_x
sigma_x_bar <- sigma_x / sqrt(n)

## do some routine stuff to plot x-bar sitribution
x <- seq(mu_x_bar - 5*sigma_x_bar, mu_x_bar + 5*sigma_x_bar, 0.01)
fx <- dnorm(x, mu_x_bar, sigma_x_bar)
d <- data.table(x, fx)

## add sample mean results to d for plotting purposes
d[, x_bar_obs_1 := x_bar_obs_1]
d[, x_bar_obs_2 := x_bar_obs_2]
d[, x_bar_obs_3 := x_bar_obs_3]

## Convert d from wide to long
d <- melt(d, measure.vars=c('x_bar_obs_1', 'x_bar_obs_2', 'x_bar_obs_3'))

## Compute 1-alpha width of the x-bar distribution
x_bar_lower <- qnorm(alph/2, mu_x_bar, sigma_x_bar)
x_bar_upper <- qnorm(1-alph/2, mu_x_bar, sigma_x_bar)

## Compute the width of the confidence interval
CI_half_width <- (x_bar_upper - x_bar_lower)/2

## Plot the distribution of sample means, and show with vertical lines the
## interval that capture 1 - alpha percent of the distribution.
ggplot(d, aes(x, fx)) +
  geom_line() +
  geom_vline(xintercept=x_bar_lower, linetype=2, colour='black') +
  geom_vline(xintercept=x_bar_upper, linetype=2, colour='black') +
  geom_vline(aes(xintercept=value-CI_half_width), linetype=2, colour='red') +
  geom_vline(aes(xintercept=value+CI_half_width), linetype=2, colour='red') +
  facet_wrap(~variable, ncol=3) +
  theme(aspect.ratio=1)
```


## Type I versus Type II errors

### Definitions

- Hypothesis testing is about using noisy data to make decisions about what we
  think the true state of the universe is.

- Sometimes, our procedure for making these decisions will lead us to make the
  correct decision, but sometimes we will make the wrong decision.

- In the context of hypothesis testing, there are two ways to make a correct
  decision and two ways to make an incorrect decision. These are summarised in
  the following table.

|                   | H0 True                 | H1 True                 |
|:------------------|:------------------------|:------------------------|
| Reject H0         | Type I error ($\alpha$) | Power ($1-\beta$)        |
| Fail to reject H0 | Confidence ($1-\alpha$) | Type II error ($\beta$) |


- Here, we have introduced the concept of **power**.

- Power is the probability of correctly rejecting $H_0$.

- Power can only be computed with respect to some fully specified $H_1$.

- This is best seen with a visualisation.

```{r, echo=F}
mu_x_0 <- 2
mu_x_1 <- 7

sigma_x <- 2

n <- 10
x_crit <- qnorm(0.95, mu_x_0, sigma_x, lower.tail=TRUE)

x <- seq(mu_x_0 - 5*sigma_x, mu_x_1 + 5*sigma_x, 0.01)
fx0 <- dnorm(x, mu_x_0, sigma_x)
fx1 <- dnorm(x, mu_x_1, sigma_x)

d <- data.table(x, fx0, fx1)

g1 <- ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx0), colour='red') +
  geom_line(aes(y=fx1), colour='blue') +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d[x<x_crit], aes(x=x, ymin=0, ymax=fx0), fill='red', alpha=0.5) +
  scale_x_continuous(breaks=c(mu_x_0, mu_x_1)) +
  ylab('Probability Density') +
  ggtitle(expression(1-alpha)) +
  theme(plot.title = element_text(size=36))

g2 <- ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx0), colour='red') +
  geom_line(aes(y=fx1), colour='blue') +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d[x>x_crit], aes(x=x, ymin=0, ymax=fx0), fill='red', alpha=0.5) +
  scale_x_continuous(breaks=c(mu_x_0, mu_x_1)) +
  ylab('Probability Density') +
  ggtitle(expression(alpha)) +
  theme(plot.title = element_text(size=36))


g3 <- ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx0), colour='red') +
  geom_line(aes(y=fx1), colour='blue') +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d[x<x_crit], aes(x=x, ymin=0, ymax=fx1), fill='blue', alpha=0.5) +
  scale_x_continuous(breaks=c(mu_x_0, mu_x_1)) +
  ylab('Probability Density') +
  ggtitle(expression(beta)) +
  theme(plot.title = element_text(size=36))


g4 <- ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx0), colour='red') +
  geom_line(aes(y=fx1), colour='blue') +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d[x>x_crit], aes(x=x, ymin=0, ymax=fx1), fill='blue', alpha=0.5) +
  scale_x_continuous(breaks=c(mu_x_0, mu_x_1)) +
  ylab('Probability Density') +
  ggtitle(expression(1 - beta)) +
  theme(plot.title = element_text(size=36))

## ggarrange(g2, g4, g1, g3, ncol=2, nrow=2)
```
<div class = "row">
<div class = "col-md-6">

```{r echo=F}
g2
```

- Type I error is given by $\alpha$
- The probability of a type I error is given by the area under the $H_0$ curve
  in the rejection region.
- A type I error is the probability of incorrectly rejecting $H_0$
- Type I error is completely determined by $H_0$
- Type I error does not depend on $H_1$

</div>
<div class = "col-md-6">

```{r echo=F}
g4
```

- Power is given by $1 - \beta$
- Power is given by the area under the $H_1$ curve in the rejection region.
- Power can depends on both $H_0$ and $H_1$
-
-

</div>
<div class = "col-md-6">

```{r echo=F}
g1
```

- Confidence is given by $1 - \alpha$
- Confidence is given by the area under the $H_0$ curve outside the rejection region.
- Confidence depends only on $H_0$
-
-
-

</div>
<div class = "col-md-6">

```{r echo=F}
g3
```

- Type II error is given by $\beta$
- The probability of a type II error is given by the area under the $H_1$ curve
  outside the rejection region.
- A type II error is the probability of incorrectly failing to reject $H_0$
- Type II error depends on both $H_0$ and $H1$
-


</div>
</div>

## power

```{r, echo=F}
mu_x_0 <- 2
mu_x_1 <- 7

sigma_x <- 2

n <- 10
x_crit <- qnorm(0.95, mu_x_0, sigma_x, lower.tail=TRUE)

x <- seq(mu_x_0 - 5*sigma_x, mu_x_1 + 5*sigma_x, 0.01)
fx0 <- dnorm(x, mu_x_0, sigma_x)
fx1 <- dnorm(x, mu_x_1, sigma_x)

d <- data.table(x, fx0, fx1)

ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx0), colour='red') +
  geom_line(aes(y=fx1), colour='blue') +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d[x>x_crit], aes(x=x, ymin=0, ymax=fx1), fill='blue', alpha=0.5) +
  scale_x_continuous(breaks=c(mu_x_0, mu_x_1)) +
  ylab('Probability Density') +
  ggtitle(expression(1-beta)) +
  theme(plot.title = element_text(size=36))
```

- In general, we want as much power as possible.
- This is because, if $H_0$ isn't true, then we'd really like to reject it.
- How can we increase power?

### Increase the distance between $H_0$ and $H_1$

```{r, echo=F}
mu_x_0 <- 2
mu_x_1 <- c(7, 10)

sigma_x <- 2

n <- 10
x_crit <- qnorm(0.95, mu_x_0, sigma_x, lower.tail=TRUE)

x <- seq(mu_x_0 - 5*sigma_x, mu_x_1[2] + 5*sigma_x, 0.01)
fx0 <- dnorm(x, mu_x_0, sigma_x)
fx1 <- dnorm(x, mu_x_1[1], sigma_x)
fx2 <- dnorm(x, mu_x_1[2], sigma_x)

d <- data.table(x, fx0, fx1, fx2)

g1 <- ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx0), colour='red') +
  geom_line(aes(y=fx1), colour='blue') +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d[x>x_crit], aes(x=x, ymin=0, ymax=fx1), fill='blue', alpha=0.5) +
  scale_x_continuous(breaks=c(mu_x_0, mu_x_1)) +
  ylab('Probability Density') +
  ggtitle(expression(1-beta)) +
  theme(
    aspect.ratio=0.85,
    plot.title = element_text(size=30))

g2 <- ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx0), colour='red') +
  geom_line(aes(y=fx2), colour='blue') +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d[x>x_crit], aes(x=x, ymin=0, ymax=fx2), fill='blue', alpha=0.5) +
  scale_x_continuous(breaks=c(mu_x_0, mu_x_1)) +
  ylab('Probability Density') +
  ggtitle(expression(1-beta)) +
  theme(
    aspect.ratio=0.85,
    plot.title = element_text(size=30))

ggarrange(g1, g2, ncol=2)
```

- The closer together the $H_0$ and $H_1$ distributions, the less power we have.


### Decrease the variance of $H_0$ and $H_1$

```{r, echo=F}
mu_x_0 <- 2
mu_x_1 <- 7

sigma_x <- c(2, 4)

n <- 10
x_crit_1 <- qnorm(0.95, mu_x_0, sigma_x[1], lower.tail=TRUE)
x_crit_2 <- qnorm(0.95, mu_x_0, sigma_x[2], lower.tail=TRUE)

x <- seq(mu_x_0 - 5*sigma_x[2], mu_x_1 + 5*sigma_x[2], 0.01)
fx01 <- dnorm(x, mu_x_0, sigma_x[1])
fx11 <- dnorm(x, mu_x_1, sigma_x[1])
fx02 <- dnorm(x, mu_x_0, sigma_x[2])
fx12 <- dnorm(x, mu_x_1, sigma_x[2])


d <- data.table(x, fx01, fx11, fx02, fx12)

g1 <- ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx01), colour='red') +
  geom_line(aes(y=fx11), colour='blue') +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d[x>x_crit], aes(x=x, ymin=0, ymax=fx11), fill='blue', alpha=0.5) +
  scale_x_continuous(breaks=c(mu_x_0, mu_x_1)) +
  ylab('Probability Density') +
  ggtitle(expression(1-beta)) +
  theme(
    aspect.ratio=0.85,
    plot.title = element_text(size=30))

g2 <- ggplot(d, aes(x=x)) +
  geom_line(aes(y=fx02), colour='red') +
  geom_line(aes(y=fx12), colour='blue') +
  geom_vline(xintercept=x_crit, linetype=2) +
  geom_ribbon(data=d[x>x_crit], aes(x=x, ymin=0, ymax=fx12), fill='blue', alpha=0.5) +
  scale_x_continuous(breaks=c(mu_x_0, mu_x_1)) +
  ylab('Probability Density') +
  ggtitle(expression(1-beta)) +
  theme(
    aspect.ratio=0.85,
    plot.title = element_text(size=30))

ggarrange(g1, g2, ncol=2)
```

- The smaller the variance of $H_0$ and $H_1$, the greater power we have


### Power as a function of sample size

- The key point here is that, when dealing with the distribution of sample
  means, variance is inversely proportional to sample size.

- $\sigma_\bar{X} = \frac{\sigma_X}{\sqrt{n}}$

- This means that power increases with increased sample size.

```{r, echo=F}
mu_x_0 <- 20
mu_x_1 <- 70

sigma_x <- 30

n <- 1:10

mu_x_bar_0 <- mu_x_0
mu_x_bar_1 <- mu_x_1

sigma_x_bar <- sigma_x / sqrt(n)

x_crit <- qnorm(0.95, mu_x_bar_0, sigma_x_bar, lower.tail=TRUE)
power <- pnorm(x_crit, mu_x_bar_1, sigma_x_bar, lower.tail=FALSE)

d <- data.table(n, power)

ggplot(d, aes(n, power)) +
  geom_line()
```