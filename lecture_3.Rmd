---
title: "Lecture 3 - Probability distributions, populations, and samples"
author: "Author: Matthew J. Cossley"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float:
        collapsed: false
        smooth_scroll: true
    toc_depth: 2
    fig_caption: yes
    # code_folding: show
    number_sections: false
    theme: cosmo
fontsize: 14pt
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Learning objectives

* Define probability and understand the principles of how to
compute it for both discrete and continuous data.

* Define and understand the relationship between
populations, samples, random variables, and probability
distributions.

* Understand the relationship between sample statistics and
population parameters.

* Understand how to compute population mean and population
variance.

* Understand how to estimate population mean and population
variance with sample statistics.

* Understand the distribution of sample means and how it
depends on sample size.


## Probability

Probability is simply the likelihood of an event occurring.
If the space of possible outcomes is **discrete**, then the
probability of an event is simple the proportion of times
that event occurs out of all possible outcomes.  A similar
definition holds if the space of possible outcomes is
**continuous**, but there are a few interesting and not
immediately intuitive little kinks that we will need to deal
with. For instance, If the space of possible outcomes is
continuous, then the probability of any any single event is
actually zero (more on this later).

Probabilities obey the following rules:

* $0 \leq P(e) \leq 1$ for all events $e$

* The probability of all events in a sample space must sum
to $1$

* For some set of events, $E = \{e_1, e_2, \ldots, e_n\}$,
the probability of all events in $E$ occurring in a single
sample is the sum of the probabilities for each occurring
individually:

$$
P(E) = P(e_1 \cup e_2 \ldots \cup e_n) = P(e_1) + P(e_2) + \ldots + P(e_n)
$$

Note that the notation $P(e_1 \cup e_2)$ indicates the
probability of the *union* of $e_1$ and $e_2$ and can be
read as the the probability of both event $e_1$ and event
$e_2$ occurring in the same outcome.

For Example, let $A$, $B$, $C$, $A \cup B$, $A \cup C$ be
the possible outcomes of an experiment with the following
probabilities of occurrence:

$$
P(A) = 0.2 \\
p(B) = 0.1 \\
P(C) = 0.1 \\
P(A \cup B) = 0.3 \\
P(A \cup C) = 0.3 \\
$$

This is a valid set of probabilities because all statements
obey:

* $0 < P , 1$ 

* $P(A) + P(B) + P(C) + P(A \cup B) + P(A \cup C) = 1$

* All union probabilities are equal to the sum of their
parts (e.g., $P(A \cup B) = P(A) + P(B)$).

If these three rules are not obeyed then you know that you
are either not dealing with probabilities or else there is
an error somewhere.


## Random variables and distributions

A **random variable** is a data generating process. You can
think of it like an infinitely deep bucket full of
experimental results. Whenever you perform an experiment,
you reach into the bucket, and pull out one result at
random. All possible experimental results contained by the
bucket define the **population** under study, and the set of
probabilities corresponding to each possible outcome is the
**probability distribution**. Probability distributions are
in turn defined as functions of one or more **population
parameters**. In particular, if $X$ is a random variable
with probability distribution $D(\boldsymbol{\theta})$ where
the $\boldsymbol{\theta}$'s are population parameters, then
we say

$$X \sim D(\theta_1, \ldots, \theta_n)$$

In words, this reads *X is distributed as D with parameters*
$\theta_1, \ldots, \theta_n$.


## Sample statistics estimate population parameters

Any result you pull out of the bucket is a **sample** from
the **random variable** or equivalently a **sample** from
the **population**.  In general, we use **sample
statistics** to **estimate** population parameters (e.g., we
use the sample mean to estimate the true population mean).
Estimation of population parameters with sample statistics
is a key step we will always need to take when performing
**inferential statistics**.

<div style="color:#990000"> **The relationship between
sample statistics and population parameters is fundamental
and absolutely critical to understand.** </div>

* Let $X$ be a random variable.

* Let $\boldsymbol{x} = \{x_1, \ldots, x_n\}$ be a sample
from $X$.

The central tendency of the sample $\boldsymbol{x}$ is given
by the **sample mean** $\bar{\boldsymbol{x}}$, but the true
central tendency of the population $X$ is given by the
**population mean** which is denoted by $\mu$ and is defined
by an operation called the expected value of $X$ denoted
$E(X)$:

$$
\begin{align}
\hat{\mu} &= \bar{\boldsymbol{x}}  \\
\bar{\boldsymbol{x}} &= \frac{1}{n} \sum_{i=1}^{n} x_{i}  \\
\\
E(\boldsymbol{X}) &= \mu \\
     &= x_1 p(x_1) + x_2 p(x_2) + \ldots + x_n p(x_n) \\
     &= \sum_{i}^{n} x_i p(x_i)\\
\end{align}
$$

A common measure of spread in the sample is given by the
**sample variance** $\boldsymbol{s}^2$ but the true variance
of the $X$ is given by the **population variance** which is
denoted by $\sigma^2$ and is defined below.

$$
\begin{align} 
\hat{\sigma^2} = \boldsymbol{s}^2 \\
\\
Var(\boldsymbol{X}) &= \sigma^2 \\
       &= E((X - \mu)^2) \\
       &= \sum_i x_{i}^2 p(x_{i}) - \mu^2
\\
\boldsymbol{s}^2 &= \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{\boldsymbol{x}})^2
\end{align}
$$

As a concrete example, consider the following discrete
probability distribution corresponding to random variable
$X$.

```{r, echo=F}
library(data.table)
library(ggplot2)

x <- c(1, 2, 3)
y <- c(0.1, 0.4, 0.5)

d <- data.table(x, y)
d[, mu := sum(x*y)]

ggplot(d, aes(x, y)) +
  geom_segment(aes(xend=x, yend=0)) +
  xlab('X=x') +
  ylab('P(X=x)') +
  theme(aspect.ratio = 1)
```

Here, we are explicitly given the probability distribution
--- i.e., we are told exactly what the probability of each
event is --- so we can calculate the population mean $\mu$
as follows:

$$
\begin{align}
\mu &= E(\boldsymbol{X}) \\
    &= \sum_{i}^{n} x_i p(x_i) \\
    &= (1 \times 0.3) + (2.0 \times 0.6) + (3.0 \times 0.1) \\
    &= 2.4
\end{align}
$$

Now suppose that we draw a the following sample of $n=10$
from this distribution:

```{r, echo=F}
n <- 10
rexdist <- function(n) {
  xs <- vector('numeric', n)
  for(i in 1:n) {
    tmp <- runif(1)
    if(tmp <= 0.1) {
      xs[i] <- 1
    }
    else if(tmp > 0.1 & tmp <= 0.4) {
      xs[i] <- 2
    } else if(tmp > 0.4) {
      xs[i] <- 3
    }
  }
  return(xs)
}
xs <- rexdist(n)
print(xs)
```

The sample mean of this sample is $\bar{\boldsymbol{x}} =$
`r mean(xs)`. Note that our sample mean is not equal to the
population mean (it's just a fluke if it is). In fact, every
time we run this experiment, we will likely get a different
sample mean. Lets run it 5 more times and check each one.

```{r, echo=F}
n <- 10
for(j in 1:5) {
  rexdist(n)
  xs <- rexdist(n) 
  print("x:")
  print(xs)
  print("sample mean:")
  print(mean(xs))
}
```

This indicates that the sample mean is different every time
we run the experiment, and therefore, the sample mean is a
random variable itself!


## Distribution of sample means

In the previous section, we saw that every time we sampled
from a Binomial random variable:

(1) We got a sample mean $\bar{\boldsymbol{x}}$ that was
close to the population mean $\mu$, but didn't match it
exactly unless by dumb luck.

(2) Every sample from $X$ lead to a different value for
$\bar{\boldsymbol{x}}$.

(3) We concluded that $\bar{\boldsymbol{x}}$ must itself be
a random variable.

Moving forward, we will denote the random variable
corresponding to the distribution of sample means with the
symbol $\bar{X}$, and continue to use $\bar{\boldsymbol{x}}$
to refer to a particular sample mean.

$$
\begin{align}
X & \rightarrow \{x_{1}, \ldots, x_{n}\} \\
\\
\bar{X} & \rightarrow \frac{1}{n} \{ x_{1} + \ldots + x_{n} \}
\end{align}
$$

* Notice that $n>1$ samples from $X$ are needed to generate
a single $n=1$ sample from $\bar{X}$.

* This means that in order to estimate the distribution of
sample means, we need to draw $n>1$ samples from $X$ many
times.

If perform an experiment in which we draw $n=10$ samples
from $X$, compute the sample mean $\bar{x}$, and repeat 500
times, then we get the following estimate for the
distribution of sample means:

```{r, echo=F, message=F, warning=F}
num_experiments <- 500
n <- 10

## Draw random samples from X (defined in previous toy
## example) in many different experiments in order to
## estimate the distribution of sample means
d_list <- vector("list", num_experiments) 
for(i in 1:num_experiments) {
  xbar <- mean(rexdist(n)) 
  d_list[[i]] <- data.table(xbar=xbar, exp=i)
}

d <- rbindlist(d_list)

ggplot(d, aes(xbar)) +
    geom_histogram(bins=10) +
    theme(aspect.ratio=1)
```

Note that even though $X$ is discrete, $\bar{X}$ is
continuous. Also note that $\bar{X}$ looks bell-shaped even
though $X$ does not. This is because of something called the
*central limit theorem*. We cover this in a later lecture.

It turns out that $\bar{X}$ depends on `n` --- where $n$ is
the sample size drawn from $X$ --- in a very important way.
In particular, the population variance of the distribution
of sample means $\sigma_{\bar{X}}^2$ decreases as $n$
increases.

The following plots show the estimated distribution of
sample means for $n=10$, $n=20$, and $n=30$.

```{r, echo=F, message=F, warning=F}
num_experiments <- 500
n <- c(10, 20, 30)

d_list <- vector("list", num_experiments * 3) 
for(j in 1:length(n)) {
  for(i in 1:num_experiments) {
    xbar <- mean(rexdist(n[j])) 
    d_list[[i + num_experiments * (j-1)]] <- data.table(xbar=xbar, exp=i, n=n[j])
  }
}

d <- rbindlist(d_list)

g1 <- ggplot(d[n==10], aes(xbar)) + 
  geom_histogram(bins=10) +
  xlim(1.5, 3.5) +
  ggtitle('n=10') +
  theme(aspect.ratio=1)
g2 <- ggplot(d[n==20], aes(xbar)) + 
  geom_histogram(bins=10) +
  xlim(1.5, 3.5) +
  ggtitle('n=20') +
  theme(aspect.ratio=1)
g3 <- ggplot(d[n==30], aes(xbar)) + 
  geom_histogram(bins=10) +
  xlim(1.5, 3.5) +
  ggtitle('n=30') +
  theme(aspect.ratio=1)
library(gridExtra)
grid.arrange(g1, g2, g3, ncol=3)

```

