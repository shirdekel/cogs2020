---
title: "Lecture 3 -Probability, Random Variables, Estimation"
author: "Author: Matthew J. Cossley"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float:
        collapsed: false
        smooth_scroll: true
    toc_depth: 2
    fig_caption: yes
    # code_folding: show
    number_sections: false
    theme: cosmo
fontsize: 14pt
editor_options: 
  chunk_output_type: console
---

```{r setup, include=F, message=F}
knitr::opts_chunk$set(echo = T, collapse=T)
```

# Learning objectives

* Define **probability** and understand the principles of
how to compute it for both discrete and continuous data.

* Define and understand the relationship between
**populations**, **samples**, **random variables**, and
**probability distributions**.

* Understand the relationship between **sample statistics**
and **population parameters**.

* Understand the definition of **Expected Value**.

* Understand how to compute **population mean** and
**population variance**.

* Understand the definition of an **estimate** of a
population parameter.

* Understand the definition of a **point estimate** and
**interval estimate** of population paratmer.

* Understand how to use esample statistics to generate point
estimates and interval estimates for **population mean** and
**population variance** parameters.

* Understand how to convert a `data.table` from **wide
format** to **long format** and vice versa.

* Understand how to use ``ggplot`` to generate scatter
plots, bar plots, box plots, histograms, and violin plots.

# Concepts

## Probability definitions

* An **outcome** is a possible result of an experiment. 

* The **sample space** of an experiment is the set of all
possible outcomes of that experiment.

* An **event** is a set of outcomes of an experiment.

* An **elementary event** is an event which contains only a
single outcome in the sample space.

* Consider the example of flipping a coin twice:

|           | 1st flip | 2nd flip |
|:---------:|:--------:|:--------:|
| outcome 1 | H        | H        |
| outcome 2 | H        | T        |   
| outcome 3 | T        | H        |
| outcome 4 | T        | T        |

* The outcomes of the experiment are the rows of the above
table.

* The set of all rows is the sample space
$\{(H,H),(H,T),(T,H),(TT)\}$.

* An example event is $\{(H,H), (T,T)\}$.

* Any individual row in the above table corresponds to an
elementary event (e.g., $(H,H)$). It is distinct from the
outcome $(H,H)$ only in that it is wrapped in a set
$\{(H,H)\}$.

* **Probability** is simply the likelihood of an event
occurring. If the sample space is **discrete**, and the
likelihood of each outcome is equal, then the probability of
an event is simply the proportion of outcomes contained by
that event relative all possible outcomes.

* A similar definition for probability holds if the space of
possible outcomes is **continuous**, but there are a few
interesting and not immediately intuitive little kinks that
we will need to deal with. For instance, if the sample space
is continuous, then the probability of any any single event
is actually zero (more on this in a later lecture).

## Rules of probability

* $0 \leq P(e) \leq 1$ for all events $e$.

* The probability of all outcomes in a sample space must sum
to $1$.

* The probability that an event $e$ does not occur is given
by $P(\neg e)=1-P(e)$.

* For some set of events that share no outcomes,
$E=\{e_1,e_2,\ldots,e_n\}$, the probability of all events in
$E$ occurring in a single sample is the sum of the
probabilities for each occurring individually:

$$
P(E) = P(e_1 \cup e_2 \ldots \cup e_n) = P(e_1) + P(e_2) + \ldots + P(e_n)
$$

* Note that the notation $P(e_1 \cup e_2)$ indicates the
probability of the *union* of $e_1$ and $e_2$ and can be
read as the the probability of $e_1$ occurring or event
$e_2$ occurring.

* Two events $e_1$ and $e_2$ are **independent** if knowing
that one occurs tells you nothing about whether the second
will occur. This is given by $P(e_1{\cap}e_2)=P(e_1)P(e_2)$.

* Note that the notation $P(e_1 \cap e_2)$ indicates the
probability of the *intersection* of $e_1$ and $e_2$ and can
be read as the the probability of $e_1$ occurring and event
$e_2$ occurring.

For Example, let $A$, $B$, $C$ share no outcomes and be the
possible events of an experiment with the following
probabilities of occurrence:

$$
P(A) = 0.4 \\
p(B) = 0.3 \\
P(C) = 0.3 \\
$$

This is a valid set of probabilities because they obey:

* $0 < P < 1$ 

* $P(A) + P(B) + P(C) = 1$

Now suppose that we are told:

$$
P(A \cup B) = 0.6 \\
P(A \cup C) = 0.7 \\
P(B \cup C) = 0.6 \\
$$

This does not obey the rules of probability because we are
told that they do not share any outcomes and yet  all union
probabilities are not equal to the sum of their parts (e.g.,
$P(A \cup B) \neq P(A) + P(B)$).

## Random variables and distributions

A **random variable** is a data generating process. You can
think of it like an infinitely deep bucket full of
experiment outcomes. Whenever you perform an experiment, you
reach into the bucket, and pull out one outcome at random.
All possible experiment outcomes contained by the bucket
define the **population** under study, and the set of
probabilities corresponding to each possible outcome is the
**probability distribution**. Probability distributions are
in turn defined as functions of one or more **population
parameters**. In particular, if $X$ is a random variable
with probability distribution $D(\boldsymbol{\theta})$ where
the $\boldsymbol{\theta}$'s are population parameters, then
we say

$$X \sim D(\theta_1, \ldots, \theta_n)$$

In words, this reads *X is distributed as D with parameters*
$\theta_1, \ldots, \theta_n$.


## Sample statistics estimate population parameters

Any outcome you pull out of the bucket is a **sample** from
the **random variable**.  In general, we use **sample
statistics** to **estimate** population parameters. An
estimate of a population parameter is our best guess for
what a population parameters true value is. For example, we
use the sample mean to estimate the population mean.
Estimation of population parameters with sample statistics
is a key step we will always need to take when performing
**inferential statistics**.

<div style="color:#990000"> **The relationship between
sample statistics and population parameters is fundamental
and absolutely critical to understand.** </div>

### Population mean for discrete $X$
* Let $X$ be a discrete random variable.

* Let $\boldsymbol{x} = \{x_1, \ldots, x_n\}$ be a sample
from $X$.

The central tendency of the sample $\boldsymbol{x}$ is given
by the **sample mean** $\bar{\boldsymbol{x}}$:
$$
\begin{align}
\bar{\boldsymbol{x}} &= \frac{1}{n} \sum_{i=1}^{n} x_{i}
\end{align}
$$

The true central tendency of $X$ is given by the
**population mean** which is denoted by $\mu$ and is defined
by an operation called the **expected value** of $X$ denoted
$\mathbb{E}\big[X\big]$:

$$
\begin{align}
\mathbb{E}\big[\boldsymbol{X}\big] &= \mu \\
     &= x_1 p(x_1) + x_2 p(x_2) + \ldots + x_n p(x_n) \\
     &= \sum_{i}^{n} x_i p(x_i)\\
\end{align}
$$

If we do not know the true value of the population mean
$\mu$ then we can estimate it using the sample mean
$\bar{\boldsymbol{x}}$.
$$
\begin{align}
\hat{\mu} &= \bar{\boldsymbol{x}}
\end{align}
$$

This is called a **point estimate** of $\mu$, because we are
specifying a single number (i.e., a single point) that is
our best guess for its true value. Later, we will learn
about **interval estimates** of population parameters, which
provide a range of best guess (e.g., we might try to say
that we are $95\%$ percent sure that the true value of some
population parameter is between some lower value and some
upper value). We will see how to generate interval estimates
in a later lecture. For now, it is sufficient to understand
their conceptual relationship to point estimates.

### Population variance for discrete $X$
Similarly, a common measure of the spread of a sample is
given by the **sample variance** $\boldsymbol{s}^2$:

$$
\begin{align} 
\boldsymbol{s}^2 &= \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{\boldsymbol{x}})^2
\end{align}
$$

The true variance of $X$ is given by the **population
variance** which is denoted by $\sigma^2$ and is defined
as follows:
$$
\begin{align} 
\mathbb{Var}\big[\boldsymbol{X}\big] &= \sigma^2 \\
       &= \mathbb{E}\big[(X - \mu)^2\big] \\
       &= \sum(x^2 - 2x\mu + \mu^2) p(x) \\
       &= \sum x^2 p(x) - \sum 2 x \mu p(x) + \sum \mu^2 p(x) \\
       &= \sum x^2 p(x) - 2 \mu \sum x p(x) + \mu^2 \sum p(x) \\
       &= \sum x^2 p(x) - 2 \mu^2 + \mu^2 \\
       &= \left(\sum_i x_{i}^2 p(x_{i})\right) - \mu^2
\end{align}
$$

If we do not know the true value of the population variance
$\sigma^2$ then we can estimate it using the sample variance
$\bar{\boldsymbol{s}^2}$.
$$
\begin{align}
\hat{\sigma^2} &= \boldsymbol{s}^2 \\
\end{align}
$$

### Discrete $X$ example
As a concrete example, consider the following discrete
probability distribution corresponding to the random
variable $X$.

```{r, echo=F}
library(data.table)
library(ggplot2)

x <- c(1, 2, 3)
y <- c(0.1, 0.4, 0.5)

d <- data.table(x, y)
d[, mu := sum(x*y)]

ggplot(d, aes(x, y)) +
  geom_segment(aes(xend=x, yend=0)) +
  xlab('X=x') +
  ylab('P(X=x)') +
  theme(aspect.ratio = 1)
```

<div style="color:#990000"> **Note that this is a
probability distribution. It defines how likely outcomes
from a random variable is. It is not itself a sample from a
random variable** </div>

Since we are explicitly given the probability distribution
--- i.e., we are told exactly what the probability of each
event is --- so we can calculate the population mean $\mu$
as follows:

$$
\begin{align}
\mu &= \mathbb{E}\big[\boldsymbol{X}\big] \\
    &= \sum_{i}^{n} x_i p(x_i) \\
    &= (1 \times 0.1) + (2.0 \times 0.4) + (3.0 \times 0.5) \\
    &= 2.4
\end{align}
$$

<div style="color:#990000"> **Notice here that we have
calculated the true value of a population parameter, yet we
have not collected a single observation from an experiment.
This is only possible because we were given the true
probability distribution. When we are given exact
probability distributions, then we can calculate population
parameters exactly. When we are not given exact probability
distributions, then we must estimate population parameters
using sample statistics obtained by performing an experiment
in which we sample from the random variable under
consideration.** </div>

Now suppose that we draw a the following sample of $n=10$
from this distribution:

```{r, echo=F}
n <- 10
rexdist <- function(n) {
  xs <- vector('numeric', n)
  for(i in 1:n) {
    tmp <- runif(1)
    if(tmp <= 0.1) {
      xs[i] <- 1
    }
    else if(tmp > 0.1 & tmp <= 0.4) {
      xs[i] <- 2
    } else if(tmp > 0.4) {
      xs[i] <- 3
    }
  }
  return(xs)
}
xs <- rexdist(n)
print("x:")
print(xs)
print("sample mean:")
print(mean(xs))
```

The sample mean of this sample is $\bar{\boldsymbol{x}} =$
`r mean(xs)`. Note that our sample mean is not equal to the
population mean (it's just a fluke if it is). In fact, every
time we run this experiment, we will likely get a different
sample mean. Lets run it 5 more times and check each one.

```{r, echo=F}
n <- 10
for(j in 1:5) {
  rexdist(n)
  xs <- rexdist(n) 
  print("x:")
  print(xs)
  print("sample mean:")
  print(mean(xs))
}
```

<div style="color:#990000"> **This indicates that the sample
mean is different every time we run the experiment, and
therefore, the sample mean is a random variable itself! This
idea is very important to how we go about performing
statistical inference, and we will treat it more formally in
later lectures. For now, make sure you see clearly why and
how we know it is a random variable** </div>

### Population mean and variance for continuous $X$
* Let $X$ be a continuous random variable.

* Let $\boldsymbol{x} = \{x_1, \ldots, x_n\}$ be a sample
from $X$.

* Sample statistics are computed in exacrtly the same way
regardless of whether $X$ is continuous or discrete.

* Population parameters are again computed in terms of the
**expected value** operator, but the way this operator works
is a bit different depending on whether $X$ is continuous or
discrete.

* In particular, if $X$ is continuous, then we will replace
discrete sums (i.e., $\sum x$) with continuous integrals
(i.e., $\int x dx$).

<!-- :::: {style="display: flex;"} -->
:::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 10px"}

::: {}
#### Continuous $X$ mean
$$
\begin{align}
\bar{\boldsymbol{x}} &= \frac{1}{n} \sum_{i=1}^{n} x_{i} 
\\
\mathbb{E}\big[\boldsymbol{X}\big] &= \mu \\
     &= \int x f(x) dx \\
\\
\hat{\mu} &= \bar{\boldsymbol{x}}
\end{align}
$$
:::

::: {}
#### Continuous $X$ variance
$$
\begin{align} 
\boldsymbol{s}^2 &= \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{\boldsymbol{x}})^2 
\\
\mathbb{Var}\big[\boldsymbol{X}\big] &= \sigma^2 \\
       &= \mathbb{E}\big[(X - \mu)^2\big] \\
       &= \int(x^2 - 2x\mu + \mu^2) f(x) dx \\
       &= \int x^2 f(x) dx - \int 2 x \mu f(x) dx + \int \mu^2 f(x) dx \\
       &= \int x^2 f(x) dx - 2 \mu \int x f(x) dx + \mu^2 \int f(x) dx \\
       &= \int x^2 f(x) dx - 2 \mu^2 + \mu^2 \\
       &= \left(\int x^2 fx \right) - \mu^2
\\       
\hat{\sigma^2} &= \boldsymbol{s}^2 
\\
\end{align}
$$
:::

::::


### Continuous $X$ example
As a concrete example, consider the following continuous
probability distribution corresponding to the random
variable $X$.

```{r, echo=F}
library(data.table)
library(ggplot2)

x <- seq(-3, 3, 0.1)
y <- dnorm(x)

d <- data.table(x, y)
d[, mu := 0]

ggplot(d, aes(x, y)) +
  geom_line() +
  xlab('X=x') +
  ylab('f(X=x)') +
  theme(aspect.ratio = 1)
```

<div style="color:#990000"> **Notice that the y-axis is not
labelled with $P(X=x)$. This is because when $X$ is
continuous $P(X=x)=0$ for all $x$. The $f(x)$ label refers
to probability density. We will discuss this more formally
in a later lecture.** </div>

As for our discrete $X$ example, we are explicitly given the
probability distribution --- i.e., we are told exactly what
the probability *density* of each event is --- so we can
calculate the population mean $\mu$ as follows:

$$
\begin{align}
\mu &= E(\boldsymbol{X}) \\
    &= \int x p(x) dx \\
    &= 0 \\
\end{align}
$$

<div style="color:#990000"> **Note that you will not be
expacted to evaluate difficulr integrals in this unit. You
will, however, need to understand the trick I used to
evaluate this integral. The trick is see that the
distribution is symmetric, and so the expected value is the
peak value.** </div>

Now suppose that we draw a the following sample of $n=10$
from this distribution:

```{r, echo=F}
n <- 10
xs <- rnorm(n)
print("x:")
print(xs)
print("sample mean:")
print(mean(xs))
```

The sample mean of this sample is $\bar{\boldsymbol{x}} =$
`r mean(xs)`. As for our discrete $X$ exmaple, we see that
our sample mean is not equal to the population mean. Lets
run it 5 more times to demonstrate again that the sample
mean is indeed a ranomd variable.

```{r, echo=F}
n <- 10
for(j in 1:5) {
  xs <- rnorm(n) 
  print("x:")
  print(xs)
  print("sample mean:")
  print(mean(xs))
}
```

# Practice with `R`

## wide to long format `data.table`

* Suppose you perform an experiment in which you collect
data from 3 participants, and each participant gives you 5
observations.

* If this data were represented in *wide* format, then data
from each participant would be in separate columns.

* If this data were represented in *long* format, then data
from every participant would be in the same column. A
separate column would *indicate* what subject a given row
corresponded to.

* To convert from wide to long format use the `melt()`
function from `data.table`.

```{r}
# define example data
x1 <- runif(5)
x2 <- runif(5)
x3 <- runif(5)

# create a wide format data.table
d_wide <- data.table(x1, x2, x3)

# convert to a long format data.table
d_long <- melt(d_wide, measure.vars=c('x1', 'x2', 'x3'))

d_wide
d_long
```

## Data visualisation using `ggplot`

Data visualisation is a core component of modern descriptive
statistics and it is essential that we become proficient at
collecting data, assembling it into csv-style formats,
piping it into `R`, and inspecting it using plots.

There are numerous methods to generate plots using `R`, but
in this unit, we will focus exclusively on first
representing our data as a `data.table` and then using
`ggplot` to make plots.

Note that many of the visualisations we will demonstrate
below look best when we have lots of data, so lets make a
bigger toy example now.

```{r}
# define example data with more observations
x1 <- runif(100)
x2 <- runif(100)
x3 <- runif(100)

# create a wide format data.table
d_wide <- data.table(x1, x2, x3)

# convert to a long format data.table
d_long <- melt(d_wide, measure.vars=c('x1', 'x2', 'x3'))
```

### `geom_point`

Perhaps the most straightforward approach to visualising
data is simply to plot points for each observation in your
sample. 

```{r, message=F}
ggplot(data=d_long, aes(x=variable, y=value)) +
  geom_point()
```


### `geom_boxplot`

Box plots give a summary of how your data is distributed by
visually marking out the median value as well as the 25th to
the 75th percentile. The idea is to concisely illustrate
where the majority of the data fall.

Whiskers will typically extend from the ends of the box to
indicate the more extreme end of your data, and very extreme
data points will often be plotted individually. This aspect
of box plots doesn't have as strong a convention as the
rest, so it's important to read the documentation to be sure
you are plotting what you think you are plotting.

I often find that it can be quite nice to use both
`geom_point` and `geom_box` in conjunction as follows.

```{r, message=F}
ggplot(data=d_long, aes(x=variable, y=value)) +
  geom_boxplot() +
  geom_point()
```


### `geom_violin`

Violin plots are similar to box plots, except rather than
represent your data in terms of percentiles, it attempts to
give you a continuous estimate of how much our your data
fall along the range of possible values. As with box plots,
I often like to overlay individual points.

```{r, message=F}
ggplot(data=d_long, aes(x=variable, y=value)) +
  geom_violin() +
  geom_point()
```


### `geom_hist`

A histogram attempts to illustrate how data is distributed
by grouping data points into a set number of bins, and
plotting a bar with height equal to the number of points in
each bin.

If you have a lot of data, this method can work really well
and convey lots of great information. With a smaller data
set --- like the toy rat example that we are currently using
--- it only works okay.  In any case, working with
histograms can involve a bit of tweaking to get things to
look nice. For example, you can control how many bins are
used and how big or small each bin is by using the `bins`
and `breaks` argument.

```{r, message=F}
ggplot(data=d_long, aes(x=value, fill=variable)) +
  geom_histogram(bins = 10) + 
  facet_wrap(~variable)
```

### `geom_bar`

Bar plots are among the most common plots you will encounter
as you navigate pretty much any scientific field. They throw
away all information about how your data is distributed, and
instead report only on the average values (unless error bars
are included).

There are many ways to use `ggplot` and `geom_bar` to make a
bar plot. A good way to start is to first create a
`data.table` that contains only the average values that you
want to be represented by the bar heights:

```{r, message=F}
d_mean = d_long[, .(var_mean = mean(value)), .(variable)]
ggplot(data=d_mean, aes(x=variable, y=var_mean)) +
  geom_bar(stat='identity')
```


### `geom_line`

Of course, in certain circumstance, we might want to make a
simple line plot. This is also easy to achieve with
`ggplot`, but to demonstrate its use, we will need a
different example data set.

```{r, message=F}
x <- seq(-10, 10, 0.1)
y <- sin(x)
d <- data.table(x, y)

ggplot(data=d, aes(x=x, y=y))+
  geom_line() +
  ylab('sin(x)')
```
