---
title: "Midterm: most common mistakes"
author: "Author: Matthew J. Crossley and Maria Korochkina"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float:
        collapsed: true
        smooth_scroll: true
    toc_depth: 4
    fig_caption: yes
    # code_folding: show
    number_sections: false
    theme: cerulean
fontsize: 14pt
---

```{r echo=FALSE, message = FALSE, warning = FALSE}
library(data.table)
library(ggplot2)
rm(list = ls())

```

This exam is open book, open internet, and open just about everything. However,
please complete this exam "on your own". This means please do not ask your
classmates for help, or post questions to online forms, etc.

You have 72 hours to complete and submit the exam. This time window is to allow
for flexibility in your schedules. The exam should not take anywhere near 72
hours to complete. Rather -- **if you diligently studied the midterm prep
material** -- I think it should take between 1 and 3 hours.

Good luck!

## 1

In the following code chunk, replace `set.seed(0)` with
`set.seed(YOUR_MQ_ID_NUMBER)` where `YOUR_MQ_ID_NUMBER` is your MQ student ID.

```{r}
## IN THE FOLLOWING LINE, REPLACE "0" WITH YOUR STUDENT ID NUMBER
set.seed(12345678)
```

## 2

In this problem, we will analyse magnetoencephalography (MEG) data collected
from a single participant while they performed a category learning experiment.
On each trial of the category learning experiment, the participant viewed a
circular sine wave grating, and had to push a button to indicate whether they
believed the stimulus belonged to category A or category B. We have seen and
worked with this type of category learning experiment many times throughout this
course, and it is further described by the following figure.

<center>
![FigName](img/cats.png){width=300px}
</center>

MEG is used to record the time-series of magnetic and electric potentials at the
scalp, which are generated by the activity of neurons. There are many sensors,
each configured to pick up signal from a different position on the scalp. This
is shown in the following figure (the text labels indicate the channel name and
are placed approximately where the MEG sensor is located on a real head).

<center>
![FigName](img/MEG_2.png){width=300px}
</center>

The data file that we will be working with is arranged into *epochs* aligned to
stimulus presentation. This means that every time a stimulus is presented we say
that an epoch has occurred. We then assign a time of $t=0$ to the exact moment
the stimulus appeared. We then typically look at the neural time series from
just before the stimulus appeared to a little while after the stimulus has
appeared. For this data, each epoch starts 0.1 seconds before stimulus onset,
and concludes 0.3 seconds after stimulus onset. The following figure shows the
MEG signal at every sensory location across the entire scalp for 5 time points
within this $[-0.1s, 0.3s]$ interval.

<center>
![FigName](img/MEG_1.png){width=550px}
</center>

Lets begin!!!

### a
Load the data into a `data.table`. It can be located here:

`https://crossley.github.io/cogs2020/data/eeg/epochs.txt`

```{r}
## YOUR CODE GOES BELOW THIS COMMENT
data <- fread('https://crossley.github.io/cogs2020/data/eeg/epochs.txt')
head(data)
```

- The `time` column contains times in seconds relative to stimulus onset.
  Stimulus onset always occurs at $0$ seconds.

- The `condition` column indicates which category the stimulus belonged to for
  the given `epoch`. We won't make use of this column here, and we will remove
  it below.

- The `epoch` column is the epoch number. You can think of this like we have
  usually thought of `trial` columns in examples throughout the course.

- The many different `MEG xyz` columns contain the actual neural time series
  signals for each sensor. See the above figure for how these column names map
  onto scalp positions.


### b
Create a `data.table` that contains only `time`, `epoch`, and a single `MEG xyz`
column (See the comments provided in the following code chunk for instructions
on how to choose this column.).

```{r}
## DO NOT REMOVE THE FOLLOWING LINES OF CODE
## The single MEG column that you should extract is the nth column in your
## data.table where n is given as follows:
n <- round(runif(1, 4, 159))

## YOUR CODE GOES BELOW THIS COMMENT
d <- data[, c(2,4,n), with = FALSE] # with = FALSE disables the ability to refer to columns as if they are variables, restoring the data.table mode
```

You could also use `names()` to do this:

```{r}
meg_xyz <- names(data)[n]
d2 <- data[, c("time", "epoch", meg_xyz), with = FALSE]

identical(d,d2)
```


### c
Convert the resulting `data.table` to long format using `time` and `epoch` as
`id.vars`

```{r}
## YOUR CODE GOES BELOW THIS COMMENT
dd <- melt(d, id.vars=c('time', 'epoch'))
```


### d
Create a `data.table` that contains the mean and the standard error of the mean
MEG signal per unit time in the single MEG channel you selected above. Compute
this mean by averaging over all epochs but keeping time points separate.

```{r}
## YOUR CODE GOES BELOW THIS COMMENT
ddd <- dd[, .(mean(value), sd(value)/sqrt(length(unique(epoch)))), .(time)]
```

Also correct would be to do this:

```{r}
ddd2 <- dd[, .(mean(value), sd(value)/sqrt(.N)),.(time)]
```

or this:

```{r}
ddd3 <- dd[, .(mean(value), sd(value)/sqrt(nrow(dd[(time==0)]))), .(time)]
```

These options are correct because mean and SEM are calculated per time unit (with the `by`-argument set to `.(time)`). Importantly, the number of observations used to compute SEM refers to the number of epochs. Conceptually, you can think of an epoch being similar to a trial, and the number of epochs as the number of trials. This means that $n$ in the SEM formula represents how many times each time unit occurred.

```{r}
dd[, length(unique(time))] # there are 41 time units
dd[, .N, .(time)] # each time unit occurred 99 times

# be careful not to confuse the latter with the number of observations in the data.table:
dd[, .N]
```

Therefore, the following answers would be **incorrect**:

```{r}
ddd4 <- dd[, .(mean(value),sd(value)/sqrt(length(unique(time)))),.(time)]

ddd5 <- dd[, .(mean(value),sd(value)/sqrt(length(unique(value)))),.(time)]
```

The latter is wrong because `dd[, length(unique(value))]` gives us the number of unique values for a particular MEG sensor:

```{r}
dd[, length(unique(value))]
```

### e
Plot the resulting time series and include error bars showing the standard error
of the mean.

```{r}
## YOUR CODE GOES BELOW THIS COMMENT
ggplot(ddd, aes(time, V1)) +
  geom_line() +
  geom_ribbon(aes(x=time, ymin=V1-V2, ymax=V1+V2), alpha=0.5)
```

Most of you one of the options shown below - both are of course perfectly correct:

```{r}
ggplot(ddd, aes(time, V1)) +
  geom_line() +
  geom_errorbar(aes(x=time, ymin=V1-V2, ymax=V1+V2), colour = "blue")

ggplot(ddd, aes(time, V1)) +
  geom_line() +
  geom_pointrange(aes(x=time, ymin=V1-V2, ymax=V1+V2), colour = "blue")
```

## 3
Sometimes, the signal acquired via MEG can be too noisy to interpret. This can
happen if there is excessive head movement, if there is a magnetic item in the
scanner (e.g., braces or other orthodontistry), etc. Suppose that in a
hypothetical MEG experiment, out of `n_epochs`, we reject `n_rejected` due to
corruption of the type just described.

**Is this a significantly greater number of rejections than would be expected if
the true probability of rejection was `p`?**

Answer this question using a Null Hypothesis Significance Test (NHST). Do this
first by performing the five steps of NHST manually, and also check your work by
using a built-in R function.

Note that `n_epochs`, `n_rejected` and `p` will be provided for you in the code
chunks below.

```{r}
## DO NOT REMOVE THE FOLLOWING LINES OF CODE
## Use the following variable definitions
n_epochs <- round(runif(1, 200, 400))
n_rejected <- round(runif(1, 10, 100))
p <- round(runif(1, 0.1, 0.3), 2)

## FILL IN YOUR CODE BELOW THIS COMMENT USING MY COMMENTS AS INSTRUCTIONS

## 1. State the null and alternative hypothesis

## Option 1 (estimate true probability with observed proportion)
# H0: p_true = p
# H1: p_true > p

## Option 2 (estimate true expected count with observed count)
# H0: mu_n = p * n
# H1: mu_n > p * n

## 2. Specify an alpha value of 0.05
alph <- 0.05

## 3a. Specify a statistic that estimates the parameter in step 1
# p_hat = n_rejected / n_epochs

## 3b. Specify how this statistic is distributed (i.e., X ~ ??? Is it Binomial?
##     Is it normal? does it have a t distribution? Something else? What are the
##     value of its parameters assuming the null hypothesis is true?)

## Option 1 (estimate true probability with observed proportion)
# p_hat = n_rejected / n_epochs ~ 1 / n_epochs Binomial(n_epochs, p)

## Option 2 (estimate true expected count with observed count)
# mu_n_hat = n_rejected ~ Binomial(n_epochs, p)


## 3c. Plot the null distribution (i.e., the distribution of the statistic you
##     specified in 3a, assuming that the null hypothesis is true.)

## Option 1 (estimate true probability with observed proportion)
x <- 0:n_epochs
fx <- dbinom(x, n_epochs, p)
x <- seq(0, 1, 1/n_epochs)
dnull <- data.table(x, fx)
ggplot(dnull, aes(x, fx)) +
  geom_point()

## Option 2 (estimate true expected count with observed count)
x <- 0:n_epochs
fx <- dbinom(x, n_epochs, p)
dnull <- data.table(x, fx)
ggplot(dnull, aes(x, fx)) +
  geom_point()

## 4. Step 4 is to perform an experiment to observe some outcome that will be
##    used as an estimate of the parameter in step 1.
xobs <- n_rejected

## 5. Compute a p-value and make a decision to reject or fail to reject the
##    null. Be careful to remember that the p-value is the probability of
##    observing what you observed plus the probability of observing a more
##    extreme result.
pval <- pbinom(xobs-1, n_epochs, p, lower.tail=FALSE)

if(pval < alph) {
  print('reject the null with p-value:')
  print(pval)
} else {
  print('fail to reject the null with p-value:')
  print(pval)
}

## 6. Use a built-in R function to check your work.
binom.test(xobs, n_epochs, p, alternative = 'greater', conf.level = 0.95)
```

Remember that `lower.tail = TRUE` gives probabilities for $P(X \leq x)$, while `lower.tail = FALSE` gives probabilities for $P(X > x)$. We need $P(X \geq x)$ to compute the p-value.

## 4
Is the mean MEG signal after stimulus presentation (i.e., $t > 0$) significantly
different from zero?

Answer this question using a Null Hypothesis Significance Test (NHST). Do this
first by performing the five steps of NHST manually, and also check your work by
using a built-in R function.

NOTE: Configure your work such that each epoch gives you one measurement of the
mean MEG signal after stimulus presentation (i.e., compute the mean MEG signal
per epoch and use this as your raw data).

NOTE: Do not assume that the population standard deviation is known.

```{r}
## FILL IN YOUR CODE BELOW THIS COMMENT USING MY COMMENTS AS INSTRUCTIONS

## 1. State the null and alternative hypothesis
# H0: mux = 0
# h1: mux != 0

## 2. Specify an alpha value of 0.05
alph <- 0.05

## 3a. Specify a statistic that estimates the parameter in step 1
# muxhat = xbar

## 3b. Specify how this statistic is distributed (i.e., X ~ ??? Is it Binomial?
##     Is it normal? does it have a t distribution? Something else? What are the
##     value of its parameters assuming the null hypothesis is true?)
# xbar ~ N(mux, sigmax/sqrt(n))
# t = (xbar - muxbar) / sigxbar ~ t(n-1)

## 3c. Plot the null distribution (i.e., the distribution of the statistic you
##     specified in 3a, assuming that the null hypothesis is true).
n <- d[, length(unique(epoch))]

x <- seq(-5, 5, 0.01)
fx <- dt(x, n-1)
dnull <- data.table(x, fx)
ggplot(dnull, aes(x, fx)) +
  geom_line()

## 4. Step 4 is to perform an experiment to observe some outcome that will be
##    used as an estimate of the parameter in step 1.
x <- dd[time > 0, .(signal = mean(value)), .(epoch)]

mux <- 0
muxbar <- mux

sx <- x[, sd(signal)]
sxbar <- sx / sqrt(n)

(xbarobs <- x[, mean(signal)])
(tobs <- (xbarobs - muxbar) / sxbar)

## 5. Compute a p-value and make a decision to reject or fail to reject the
##    null.
pupper <- pt(abs(tobs), n-1, lower.tail=F)
plower <- pt(-abs(tobs), n-1, lower.tail=T)
(pval <- pupper + plower)

if(pval < alph) {
  print('reject the null with p-vale:')
  print(pval)
} else {
  print('fail to reject the null with p-value:')
  print(pval)
}

t.test(x[,signal], mu=0, alternative='two.sided', conf.level=0.95)

## 6. Compute the critical values for this test and use them to make a decision
##    to reject or fail to reject the null.
t_crit_lower <- qt(0.025, n-1, lower.tail=TRUE)
t_crit_upper <- qt(0.025, n-1, lower.tail=FALSE)

if(tobs > t_crit_upper) {
  print('reject the null with p-value:')
  print(pval)
} else
if(tobs < t_crit_lower) {
  print('reject the null with p-value:')
  print(pval)
} else {
  print('fail to reject with p-value:')
  print(pval)
}

## 7. Compute the 95% confidence interval estimate for the parameter in step 1.
ci_width <- (t_crit_upper - t_crit_lower) * sxbar
ci_lower <- xbarobs - ci_width/2
ci_upper <- xbarobs + ci_width/2

print('95% CI: ')
print(ci_lower)
print(ci_upper)
```

Another correct option for question 7 would be to do this:

```{r}
(ci_lower2 <- xbarobs + t_crit_lower*sxbar)
(ci_upper2 <- xbarobs + t_crit_upper*sxbar)
```

or this:

```{r}
(ci_lower3 <- xbarobs - qt(0.975, n-1)*sxbar) # xbarobs - t_crit_upper
(ci_upper3 <- xbarobs + qt(0.975, n-1)*sxbar) # xbarobs + t_crit_upper
```

However, it is **incorrect** to do this:

```{r}
ci_width <- (t_crit_upper - t_crit_lower) * sxbar
(ci_lower4 <- tobs - ci_width/2)
(ci_upper4 <- tobs + ci_width/2)
```

Remember that 

$$
t \times SE = \overline{x} - \mu
$$

So computing the difference between the lower and the upper critical t-value and multiplying the result by the standard error brings you to the "original" scale of the observed value $\overline{x}$.

Therefore, you can either don't multiply by SE and compute the confidence interval for the t-value (however, this would not be the answer to the question because you were supposed to calculate the CI for the parameter $\overline{x}$):

```{r}
(ci_width_t <- t_crit_upper - t_crit_lower)
(ci_lower_t <- tobs - ci_width/2)
(ci_upper_t <- tobs + ci_width/2)
```

or multiply by SE and use $\overline{x}$:

```{r}
(ci_width <- (t_crit_upper - t_crit_lower) * sxbar)
(ci_lower <- xbarobs - ci_width/2)
(ci_upper <- xbarobs + ci_width/2)
```

## 5

### a

For the probability density function shown below, compute $P(X > x_2)$.

Note that the values of all variables shown in the plot are available to you in
the code chunk below.

```{r message=FALSE}
## DO NOT REMOVE THE FOLLOWING LINES OF CODE
x1 <- runif(1, 11, 20)
x2 <- runif(1, 21, 30)
x3 <- runif(1, 31, 40)

fx1 <- runif(1, 0, 1)
fx2 <- runif(1, 1, 2)

d <- data.table(x = c(x1, x2, x3), fx = c(fx1, fx2, fx1))
d[, total_area := 0.5*(x2-x1)*(fx2-fx1) + 0.5*(x3-x2)*(fx2-fx1)]
d[, fx := fx / total_area]

ggplot(d, aes(x, fx)) +
  geom_line() +
  ylim(0, d[, fx[2]]) +
  ylab('Probability Density for RV X') +
  xlab('x') +
  scale_x_continuous(breaks=d[, x], labels=c('x1', 'x2', 'x3')) +
  scale_y_continuous(breaks=d[, fx], labels=c('y1', 'y2', 'y1')) +
  geom_vline(xintercept=d[, x], linetype=2, colour='grey') +
  geom_hline(yintercept=d[, fx], linetype=2, colour='grey') +
  theme_classic()

## YOUR CODE GOES BELOW THIS COMMENT
(base <- d[, x[3]-x[2]])
(height <- d[, fx[3]-fx[2]])
(p <- abs(0.5 * base * height))
```

Also correct would be to do this:

```{r}
base2 <- x3-x2
height2 <- d[, fx[2]-fx[3]]
(p2 <- height2*base2/2)
```

or this:

```{r}
y2 <- fx2/unique(d[,total_area])
y1 <- fx1/unique(d[,total_area])
(p3 <- ((x3-x2)*(y2-y1))/2)
```

or this:

```{r}
base3 <- x3-x2
total_area <- 0.5*(x2-x1)*(fx2-fx1) + 0.5*(x3-x2)*(fx2-fx1)
height3 <- (fx2-fx1)/total_area
(p4 <- 0.5*base3*height3)
```

but **NOT this**:

```{r}
(height4 <- fx2)
(p5 <- height4*base3*0.5)
```

and **NOT this**:

```{r}
(p6 <- punif(x2, 21, 30, lower.tail = F))
```

Why? Because original `fx` was re-defined to be `d[, fx := fx / total_area]`.

Also, if you get a probability that is greater than 1 (as in `p5`), you should start questioning your answer!

### b

For the probability mass function shown below, compute $P(X < x_2)$

Note that the values of all variables shown in the plot are available to you in
the code chunk below.

```{r}
## DO NOT REMOVE THE FOLLOWING LINES OF CODE
x1 <- runif(1, 1, 2)
x2 <- runif(1, 3, 4)
x3 <- runif(1, 5, 6)

fx1 <- runif(1, 0, 1)
fx2 <- runif(1, 0, 1)
fx3 <- runif(1, 0, 1)

d <- data.table(x = c(x1, x2, x3), fx = c(fx1, fx2, fx3))
d[, total_height := sum(fx)]
d[, fx := fx / total_height]

ggplot(d, aes(x, fx)) +
  geom_segment(aes(x=x, xend=x, y=0, yend=fx)) +
  ylim(0, d[, fx[2]]) +
  ylab('Probability Mass for RV X') +
  xlab('x') +
  scale_x_continuous(breaks=d[, sort(x)], labels=c('x1', 'x2', 'x3')) +
  scale_y_continuous(breaks=d[, sort(fx)], labels=c('y1', 'y2', 'y3')) +
  geom_hline(yintercept=d[, fx], linetype=2, colour='grey') +
  theme_classic()

## YOUR CODE GOES BELOW THIS COMMENT
(p <- d[, fx[1]])
```

Also correct would be to this:

```{r}
(p2 <- d[x < x2, sum(fx)])
```

or this:

```{r}
total_height <- fx1 + fx2 + fx3
(p3 <- fx1/total_height)
```

but **NOT this**:

```{r}
(p4 <- fx1 + fx2)
```

and **NOT this**:

```{r}
(p5 <- fx1)
```

The reason is the same is in 5A: the original `fx` was re-defined to be `d[, fx := fx / total_area]`.

### c

For the cumulative probability function shown below (read the axis labels
carefully!), compute $P(X > x_1)$.

Note that the values of all variables shown in the plot are available to you in
the code chunk below. Also note that the line shown is the $y=x$ line (i.e.,
slope=1, intercept=0).

```{r}
## DO NOT REMOVE THE FOLLOWING LINES OF CODE
x1 <- runif(1, 0, 1)
d <- data.table(x=c(0, 1), fx=c(0, 1))
ggplot(d, aes(x, fx)) +
  geom_line() +
  geom_vline(xintercept=x1, linetype=2, colour='grey') +
  geom_hline(yintercept=x1, linetype=2, colour='grey') +
  scale_x_continuous(breaks=c(0, 0.5, 1, x1), labels=c('0.0', '0.5', '1.0', 'x1')) +
  scale_y_continuous(breaks=c(0, 0.5, 1, x1), labels=c('0.0', '0.5', '1.0', 'x1')) +
  ylab('P(X < x)') +
  xlab('x') +
  theme_classic()

## YOUR CODE GOES BELOW THIS COMMENT
(p <- 1 - x1)
```

In this case, it would be correct to use `punif()` to compute the probability of $P(X > x_1)$:

```{r}
(p2 <- punif(x1, 0, 1, lower.tail = F))
```

