---
title: "Lecture 2 - Descriptive statistics"
author: "Author: Matthew J. Cossley"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float:
        collapsed: false
        smooth_scroll: true
    toc_depth: 3
    fig_caption: yes
    # code_folding: show
    number_sections: false
    theme: cosmo
fontsize: 14pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Learning objectives

- Understand the relationship between a sample and a
population.

- Understand the relationship between descriptive and
inferential statistics.

- Understand the difference between continuous and discrete
data observations.

- Understand the difference between wide and long data
formats and be able to convert from wide to long.

- Understand and be able to generate the following plots
using `data.table` and `ggplot`:
  - `geom_point`
  - `geom_box`
  - `geom_violin`
  - `geom_hist`
  - `geom_bar`
  - `geom_line`

- Understand conceptually and be able to compute by hand and
using R the following descriptive statistics:
  - sample mean
  - sample median
  - sample mode
  - sample variance
  - sample standard deviation
  - sample range


## What is statistics?

Very roughly speaking, we can think of statistics as a
mathematical tool for describing data using **descriptive
statistics** and for making decisions on the basis of data
using **inferential statistics**. In statistics, it is safe
to think of all data as coming from an experiment. The act
of performing an experiment is equivalent to simply
assigning numbers to events that we observed in the world.
These numbers are called a **sample**. In general, if we
perform an experiment in which we observe $n$ outcomes, then
our sample is written as:

$$
\boldsymbol{x} = ( x_1, x_2, x_3, ..., x_n )
$$

For example, we might perform an experiment where we observe
a rat navigate a maze several times, and measure the time to
completion for each maze run. Suppose that we observed 10
runs with the following times in seconds:

$$
\boldsymbol{x} = ( 52.38, 55.41, 70.88, 43.30, 50.15, 41.99, 36.82, 34.05, 52.70, 72.25 )
$$

We would say that $\boldsymbol{x}$ is our sample, and in
this case consists of **continuous** data observations. This
is because we are measuring time, and time can yield any
real number as a measurement. If instead we were to measure
the number of wrong turns, our observations would be
**discrete**, and might look like this:

$$
\boldsymbol{x} = ( 8, 2, 10, 7, 3, 1, 6, 9, 5, 4 )
$$

If somebody asks you about your experiment, it is often not
very practical to list all the numbers that you observed.
Instead, you would want some way to concisely summarise the
sample that you obtained. In the following sections, we will
cover some basic **descriptive statistics** that do exactly
this.

In general, we may not want to merely describe our data, but
we may want to use it to help us make decisions. For
example, suppose that any rat that can reliably run this
maze in under 50 seconds is considered a super-mega-genius.
Should you put in the super-mega-genius paperwork for this
rat? Later in the course, we will go deeper into the
**inferential statistics** that will help us in this
scenario, but we will begin developing an intuition now.

As it turns out, answering this question intelligently
requires using our **sample** to make inferences about a
**population**, which can be roughly thought of as the set
of all possible maze times and their relative frequencies
that you will ever observe you run this experiment forever.

Another way of thinking about it is that the **population**
is the process that generates the maze times you will
observe whenever you perform your experiment. A
**population** is the true state of the world --- it is
either the entire collection of things that you are
interested in studying, or else it is the process that
generates a potentially infinite set of things that you are
interested in. For example, if you perform your experiment
another two times, you might end up with the following
samples:

$$
\boldsymbol{x_1} = ( 52.38, 55.41, 70.88, 43.30, 50.15, 41.99, 36.82, 34.05, 52.70, 72.25 )
$$
$$
\boldsymbol{x_2} = ( 62.36, 53.89, 53.95, 33.81, 61.12, 61.48, 36.89, 49.45, 52.50, 50.95 )
$$
$$
\boldsymbol{x_3} = ( 52.04, 48.28, 48.12, 58.89, 51.76, 42.88, 49.04, 60.41, 53.99, 70.06 )
$$

Each sample is composed of very similar but not identical
observations. In statistics, we think of each **sample** as
being drawn or generated from a **population**, and we want
to estimate properties of the **population** by using our
**sample**.

#### Summary

* data from an experiment is a **sample** from a
**population**. 

* We concisely describe our data using **descriptive
statistics**.

* We will use our data to make decisions using **inferential
statistics**.


## Data representation

### Wide vs long format

Consider the rat maze data example given above. We can get
that data into a `data.table` in a number of ways. Here is
an example of how to do it by first using `vectors`.

```{r message=F}
# Our first action will almost always be to start R, load
# the `data.table` and `ggplot2` libraries, and clear the
# current session
library(data.table)
library(ggplot2)

rm(list=ls())

x1 <- c(52.38, 55.41, 70.88, 43.30, 50.15, 41.99, 36.82, 34.05, 52.70, 72.25)
x2 <- c(62.36, 53.89, 53.95, 33.81, 61.12, 61.48, 36.89, 49.45, 52.50, 50.95)
x3 <- c(52.04, 48.28, 48.12, 58.89, 51.76, 42.88, 49.04, 60.41, 53.99, 70.06)

d_wide <- data.table(x1, x2, x3)
```

The `data.table` we created above is in **wide format**
because the results from each of the three experiments that
we ran are contained in separate columns.

Although it is possible to use `ggplot` with wide format
data, it is usually much more efficient to use **long
format** data. As such, this will almost always be our
convention. We can convert from wide to long by using the
`melt()` function as follows:

```{r, message=F}
d_long = melt(d_wide, 
              measure.vars=c('x1', 'x2', 'x3'),
              variable.name = 'experiment_id',
              value.name = 'time')
```

On the other hand, we could have created a long format
`data.table` right off the bat as follows:

```{r, message=F}
time <- c(x1, x2, x3)
experiment_id = c(rep('x1', length(x1)), 
                  rep('x2', length(x1)), 
                  rep('x3', length(x1)))
d_long = data.table(experiment_id, time)
```

## Data visualisation

Data visualisation is a core component of modern descriptive
statistics and it is essential that we become proficient at
collecting data, assembling it into csv-style formats,
piping it into `R`, and inspecting it using plots.

There are numerous methods to generate plots using `R`, but
in this unit, we will focus exclusively on first
representing our data as a `data.table` and then using
`ggplot` to make plots.


### `geom_point`

Perhaps the most straightforward approach to visualising
data is simply to plot points for each observation in your
sample. We have separate experiments -- coded by
`experiment_id` -- so we will keep them separate.

```{r, message=F}
ggplot(data=d_long, aes(x=experiment_id, y=time)) +
  geom_point()
```


### `geom_box`

Box plots give a summary of how your data is distributed by
visually marking out the median value as well as the 25th to
the 75th percentile. The idea is to concisely illustrate
where the majority of the data fall. 

Whiskers will typically extend from the ends of the box to
indicate the more extreme end of your data, and very extreme
data points will often be plotted individually. This aspect
of box plots doesn't have as strong a convention as the
rest, so it's important to read the documentation to be sure
you are plotting what you think you are plotting.

I often find that it can be quite nice to use both
`geom_point` and `geom_box` in conjunction as follows.

```{r, message=F}
ggplot(data=d_long, aes(x=experiment_id, y=time)) +
  geom_boxplot() + 
  geom_point()
```


### `geom_violin`

Violin plots are similar to box plots, except rather than
represent your data in terms of percentiles, it attempts to
give you a continuous estimate of how much our your data
fall along the range of possible values. As with box plots,
I often like to overlay individual points.

```{r, message=F}
ggplot(data=d_long, aes(x=experiment_id, y=time)) +
  geom_violin() + 
  geom_point()
```


### `geom_hist`

A histogram attempts to illustrate how data is distributed
by grouping data points into a set number of bins, and
plotting a bar with height equal to the number of points in
each bin. 

If you have a lot of data, this method can work really well
and convey lots of great information. With a smaller data
set --- like the toy rat example that we are currently using
--- it only works okay.  In any case, working with
histograms can involve a bit of tweaking to get things to
look nice. For example, you can control how many bins are
used and how big or small each bin is by using the `bins`
and `breaks` argument.

```{r, message=F}
ggplot(data=d_long, aes(x=time, fill=experiment_id)) +
  geom_histogram(bins = 5) +
  facet_wrap(~experiment_id)
```


### `geom_bar`

Bar plots are among the most common plots you will encounter
as you navigate pretty much any scientific field. They throw
away all information about how your data is distributed, and
instead report only on the average values (unless error bars
are included).

There are many ways to use `ggplot` and `geom_bar` to make a
bar plot. A good way to start is to first create a
`data.table` that contains only the average values that you
want to be represented by the bar heights:

```{r, message=F}
d_mean = d_long[, .(time = mean(time)), .(experiment_id)]
ggplot(data=d_mean, aes(x=experiment_id, y=time)) +
  geom_bar(stat='identity')
```


### `geom_line`

We might be interested in examining whether the rat got fast
from run to run within each experiment. To do this, it would
be reasonable to place `run` on the x-axis and `time` on the
y-axis. One issue that are faced with right away is that
`d_long` doesn't have a `run` column --- i.e., `run` is
implicitly indicated by the row index. Below, we will first
add a `run` column, and then make the plot described above.

```{r, message=F}
d_long[, run := 1:.N, .(experiment_id)]
ggplot(data=d_long, aes(x=run, y=time, colour=experiment_id)) +
  geom_line()
```


## Central tendency of a sample

Given a sample, a measure of central tendency is supposed to
tell us where most values tend to be clustered. One very
common measure of sample central tendency is called the
**sample mean**. The **sample mean** is denoted by
$\overline{\boldsymbol{x}}$, and is defined by the following
equation:

$$
\overline{\boldsymbol{x}} = \frac{x_1 + x_2 + x_3 + ... + x_n}{n}
$$

We can write this concisely as:

$$
\overline{\boldsymbol{x}} = \frac{1}{n} \sum_{i=1}^{n} x_{i}
$$

Another common measure of sample central tendency is called
the **sample median**. We will denote it by
$\widetilde{\boldsymbol{x}}$, and it is defined simply as
the value that splits the observations in half. Finally,
**sample mode** is the element that occurs most often in the
sample.

### Central tendency by hand

Suppose you have the following observations:
$$
\boldsymbol{x} = (55, 35, 23, 44, 31)
$$

To compute the mean, we simply plug these numbers into the
equation.
$$
\overline{\boldsymbol{x}} = \frac{55 + 35 + 23 + 44 + 31}{5} = \frac{188}{5} = 37.6
$$

To compute the median, first sort the data from smallest to
largest:
$$
\boldsymbol{x}_{sorted} = (23, 31, 35, 44, 55)
$$

Then, pick the value that ends up in the middle:
$$
\widetilde{\boldsymbol{x}} = 35
$$

Since we have an odd number of observations, finding the
median is pretty intuitive, but what if we had an even
number of observations? In this case, we will take the mean
of the middle two numbers.

$$
\boldsymbol{x} = (55, 35, 23, 44)
$$

$$
\boldsymbol{x}_{sorted} = (23, 35, 44, 55)
$$

$$
\widetilde{\boldsymbol{x}} = \frac{35 + 44}{2} = 39.5
$$


### Central tendency using R

In general, things are easier and we are happier and more
productive human beings if we use R. We just store our
sample observations in a variable `x`, and use built-in `R`
functions `mean()` and `median()` to compute the sample mean
and sample median.

```{r eval=TRUE, message=FALSE}
x <- c(55, 35, 23, 44, 31)
mean(x)
median(x)
```

While we're here, we might as well verify our computations
for both samples.

```{r eval=TRUE, message=FALSE}
x <- c(55, 35, 23, 44)
mean(x)
median(x)
```


### Central tendency and outliers

Sometimes a sample contains a few observations that are very
different from the majority of the others. Theses
observations are called **outliers**. How will outliers
influence our measures of central tendency?

To answer this question, consider the rat maze running
example from above. Lets pool all observations across all
three experiments into a single histogram.

```{r}
ggplot(d_long, aes(x='', y=time)) + 
  geom_violin() +
  geom_point()
```

Taking a look at our sample, no observation seems much
different from any other (i.e., there are no obvious
outliers). How do the sample mean and sample median behave
in this case?

```{r}
d_long[, .('sample_mean' = mean(time), 'sample_median' = median(time))]
```
We see that the sample mean and sample median are nearly
identical when there are no outliers.  Lets consider another
example to see how the sample mean and sample median behave
when our sample does contain outliers.

```{r}
d_long_out = data.table(d_long)
d_long_out[.N, time := time * 10] # make the last observation an outlier

ggplot(d_long_out, aes(x='', y=time)) + 
  geom_violin() +
  geom_point()
```

We have clearly introduced an outlier. How do our mesures of
central tendancy behave in this case?

```{r}
d_long_out[, .('sample_mean' = mean(time), 'sample_median' = median(time))]
```

Here, you can see that the mean, but much less so the
median, is sensitive to outliers. So, which is a better
measure of central tendency? The answer to this question
depends entirely on what you think is an outlier and how
much you care about them. Saying much more than that is
beyond the scope of this lecture, but we should leave with
at least a simple lesson: **it is always a good idea to
identify and investigate outliers in our data.**


## Spread of a sample

Measures of spread of a sample are supposed to tell us how
widely the sample observations are distributed. One very
common measure of spread is called **sample variance**. It
is denoted by $\boldsymbol{s}^2$ and it is defined as:

$$
\boldsymbol{s}^2 = \frac{1}{n-1} \sum_{i=1}^{n} ( x_{i} - \overline{\boldsymbol{x}} )^2
$$

An additional measure of spread is called the **sample
standard deviation**. It is denoted by, $\boldsymbol{s}$,
and it is defined simply as the square root of the sample
variance.

$$
\boldsymbol{s} = \sqrt{\boldsymbol{s}^2}
$$

A third measure of spread that we will consider is called
the **sample range**, and it is defined as the difference
between the most extreme observed values.


### Spread by hand

Consider the following sample:

$$
\boldsymbol{x} = (55, 35, 23, 44)
$$

If for some reason you needed to compute sample variance,
and every computer near you was broken, then you could
compute the sample variance of this sample by hand as
follows:

$$
\boldsymbol{s}^2 = \frac{ (55-39.25)^2 + (35 -39.25)^2 + (23-39.25)^2 + (44-39.25)^2 }{4-1}
$$

$$
\boldsymbol{s}^2 = \frac{ (15.75)^2 + (-4.25)^2 + (-16.25)^2 + (4.75)^2 }{4-1}
$$

$$
\boldsymbol{s}^2 = \frac{ 248.0625 + 18.0625 + 264.0625 + 22.5625 }{4-1}
$$

$$
\boldsymbol{s}^2 = \frac{ 552.75 }{4-1} = 184.25
$$

Well, that sucked, and in the next section we will see that
`R` will do this for us with grace and ease.


### Spread by `R`

We now use `R` to quickly compute the all measures of spread
just covered.

```{r eval=TRUE, message=FALSE}
d_long[, .('sample_variance' = var(time),
           'sample_standard_deviation' = sd(time),
           'sample_range' = diff(range(time)))]
```

Notice that that sample variance is larger than sample
standard deviation. This will always true.

We will now finish up by adding the sample mean, sample
variance, standard deviation, and range to our previous
histogram plot. This is a useful exercise to develop an
intuition for what these quantities represent.

```{r, message=F}
ggplot(data=d_long, aes(x=time)) +
  geom_histogram(bins = 10) + 
  # sample variance
  geom_segment(aes(x=mean(time)-var(time)/2,
                   xend=mean(time)+var(time)/2,
                   y = 20,
                   yend = 20), colour='red') +
  # sample sd
  geom_segment(aes(x=mean(time)-sd(time)/2,
                   xend=mean(time)+sd(time)/2,
                   y = 18,
                   yend = 18), colour='blue') +
  # sample range
  geom_segment(aes(x=mean(time)-diff(range(time))/2,
                   xend=mean(time)+diff(range(time))/2,
                   y = 16,
                   yend = 16), colour='green')
```

