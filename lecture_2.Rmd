---
title: "Lecture 2 - Descriptive statistics"
author: "Author: Matthew J. Cossley"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float:
        collapsed: false
        smooth_scroll: true
    toc_depth: 3
    fig_caption: yes
    # code_folding: show
    number_sections: false
    theme: cosmo
fontsize: 14pt
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, message=F}
knitr::opts_chunk$set(echo = T, collapse=T)
```

## Learning objectives

- Understand the relationship between a **sample** and a
**population**.

- Understand the relationship between a **population** and a
**random variable**.

- Understand what a **random sample** is and what is means
for a sample to be **biased**.

- Understand the relationship between **descriptive** and
**inferential statistics**.

- Understand the difference between **continuous** and
**discrete** observations.

- Understand what it means for an experiment design to be
**between-subjects** or **within-subjects**.

- Understand what it means for an experiment design to be
**repeated measures**.

- Understand conceptually and be able to compute by hand and
using `R` the following descriptive statistics:
  - **sample mean**
  - **sample median**
  - **sample mode**
  - **sample variance**
  - **sample standard deviation**
  - **sample range**
  
- Understand and be able to read common plots used to
communicate descriptive statistics including scatter plots,
bar plots, box plots, histograms, and violin plots.

- Understand how to install and load **libraries** in `R`.

- Understand the basics of working with the `data.table`
package in `R`.


## What is statistics?

Very roughly speaking, we can think of statistics as a
mathematical tool for describing data using **descriptive
statistics** and for making decisions on the basis of data
using **inferential statistics**. In statistics, it is safe
to think of all data as coming from an experiment. The act
of performing an experiment is equivalent to simply
assigning numbers to events that we observe in the world.
These numbers are called a **sample**. In general, if we
perform an experiment in which we observe $n$ outcomes, then
our sample is written as:

$$
\boldsymbol{x} = ( x_1, x_2, x_3, ..., x_n )
$$

For example, we might perform an experiment where we observe
a rat navigate a maze several times, and measure the time to
completion for each maze run. Suppose that we observed 10
runs with the following times in seconds:

$$
\boldsymbol{x} = ( 52.38, 55.41, 70.88, 43.30, 50.15, 41.99, 36.82, 34.05, 52.70, 72.25 )
$$

We would say that $\boldsymbol{x}$ is our sample, and in
this case consists of **continuous** data observations. This
is because we are measuring time, and time can yield any
real number as a measurement. If instead we were to measure
the number of wrong turns, our observations would be
**discrete**, and might look like this:

$$
\boldsymbol{x} = ( 8, 2, 10, 7, 3, 1, 6, 9, 5, 4 )
$$

If somebody asks you about your experiment, it is often not
very practical to list all the numbers that you observed.
Instead, you would want some way to concisely summarise the
sample that you obtained. In the following sections, we will
cover some basic **descriptive statistics** that do exactly
this.

In general, we may not want to merely describe our data, but
we may want to use it to help us make decisions. For
example, suppose that any rat that can reliably run this
maze in under 50 seconds is considered a super-mega-genius.
Should you put in the super-mega-genius paperwork for this
rat? Later in the course, we will go deeper into the
**inferential statistics** that will help us in this
scenario, but we will begin developing an intuition now.

As it turns out, answering this question intelligently
requires using our **sample** to make inferences about a
**population**, which in this case can be roughly thought of
as the set of all possible maze times and their relative
frequencies that you will ever observe if you run this
experiment forever.

For example, if you perform your experiment another two
times, you might end up with the following samples:

$$
\boldsymbol{x_1} = ( 52.38, 55.41, 70.88, 43.30, 50.15, 41.99, 36.82, 34.05, 52.70, 72.25 )
$$
$$
\boldsymbol{x_2} = ( 62.36, 53.89, 53.95, 33.81, 61.12, 61.48, 36.89, 49.45, 52.50, 50.95 )
$$
$$
\boldsymbol{x_3} = ( 52.04, 48.28, 48.12, 58.89, 51.76, 42.88, 49.04, 60.41, 53.99, 70.06 )
$$

Each sample is composed of very similar but not identical
observations. A **random variable** is a process that
generates outcomes for a particular experiment, a
**population** is the collection of all possible outcomes,
and a **sample** is a particular outcome obtained when the
experiment is performed (i.e., $\boldsymbol{x_1}$,
$\boldsymbol{x_2}$ and $\boldsymbol{x_3}$ above). In
statistics, we want to estimate properties of the
**population** by using our **sample**.


## What is experiment design?

### Continuous vs discrete data
Random variables are processes that generate data in an
experiment performed by sampling from a particular
population. The data that random variables produce can be
either discrete or continuous. In a rather straight-forward
use of terms, a random variable that produces discrete data
is called a **discrete random variable** and a random
variable that produces continuous data is called a
**continuous random variable**.

For example, consider the random variable $X$ defined as a
process that returns the body lengths in cm of rats drawn
from a **population** defined as all rats in the universe. A
sample from this random variable might look as follows:
$$
\boldsymbol{x} = (2.877970, 7.497241, 5.455286, 4.903578, 7.806955)
$$
Here, $X$ is clearly a continuous random variable because it
seems that any non-zero value can be obtained when sampling
from it.

As another example, consider the random variable $X$ defined
as a process that returns the number of neurons in a rat
cerebral cortex drawn from the same population as in the
previous example. A sample from this random variable might
look as follows:
$$
\boldsymbol{x} = (21e6 21e6 18e6 20e6 22e6)
$$
Here, $X$ is clearly a discrete random variable because that
only integer values can be obtained when sampling from it.

### Between-subjects vs within-subjects
Consider a random variable $X$ defined as the number of
neurons in a rat prefrontal cortex, and another random
variable $Y$ defined as the number of neurons in a rat
cerebellum. Both $X$ and $Y$ are linked to the same
population (i.e., all rats in the universe). If we draw one
random sample from $X$ and one random sample from $Y$ using
a different sample of rats for each, then we say our
experiment design is **between-subjects**. If instead we
draw one random sample from $X$ and another random sample
from $Y$ using the same sample of rats, then we say our
experiment design is **within-subjects**.

### Repeated measures
In a **repeated measures** experiment design, multiple
measurements are obtained from the same level of some
experimental factor. At this stage, this is all a bit
jargon-y -- it will fall into place more naturally as the
unit proceeds -- but the thing to notice is that a
**within-subjects** experiment design is also a **repeated
measures** experiment design, whereas a **between-subjects**
design is not.


## Central tendency of a sample

Given a sample, a measure of central tendency is supposed to
tell us where most values tend to be clustered. One very
common measure of sample central tendency is called the
**sample mean**. The **sample mean** is denoted by
$\overline{\boldsymbol{x}}$, and is defined by the following
equation:

$$
\overline{\boldsymbol{x}} = \frac{x_1 + x_2 + x_3 + ... + x_n}{n}
$$

We can write this concisely as:

$$
\overline{\boldsymbol{x}} = \frac{1}{n} \sum_{i=1}^{n} x_{i}
$$

Another common measure of sample central tendency is called
the **sample median**. We will denote it by
$\widetilde{\boldsymbol{x}}$, and it is defined simply as
the value that splits the observations in half. Finally,
**sample mode** is the element that occurs most often in the
sample.

### Central tendency by hand

Suppose you have the following observations:
$$
\boldsymbol{x} = (55, 35, 23, 44, 31)
$$

To compute the mean, we simply plug these numbers into the
equation.
$$
\overline{\boldsymbol{x}} = \frac{55 + 35 + 23 + 44 + 31}{5} = \frac{188}{5} = 37.6
$$

To compute the median, first sort the data from smallest to
largest:
$$
\boldsymbol{x}_{sorted} = (23, 31, 35, 44, 55)
$$

Then, pick the value that ends up in the middle:
$$
\widetilde{\boldsymbol{x}} = 35
$$

Since we have an odd number of observations, finding the
median is pretty intuitive, but what if we had an even
number of observations? In this case, we will take the mean
of the middle two numbers.

$$
\boldsymbol{x} = (55, 35, 23, 44)
$$

$$
\boldsymbol{x}_{sorted} = (23, 35, 44, 55)
$$

$$
\widetilde{\boldsymbol{x}} = \frac{35 + 44}{2} = 39.5
$$


### Central tendency using R

In general, things are easier and we are happier and more
productive human beings if we use R. We just store our
sample observations in a variable `x`, and use built-in `R`
functions `mean()` and `median()` to compute the sample mean
and sample median.

```{r }
# mean and median of sample 1 above
x <- c(55, 35, 23, 44, 31)
mean(x)
median(x)

# mean and median of sample 2 above
x <- c(55, 35, 23, 44)
mean(x)
median(x)
```


### Central tendency and outliers

Sometimes a sample contains a few observations that are very
different from the majority of the others. Theses
observations are called **outliers**. How will outliers
influence our measures of central tendency?  To answer this
question, consider the rat maze running example from above.

```{r}
# define vectors that contain the maze times from the
# example given at the top of the lecture.
x1 <- c(52.38, 55.41, 70.88, 43.30, 50.15, 41.99, 36.82, 34.05, 52.70, 72.25)
x2 <- c(62.36, 53.89, 53.95, 33.81, 61.12, 61.48, 36.89, 49.45, 52.50, 50.95)
x3 <- c(52.04, 48.28, 48.12, 58.89, 51.76, 42.88, 49.04, 60.41, 53.99, 70.06)

# combine all observations into a single vector
x <- c(x1, x2, x3)

# compute the mean and median of x
mean(x)
median(x)

# add an outlier to x
x <- c(x, 300)

# compute the mean and median of x with the outlier in the data
mean(x)
median(x)
```

Here, you can see that the mean, but much less so the
median, is sensitive to outliers. So, which is a better
measure of central tendency? The answer to this question
depends entirely on what you think is an outlier and how
much you care about them. Saying much more than that is
beyond the scope of this lecture, but we should leave with
at least a simple lesson: **it is always a good idea to
identify and investigate outliers in our data.**


## Spread of a sample

Measures of spread of a sample are supposed to tell us how
widely the sample observations are distributed. One very
common measure of spread is called **sample variance**. It
is denoted by $\boldsymbol{s}^2$ and it is defined as:

$$
\boldsymbol{s}^2 = \frac{1}{n-1} \sum_{i=1}^{n} ( x_{i} - \overline{\boldsymbol{x}} )^2
$$

An additional measure of spread is called the **sample
standard deviation**. It is denoted by, $\boldsymbol{s}$,
and it is defined simply as the square root of the sample
variance.

$$
\boldsymbol{s} = \sqrt{\boldsymbol{s}^2}
$$

A third measure of spread that we will consider is called
the **sample range**, and it is defined as the difference
between the most extreme observed values.


### Spread by hand

Consider the following sample:

$$
\boldsymbol{x} = (55, 35, 23, 44)
$$

If for some reason you needed to compute sample variance,
and every computer near you was broken, then you could
compute the sample variance of this sample by hand as
follows:

$$
\boldsymbol{s}^2 = \frac{ (55-39.25)^2 + (35 -39.25)^2 + (23-39.25)^2 + (44-39.25)^2 }{4-1}
$$

$$
\boldsymbol{s}^2 = \frac{ (15.75)^2 + (-4.25)^2 + (-16.25)^2 + (4.75)^2 }{4-1}
$$

$$
\boldsymbol{s}^2 = \frac{ 248.0625 + 18.0625 + 264.0625 + 22.5625 }{4-1}
$$

$$
\boldsymbol{s}^2 = \frac{ 552.75 }{4-1} = 184.25
$$

Well, that sucked, and in the next section we will see that
`R` will do this for us with grace and ease.


### Spread by `R`

We now use `R` to quickly compute the all measures of spread
just covered.

```{r}
var(x)
sd(x)
diff(range(x))
```

Notice that that sample variance is larger than sample
standard deviation. This will always be true.

We will now finish up by adding the sample mean, sample
variance, standard deviation, and range to our previous
histogram plot. This is a useful exercise to develop an
intuition for what these quantities represent.

## Descriptive visuals
Very often we report descriptive statistics in the form of
plots. In this section we will start getting a feel for some
of the more useful and common types of plots.

```{r include=T, echo=F}
library(ggplot2)
library(data.table)

d <- data.table(sub=integer(1000), x=rnorm(1000, 100, 10))
```

### Histogram
```{r include=T, echo=F}
ggplot(data=d, aes(x=x)) +
  geom_histogram(bins = 30) +
  geom_segment(aes(x=mean(x)-var(x)/2,
                   xend=mean(x)+var(x)/2,
                   y = 20,
                   yend = 20), colour='red') +
  geom_segment(aes(x=mean(x)-sd(x)/2,
                   xend=mean(x)+sd(x)/2,
                   y = 15,
                   yend = 15), colour='blue') +
  geom_segment(aes(x=mean(x)-diff(range(x))/2,
                   xend=mean(x)+diff(range(x))/2,
                   y = 10,
                   yend = 10), colour='green')
```

### Barplot
```{r include=T, echo=F}
d <- data.table(x=rnorm(1000, 100, 5))
d[, condition := factor(rep(c(1, 2), .N/2))]
dd <- d[, .(mean_x=mean(x), var_x=var(x)), .(condition)]
ggplot(data=dd, aes(x=condition, y=mean_x)) +
  geom_col(width=0.5) +
  geom_errorbar(aes(ymin=mean_x-var_x, ymax=mean_x+var_x), width=0.1) 
```

### Boxplot
```{r include=T, echo=F}
d <- data.table(x=rnorm(1000, 100, 5))
d[, condition := factor(rep(c(1, 2), .N/2))]
ggplot(data=d, aes(x=condition, y=x)) +
  geom_boxplot(width=0.5)
```

### Violin plot 
```{r include=T, echo=F}
d <- data.table(x=rnorm(1000, 100, 5))
d[, condition := factor(rep(c(1, 2), .N/2))]
ggplot(data=d, aes(x=condition, y=x)) +
  geom_violin(width=0.5)
```


## `data.table`

* `data.table` does everything `data.frame` does, and much more.

* Because `data.table` is so powerful and so fast, we won't
really bother much with the finer details of `data.frame`.

* To use `data.table` we must first load it into our
environment using the `library()` function. 

* If you get an error message complaining that `there is no
package called data.table` then install it by executing
`install.packages('data.table')`.

* Create a `data.table` with the `data.table()` function.

* General form: `DT[i, j, by]`

  * `i`: selects rows
  
  * `j`: specifies columns and operations to perform on those
  columns
  
  * `by`: specifies columns to group the operation performed
  in `j` by

```{r}
# Load the data.table library 
library(data.table)

# Create vectors to later store in a data frame
x <- c('I', 'I', 'I', 'II', 'II', 'II', 'III', 'III', 'III', 'IV', 'IV', 'IV')
y <- c('a', 'a', 'b', 'b', 'c', 'c', 'd', 'd', 'e', 'e', 'f', 'f')
z <- rnorm(12)

# create a data table
dt <- data.table(x, y, z)
dt
```

### Subset rows in `i`

* There are many ways to select rows.

* Set `i` equal to a vector of integers.

* Set `i` equal to a logical expression.

* It will sometimes be useful to use the `%in%` operator
when specifying `i`.
  
```{r}
# select rows by passing integer indices
dt[1:4]
dt[4:7]
dt[c(2, 4)]
dt[c(2, 5, 11)]

# select rows by passing a logical expression
dt[x=='II']
dt[(x=='II') & (y=='c')]
dt[(x=='II') & (y=='c') & (z > 0)]

# select rows by implementing a logical expression using
# `chaining`
dt[x=='II'][y=='c'][z > 0]

# select rows using the `%in%` operator
dt[x %in% c('I', 'II')]
dt[y %in% c('b', 'f')]

# Notice that using the `%in%` operator is essentially
# performing an *or* logical operation
dt[(x=='I') | (x=='II')]
dt[(y=='b') | (y=='f')]
```

* If you want to operate on every row of some set of
columns, leave the `i` argument blank.

* This will result in the first character inside the square
brackets being a comma, which looks a bit strange if you're
not used to it. Just remember to read it as *select every row*.

* `data.table` also provides a keyword `.N` that can be
useful in a number of ways. One such way is to use it in
selecting rows as shown in the next code chunk.

```{r}
dt # select every row
dt[,] # select every row
dt[1:.N] # select every row using the .N keyword
dt[2:.N] # select all but the first row using the .N keyword
```

### Select columns with `j`
* To operate on specific columns, set `j` equal to a list
of the names of the columns you want.

* Inside the square brackets of a `data.table`, `list` can
be abbreviated with a `.`, which can also look strage at
first. Once you get used to it, you will appreciate the
brevity.

### Select and operate on columns with `j`
* You can do a lot more than just return the columns
specified in `j`. In fact, you can perform any operation you
want on them.

```{r}
# Select every row of columns y and z the long way
dt[, list(y, z)]

# Select every row of columns y and z the cool way
dt[, .(y, z)]

# select rows 1:4 and return the z column of the result as a
# vector
dt[1:4, z]

# select rows 1:4 and return the z column of the result as a
# data.table
dt[1:4, list(z)]

# select rows 1:4 and return the z column of the result as a
# data.table, but use `.` to abbreviate `list`. This is a
# thing specific to data.table.
dt[1:4, .(z)]

# select and operate on columns
# select rows 1:4 and return a summary statistic of the
# resulting z column
dt[1:4, mean(z)]
dt[1:4, sd(z)]

# notice the default naming of the columns in the following
# result
dt[1:4, list(mean(z), sd(z))]

# control the names of the resulting columns
dt[1:4, list(z_mean = mean(z), z_sd = sd(z))]

# select all rows and compute summary statistics of the
# resulting data.table
dt[, .(mean(z), sd(z))] # default column names
dt[, list(z_mean=mean(z), z_sd=sd(z))] # custom column names
```


### Combine `i` and `j`

* It is straightforward to combine `i` and `j`.

```{r}
# Select all rows for which x==II and return columns y and z
dt[x=='II', .(y, z)]
```


### Group using `by`

* The real magic of `data.table` comes in the power of the
`by` argument.

* `by` allows you apply the operation that you specify with
`j` to separate groups of rows defined by the unique values
in the columns specified in `by`.

* Put another way, whatever column you pass to `by` will be
split up into groups with each unique value getting its own
group. Those groups will then be applied to the columns you
specify in `j` and the operation also specified in `j` will
be applied only to rows from the same group. Then, after all
is done, everything is put back into a single `data.table`.

```{r}
# inefficient and tiring way to group by x
dt[x=="I", mean(z)]
dt[x=="II", mean(z)]
dt[x=="III", mean(z)]
dt[x=="IV", mean(z)]

# efficient and beautiful way to group by x
dt[, mean(z), .(x)]

# the efficient and beautiful way of doing things extends to
# grouping by multiple variables with ease
dt[, mean(z), .(x, y)]

# return more than one summary statistic grouped by x
dt[, .(mean(z), sd(z)), .(x)] # default naming of resulting columns
dt[, .(z_mean=mean(z), z_sd=sd(z)), .(x)] # custom naming of resulting columns
```

### Adding and modifying columns

* Use the `:=` operator inside the `j` argument of a
`data.table` to add or modify a column.

```{r, error=T}
# if you pass a single number, data.table will fill the
# entire column with that value
dt[, a := 9]

# otherwise, just pass a vector of the same length as the
# data.table
dt[, b := seq(2, 24, 2)]

# notice that you get an error if you try to create a column
# using a vector that isn't exactly the correct length
dt[, b := seq(2, 26, 2)]
# redefine or modify an existing column the same way
dt[, b := seq(4, 48, 4)]

# remove a column using := and the NULL keyword
dt[, b := NULL]

# don't use the `<-` operator and the `:=`operator at the
# same time. It works but is inefficient
dt <- dt[, c := 9] # don't do this

# it all works the same with non-numerical columns
dt[, betsy := 'mouse']
dt[, betsy := c('mouse', 'rat')] # notice the error
new_col_betsy <- rep(c('mouse', 'rat'), 6)
dt[, betsy := new_col_betsy] # add betsy
```

### Be careful when copying `data.table` objects

* `dt2 = dt` creates a *shallow copy*. This means that there
is really only one `data.table` object in memory, but it can
be referred to by both names.

* This means that changes to `dt2` will be reflected in `dt`
and vice-versa.

* You can show that this is the case by modifying dt2 and
observing that dt also changes (see code chunks below).

* To create a *deep copy* use `dt2 = data.table(dt)`.

```{r}
# print dt to see what it looks like before messing with it
dt 

# this is a shallow copy
dt_copy = dt
dt_copy[, a := NULL]

# notice we also deleted the `a` column from dt, not just from
# dt_copy
dt

# this is a deep copy
dt_copy = data.table(dt)
dt_copy[, x := NULL]

# now our changes to dt_copy didn't also change dt
dt
```