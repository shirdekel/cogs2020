---
title: "Lecture 4 - Binomial and Normal distributions"
author: "Author: Matthew J. Cossley"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float:
        collapsed: false
        smooth_scroll: true
    toc_depth: 2
    fig_caption: yes
    # code_folding: show
    number_sections: false
    theme: cosmo
fontsize: 14pt
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Learning objectives

* Define and understand Bernoulli random variables.

* Define and understand Binomial random variables.

* Define and understand Normal random variables.

* Understand how, in principle, to compute expected values
from continuous and discrete random variables.

* Be able to read simple expected values and probabilities
from probability mass and density functions, and cumulative
distribution functions.

* Use `R` functions to compute probabilities from binomial
and normal distributions.

## Bernoulli random variable

### Definition
A single sample from a any random variable that produces a
dichotomous outcome is formally defined as a *Bernoulli*
trial. To be a Bernoulli trial, a few conditions must be
met:

(1) Each trial yields one of the two outcomes usually called
success ($S$) and failure ($F$).

(2) For each trial, the probability of success $P(S)$ is the
same and is denoted by $p = P(S)$. The probability of
failure is then $q = P(F) = 1 - P(S)$ for each trial.

(3) Trials are **independent**. The probability of success
in a trial does not change given any amount of information
about the outcomes of other trials.

Bernoulli random variables are completely defined by the
single parameter $p = P(S)$. If $X$ is a Bernoulli random
variable then we would write:

$X \sim Bernoulli(p)$

### Probability distribution
The probability distribution of a Bernoulli random variable
is very simple. We simply need to indicate two probabilities
as follows.

```{r, echo=F}
library(data.table)
library(ggplot2)

rm(list=ls())

# Specify the probability of success
p <- 0.3

# On the y-axis, we want to plot the probability of the
# possible outcomes (S and F)
y <- c(0.3, 0.7)

# On the x-axis, we want to indicate the outcome names
x <- c('S', 'F')

# To plot things with ggplot we need our data in a
# data.table
d <- data.table(x, y)

ggplot(d, aes(x, y)) +
  geom_segment(aes(xend=x, yend=0)) +
  xlab('X = Bernoulli') +
  ylab('Probability') +
  theme(aspect.ratio = 1)
```


## Binomial random variable
### Definition
Consider a fixed number $n$ of Bernoulli trials conducted
with success probability $P(S)=p$ and failure probability
$P(F)=q=1-p$ in each trial. Define the random variable $X$
as follows:

$X =$ the sum of the number of Successes from $n$ of the
above Bernoulli trials.

Then $X$ is called a binomial distribution with $n$ trials
and success probability $p$. The binomial distribution is
completely defined by two parameters, $n$ and $p$, and we
can write:

$X \sim Binomial(n,p)$

### Probability distribution

#### Population parameters: n=3, p=0.5

Define the random variable $X$ as follows:

$X$ = the number of *heads* in $3$ flips of a *fair* coin.

Then $X \sim Binomial(n=2, p=0.5)$ 

To obtain the probability distribution, we must compute the
probability of every possible outcome that $X$ can produce.
With $n=2$ flips, it is easy to see that there are 3
possible outcomes (0, 1, or 2 heads) which are obtained with
the following possible events and corresponding
probabilities:

\begin{align}
P(X=0) &= P(TT) \\
       &= P(T)P(T) \\
       &= 0.5 \times 0.5 \\
       &= 0.25 \\\\
P(X=1) &= P(TH) + P(HT) \\
       &= P(T)P(H) + P(H)P(T) \\
       &= 0.5 \times 0.5 + 0.5 \times 0.5 \\
       &= 0.25 + 0.25 \\
       &= 0.5 \\\\
P(X=2) &= P(HH) \\
       &= P(H)P(H) \\
       &= 0.5 \times 0.5 \\
       &= 0.25
\end{align}

Above, we were able to write $P(TT)=P(T)P(T)$ because coin
flips are statistically **independent**.

In simple cases like this it's no big deal to compute the
probabilities or all possible events by hand, but this
approach quickly becomes intractable in real world
situations. Luckily, `R` provides functions for essentially
all the probability distributions you will likely ever find
yourself using. In the case of a binomial distribution, we
will use the `dbinom()` function as illustrated below.

```{r, echo=F}
# set Binomial parameter values
n <- 2
p <- 0.5

# plot binomial probability distribution
x <- 0:n
y <- dbinom(x, n, p)
d <- data.table(x,y)
ggplot(d, aes(x=x,y=y)) +
  geom_point() +
  geom_segment(aes(xend=x, yend=0)) +
  xlab('X = number successes in n=3 Bernoulli trials') +
  ylab('Probability') +
  theme(aspect.ratio = 1)
 
```


#### Population parameters: function of n and p

Now lets run through this exercise with different $n$ and
$p$ values.  Below, notice that when $p=0.1$, the
distribution is skewed to the left. This makes sense because
if the probability of success is very small, then the
probability of achieving zero or only few successes is high.
Similarly, notice that when $p=0.9$, the distribution is
skewed to the right. The same logic applied in the previous
sentence applies here. If the probability of success is very
high, then we should expect many successes, and most of the
probability will correspond to larger outcomes.

```{r, echo=F}

# Explore 3 levels each of n and p
n_levels <- c(5, 15, 30)
p_levels <- c(.1, .5, .9)

# Below, we use a couple of for loops to iterate through the
# possible combinations of n and p defined above. We will
# want to plot them all at once, so we will store each
# probability distribution computed in a list (d_list) for
# later use.
d_list = vector('list', 9)
for(i in 1:3) {
  for(j in 1:3) {
    n <- n_levels[i]
    p <- p_levels[j]
    x <- 0:n
    y <- dbinom(0:n, n, p)
    d <- data.table(n, p, x, y)
    
    # On the following line, j + 3*(i-1) is a method to
    # convert two indices (i and j) into a single index.
    # This is necessary because d_list is one-deminsional.
    d_list[[j + 3*(i-1)]] = d
  }
}

# If you have a bunch of data.tables with the same format
# stored in a list then you can use the function rbindlist()
# to stack them into a single longer data.table
d <- rbindlist(d_list)

ggplot(d, aes(x=x, y=y)) +
    geom_point() +
    geom_segment(aes(xend=x, yend=0)) +
    xlab('X = number successes in n Bernoulli trials') +
    ylab('Probability') +
    facet_grid(n~p)

```

## Normal distribution

The normal distribution is defined by two parameters:

- $\mu$: population mean
- $\sigma$: population standard deviation

It is just a quirk of the normal distribution that the
parameters that define it correspond to its mean and
standard deviation. This will not generally be true for most
other distributions that we will deal with.

### Probability distribution

The normal distribution is continuous, and the continuous
probability distributions are called probability density
functions (pdfs).

The following equation defines the pdf of a normal
distribution:

$$
f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2}{\frac{(x - \mu)^2}{\sigma}}}
$$

#### The standard normal (Z)

The *standard normal* distribution is a normal distribution
with zero mean and unit variance ($\mu=0$, $\sigma^2=1$). It
is also called the $Z$ distribution. It looks like this:

```{r, echo=F}

mu <- 0
sigma <- 1

x <- seq(-5, 5, 0.01)
fx <- dnorm(x, mu, sigma)
d <- data.table(x, fx)
ggplot(d, aes(x, fx)) +
  geom_line()

```


#### Normal distribution with different parameters

Lets have a look at how the normal distribution changes with
different mean and variance.

```{r, echo=F}

## Lets more formally examine the of effect different values of n and p
mean_levels <- c(-25, 0, 25)
var_levels <- c(1, 5, 10)

x <- seq(-50, 50, 0.1)

## Watch out: `for()` loops ahead!
mean_rec <-  c()
var_rec <- c()
x_rec <- c()
p_norm_rec <- c()
for(mu in mean_levels) {
    for(sigma in var_levels) {
        mean_rec <- c(mean_rec, rep(mu, length(x)))
        var_rec <- c(var_rec, rep(sigma, length(x)))
        x_rec <- c(x_rec, x)
        p_norm_rec <- c(p_norm_rec, dnorm(x, mu, sigma))
    }
}

d <- data.table(mean=mean_rec, var=var_rec, x=x_rec, p_norm=p_norm_rec)

ggplot(d, aes(x=x, y=p_norm)) +
    geom_line() +
    xlab('X') +
    ylab('Probability Density') +
    facet_grid(mean~var)

```

## Central tendency

We have previously seen that central tendency is given by
the expected value of a random variable, and that for a
discrete a random variable, expected value is given by the
following equation:

\begin{align}
E(X) &= \mu \\
     &= x_1 p(x_1) + x_2 p(x_2) + \ldots + x_n p(x_n) \\
     &= \sum_{i}^{n} x_i p(x_i)
\end{align}

When dealing with continuous random variables, this equation
becomes the following:

\begin{align}
E(X) &= \mu \\
     &= \int_{a}^{b} f(x) dx
\end{align}

where the possible outcomes of the random variable $X$ are
continuous in the interval $[a, b]$, and $f(x)$ is the pdf
of $X.

For a normal distribution, this becomes

\begin{align}
E(X) &= \mu \\
     &= \int_{-\infty}^{\infty} f(x) dx
\end{align}

because the possible outcomes of a random variable with a
normal distribution are continuous in the interval
$[-\infty, \infty]$.

## Population spread

We saw that for discrete random variables, population
variance is defined as the expected squared deviation from
the mean, as given by the following equation:

\begin{align}
Var(X) &= \sigma^2 \\
       &= E((X - \mu)^2) \\
       &= \sum_i x_{i}^2 p(X=x_{i}) - \mu^2
\end{align}

For continuous random variables, this equation becomes the
following:

\begin{align}
Var(X) &= \sigma^2 \\
       &= E((X - \mu)^2) \\
       &= \int_{a}^{b} (x - \mu)^2 f(x) dx
\end{align}

For a normal distribution, this becomes:

\begin{align}
Var(X) &= \sigma^2 \\
       &= E((X - \mu)^2) \\
       &= \int_{-\infty}^{\infty} (x - \mu)^2 f(x) dx
\end{align}

because the possible outcomes of a random variable with a
normal distribution are continuous in the interval
$[-\infty, \infty]$.


<div style="color:#990000">
**Computing expect values (e.g., population means and
population variances) from continuous random variables
required the evauluation of integrals. I won't expect that
you do that in this course. Rather, it is important that you
understand how to estimate these things from plots.**
<br></br>
</div>

## Computing Probabilities

With discrete probability distribution, we were able to
compute the probability of certain events by counting the
total number of ways for an outcome to occur and dividing by
the total number of possible outcomes. However, for
*continuous* probability distributions, this methods doesn't
work. To see this, consider a Normal distribution. We first
need to count the number of ways for an outcome to occur.
Suppose we are interested in $p(x<1)$. How many ways are
there for $x$ to be less than $1$? Since $x$ is continuous,
there are *infinite* ways for this occur! What about the
total number of possible outcomes? Right. Infinity again. So
we are left dividing one infinity by another infinity. Quite
a confusing and ill-defined situation.

<div class = "row">
<div class = "col-md-6">

### Discrete distributions

With discrete probability distributions -- also called
probability mass functions (pmfs) -- finite and well-defined
probabilities exist for every single possible outcome. This
means that you can plot the distribution, and read
probabilities right off the y-axis of the plot.

#### Binomial pmf

```{r, echo=F}

x <- seq(0, 15, 1)
fx <- dbinom(x, 15, 0.5)
d <- data.table(x=x, fx=fx)

ggplot(d, aes(x, fx)) +
    geom_point() +
    ylab('Probability') +
    theme(
        axis.text=element_text(size=18),
        axis.title=element_text(size=24)
    )

```

</div>
<div class = "col-md-6">

### Continuous distributions

With continuous probability distributions -- aslo called
probability density functions (pdfs) -- the probability of
any single outcome is zero. The distribution represents
*probability density* rather than probability. In this case,
probability is computed by taking the area under the curve.

#### Normal pdf

```{r, echo=F}

x <- seq(-5, 5, 0.01)
fx <- dnorm(x, 0, 1)
d <- data.table(x=x, fx=fx)

ggplot(d, aes(x, fx)) +
    geom_line() +
    ylab('Probability Density') +
    theme(
        axis.text=element_text(size=18),
        axis.title=element_text(size=24)
    )

```

</div>
</div>

<div class = "row">
<div class = "col-md-6">

#### Binomial example: P(X < 8)

```{r, echo=F}

x <- seq(0, 15, 1)
fx <- dbinom(x, 15, 0.5)
d <- data.table(x=x, fx=fx)
d[, px := x < 8]

ggplot(d, aes(x,
              fx,
              colour=factor(px),
              fill=factor(px))) +
    geom_point() +
    geom_segment(data=d[px==TRUE],
                 aes(x=x,
                     xend=x,
                     yend=fx,
                     y=0)) +
    ylab('Probability') +
    theme(
        axis.text=element_text(size=18),
        axis.title=element_text(size=24),
        legend.position='none'
    )

```

$$ p(X<8) = \sum_{i=0}^7 p(x_{i}) $$

</div>
<div class = "col-md-6">

#### Normal example: P(X < 1)

```{r, echo=F}

x <- seq(-5, 5, 0.01)
fx <- dnorm(x, 0, 1)
d <- data.table(x=x, fx=fx)
d[, px := x < 1]

ggplot(d, aes(x,
              fx,
              colour=factor(px),
              fill=factor(px))) +
    geom_line() +
    geom_ribbon(data=d[px==TRUE],
                aes(x=x,
                    ymin=0,
                    ymax=fx),
                alpha=0.3) +
    ylab('Probability Density') +
    theme(
        axis.text=element_text(size=18),
        axis.title=element_text(size=24),
        legend.position='none'
    )

```

$$ p(X<1) = \int_{-\inf}^{1} f(x) dx $$

</div>
</div>


### Using R to compute probabilities

For most probability distributions, R has 4 built-in
functions that tell you almost everything you will ever want
to know about them.

For the Binomial distribution, these functions are the
following:

* `dbinom(x)`: Probability mass function

* `pbinom(x)`: Cumulative distribution function

* `qbinom(p)`: quantile function

* `rbinom(n)`: function for random samples

For the Normal distributions, these functions are the
following:

* `dnorm(x)`: Probability density function

* `pnorm(x)`: Cumulative distribution function

* `qnorm(p)`: quantile function

* `rnorm(n)`: function for random samples

You can see the naming convention adopted by R right away.

* The `d` functions are mass or density functions.

  * For discrete distributions, these functions return $P(X=x)$.
  E.g., `dbinom(x, n, p)` returns the probability that a
  binomial random variable with parameters $n$ and $p$ will
  yield a value of $x$.

  * For continuous distributions, these functions return a
  probability density. To get probability, we must consider a
  range of outcomes $[a, b]$ and compute the area under the
  curve. Computing the exact area under the curve requires
  evaluating an integral, which is too hard for us and awkward
  to do using a `d` function. It will be best to use a `p`
  function in this case (see below).
  
* The `p` functions are cumulative probability functions.

  * By default these functions return $P(X \leq x)$. If you
  specify `lower.tail=FALSE` then these functions return
  $P(X>x)$. Be careful when using these functions to
  appreciate that for continuous distributions
  $P(X \leq x)=1-P(X \geq x)$ but for discrete distributions
  $P(X \leq x)=1-P(X \geq x+1)$. All that is basically just
  to say be careful when considering whether to use greater
  than or greather than and equal to etc.

* The `q` functions are quantile functions. 

  * They are the inverse of the cumulative probability
  functions. Here, you specify a cumulative probability $q$,
  and the function returns the value of $x$ such that
  $P(X<x)=q$.

* The `r` functions generate random samples.


</div>
<div class = "col-md-6">
**pmf**: probabilities are given by values on the y-axis.
```{r, echo=F}
n <- 15
p <- 0.5
x <- seq(0, n, 1)
pdf <- data.table(id='pdf', x=x, fx=dbinom(x, n, p))
ggplot(pdf, aes(x, fx)) + 
  geom_point() +
  xlab('x') +
  ylab('P(X=x)') +
  theme(text = element_text(size=20))
```

**cdf**: cumulative probabilities (X<x) are given by reading
values on the y-axis.
```{r, echo=F}
cdf <- data.table(id='cdf', x=x, fx=pbinom(x, n, p))
ggplot(cdf, aes(x, fx)) + 
  geom_point()+
  xlab('x') +
  ylab('P(X <= x)') +
  theme(text = element_text(size=20))
```

**qf**: use this function to specify a probability (x-axis)
and get the value that satisfies this probability from the
y-axis.
```{r, echo=F}
p <- seq(0.01, 0.99, 0.001)
qf <- data.table(id='qf', x=p, fx=qbinom(p, n, p))
ggplot(qf, aes(x, fx)) + 
  geom_point() +
  xlab('P(X <= x)') +
  ylab('x') +
  theme(text = element_text(size=20))
```
</div>

<div class = "col-md-6">
**pdf**: probabilities are given by the area under the
curve.
```{r, echo=F}
x <- seq(-5, 5, 0.01)
pdf <- data.table(id='pdf', x=x, fx=dnorm(x))
ggplot(pdf, aes(x, fx)) + 
  geom_line() +
  xlab('x') +
  ylab('Prob Dens. (x)') +
  theme(text = element_text(size=20))
```

**cdf**: cumulative probabilities (X<x) are given by reading
values on the y-axis.
```{r, echo=F}
cdf <- data.table(id='cdf', x=x, fx=pnorm(x))
ggplot(cdf, aes(x, fx)) + 
  geom_line() +
  xlab('x') +
  ylab('P(X <= x)') +
  theme(text = element_text(size=20))
```

**qf**: use this function to specify a probability (x-axis)
and get the value that satisfies this probability from the
y-axis.
```{r, echo=F}
p <- seq(0.01, 0.99, 0.001)
qf <- data.table(id='qf', x=p, fx=qnorm(p))
ggplot(qf, aes(x, fx)) + 
  geom_line() +
  xlab('P(X <= x') +
  ylab('x') +
  theme(text = element_text(size=20))
```
</div>
</div>

<div style="color:#990000">
**In general, I think it is important for you to be able to
read each of the types of plots above, so please really try
to encode these and think about them deeply.**
<br></br>
</div>

<div class = "row">
<div class = "col-md-6">

#### Binomial example: P(X < 8)

```{r, echo=F}

x <- seq(0, 15, 1)
fx <- dbinom(x, 15, 0.5)
d <- data.table(x=x, fx=fx)
d[, px := x < 8]

ggplot(d, aes(x,
              fx,
              colour=factor(px),
              fill=factor(px))) +
    geom_point() +
    geom_segment(data=d[px==TRUE],
                 aes(x=x,
                     xend=x,
                     yend=fx,
                     y=0)) +
    ylab('Probability') +
    theme(
        axis.text=element_text(size=18),
        axis.title=element_text(size=24),
        legend.position='none'
    )

```

```{r}
n <- 15
p <- 0.5

## Using pmf (exact)
px <- sum(dbinom(0:7, n, p))
px

## using cdf (exact)
px <- pbinom(7, n, p)
px
```
</div>

<div class = "col-md-6">

#### Normal example: P(X < 1)

```{r, echo=F}

x <- seq(-5, 5, 0.01)
fx <- dnorm(x, 0, 1)
d <- data.table(x=x, fx=fx)
d[, px := x < 1]

ggplot(d, aes(x,
              fx,
              colour=factor(px),
              fill=factor(px))) +
    geom_line() +
    geom_ribbon(data=d[px==TRUE],
                aes(x=x,
                    ymin=0,
                    ymax=fx),
                alpha=0.3) +
    ylab('Probability Density') +
    theme(
        axis.text=element_text(size=18),
        axis.title=element_text(size=24),
        legend.position='none'
    )

```

```{r}
mu <- 0
sig <- 1

## Using pdf (not exact)
px <- sum(dnorm(seq(-5, 1, .1), mu, sig)*.1)
px

## using cdf (exact)
px <- pnorm(1, mu, sig)
px
```

</div>
</div>



<div class = "row">
<div class = "col-md-6">

#### Binomial example: P(X > 9)

```{r, echo=F}

x <- seq(0, 15, 1)
fx <- dbinom(x, 15, 0.5)
d <- data.table(x=x, fx=fx)
d[, px := x > 9]

ggplot(d, aes(x,
              fx,
              colour=factor(px),
              fill=factor(px))) +
    geom_point() +
    geom_segment(data=d[px==TRUE],
                 aes(x=x,
                     xend=x,
                     yend=fx,
                     y=0)) +
    ylab('Probability') +
    theme(
        axis.text=element_text(size=18),
        axis.title=element_text(size=24),
        legend.position='none'
    )

```

```{r}
n <- 15
p <- 0.5

## Using pmf (exact)
px <- sum(dbinom(10:n, n, p))
px

## using cdf (exact)
px <- pbinom(9, n, p, lower.tail=F)
px
```
</div>

<div class = "col-md-6">

#### Normal example: P(X > 1.8)

```{r, echo=F}

x <- seq(-5, 5, 0.01)
fx <- dnorm(x, 0, 1)
d <- data.table(x=x, fx=fx)
d[, px := x > 1.8]

ggplot(d, aes(x,
              fx,
              colour=factor(px),
              fill=factor(px))) +
    geom_line() +
    geom_ribbon(data=d[px==TRUE],
                aes(x=x,
                    ymin=0,
                    ymax=fx),
                alpha=0.3) +
    ylab('Probability Density') +
    theme(
        axis.text=element_text(size=18),
        axis.title=element_text(size=24),
        legend.position='none'
    )

```


```{r}
mu <- 0
sig <- 1

## Using pdf (not exact)
p <- sum(dnorm(seq(1.8, 5, .1), mu, sig)*.1)
p

## using cdf (exact)
p <- pnorm(1.8, mu, sig, lower.tail=F)
p
```

</div>



## Central limit theorem

The **central limit theorem** tells us that the sum of
independent and identically distributed random variables
approximates a Normal distribution.

Let 

$\boldsymbol{Y} = \boldsymbol{X}_1 + \boldsymbol{X}_2 + \dots + \boldsymbol{X}_n$

and 

$\boldsymbol{X}_i \sim \boldsymbol{D}$

where $\boldsymbol{D}$ can have any distribution whatsoever.
Then,

$\boldsymbol{Y} \sim N(\mu_{Y}, \sigma_{y}^2)$

where $N$ stands for a *Normal* distribution.

It turns out that we can show:

$\mu_{Y} = \mu_{X}$ 

$\sigma_{y}^2 = \frac{\sigma_{X}^2}{n}$

As a matter of terminology, $\frac{\sigma_{X}}{\sqrt{n}}$
is called the *standard error*.

</div>