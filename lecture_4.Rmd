---
title: "Lecture 4 - Binomial and Normal distributions"
author: "Author: Matthew J. Cossley"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float:
        collapsed: false
        smooth_scroll: true
    toc_depth: 2
    fig_caption: yes
    # code_folding: show
    number_sections: false
    theme: cosmo
fontsize: 14pt
editor_options: 
  chunk_output_type: console
---

```{r setup, include=F, message=F}
knitr::opts_chunk$set(echo = T, collapse=T)
```


## Learning objectives

* Define and understand **Bernoulli** random variables.

* Define and understand **Binomial** random variables.

* Define and understand **Normal** random variables.

* Understand how, in principle, to **compute expected
values** from continuous and discrete random variables.

* Define and understand **probability distributions**
including **probability mass functions**, and **probability
density functions**, **cumulative probability functions**
and **quantile functions**.

* Be able to read simple expected values and probabilities
from probability mass and density functions, and cumulative
distribution functions.

* Understand the **central limit theorem** and the
**distribution of sample means** and how it depends on
sample size.

* Use `R` functions to obtain useful things about binomial
and normal distributions.

  * `dbinom`
  
  * `pbinom`
  
  * `qbinom`
  
  * `rbinom`
  
  * `dnorm`
  
  * `pnorm`
  
  * `qnorm`
  
  * `rnorm`

## Bernoulli random variable

### Definition
A single sample from a any random variable that produces a
dichotomous outcome is formally defined as a *Bernoulli*
trial. To be a Bernoulli trial, a few conditions must be
met:

(1) Each trial yields one of the two outcomes usually called
success ($S$) and failure ($F$).

(2) For each trial, the probability of success $P(S)$ is the
same and is denoted by $p = P(S)$. The probability of
failure is then $q = P(F) = 1 - P(S)$ for each trial.

(3) Trials are **independent**. The probability of success
in a trial does not change given any amount of information
about the outcomes of other trials.

Bernoulli random variables are completely defined by the
single parameter $p = P(S)$. If $X$ is a Bernoulli random
variable then we would write:

$X \sim Bernoulli(p)$

### Probability distribution
The probability distribution of a Bernoulli random variable
is very simple. We simply need to indicate two probabilities
as follows.

```{r, echo=F}
library(data.table)
library(ggplot2)

rm(list=ls())

# Specify the probability of success
p <- 0.3

# On the y-axis, we want to plot the probability of the
# possible outcomes (S and F)
y <- c(0.3, 0.7)

# On the x-axis, we want to indicate the outcome names
x <- c('S', 'F')

# To plot things with ggplot we need our data in a
# data.table
d <- data.table(x, y)

ggplot(d, aes(x, y)) +
  geom_segment(aes(xend=x, yend=0)) +
  xlab('X = Bernoulli') +
  ylab('Probability') +
  theme(aspect.ratio = 1)
```


## Binomial random variable
### Definition
Consider a fixed number $n$ of Bernoulli trials conducted
with success probability $P(S)=p$ and failure probability
$P(F)=q=1-p$ in each trial. Define the random variable $X$
as follows:

$X =$ the sum of the number of Successes from $n$ of the
above Bernoulli trials.

Then $X$ is called a binomial distribution with $n$ trials
and success probability $p$. The binomial distribution is
completely defined by two parameters, $n$ and $p$, and we
can write:

$X \sim Binomial(n,p)$

### Probability distribution

#### Population parameters: n=2, p=0.5

Define the random variable $X$ as follows:

$X$ = the number of *heads* in $2$ flips of a *fair* coin.

Then $X \sim Binomial(n=2, p=0.5)$ 

To obtain the probability distribution, we must compute the
probability of every possible outcome that $X$ can produce.
With $n=2$ flips, it is easy to see that there are 3
possible outcomes (0, 1, or 2 heads) which are obtained with
the following possible events and corresponding
probabilities:

\begin{align}
P(X=0) &= P(TT) \\
       &= P(T)P(T) \\
       &= 0.5 \times 0.5 \\
       &= 0.25 \\\\
P(X=1) &= P(TH) + P(HT) \\
       &= P(T)P(H) + P(H)P(T) \\
       &= 0.5 \times 0.5 + 0.5 \times 0.5 \\
       &= 0.25 + 0.25 \\
       &= 0.5 \\\\
P(X=2) &= P(HH) \\
       &= P(H)P(H) \\
       &= 0.5 \times 0.5 \\
       &= 0.25
\end{align}

Above, we were able to write $P(TT)=P(T)P(T)$ because coin
flips are statistically **independent**.

In simple cases like this it's no big deal to compute the
probabilities or all possible events by hand, but this
approach quickly becomes intractable in real world
situations. Luckily, `R` provides functions for essentially
all the probability distributions you will likely ever find
yourself using. In the case of a binomial distribution, we
will use the `dbinom()` function as illustrated below.

```{r, echo=F}
# set Binomial parameter values
n <- 2
p <- 0.5

# plot binomial probability distribution
x <- 0:n
y <- dbinom(x, n, p)
d <- data.table(x,y)
ggplot(d, aes(x=x,y=y)) +
  geom_point() +
  geom_segment(aes(xend=x, yend=0)) +
  xlab('X = number successes in n=2 Bernoulli trials') +
  ylab('Probability') +
  theme(aspect.ratio = 1)
 
```


#### Population parameters: function of n and p

Now lets run through this exercise with different $n$ and
$p$ values.  Below, notice that when $p=0.1$, the
distribution is skewed to the left. This makes sense because
if the probability of success is very small, then the
probability of achieving zero or only few successes is high.
Similarly, notice that when $p=0.9$, the distribution is
skewed to the right. The same logic applied in the previous
sentence applies here. If the probability of success is very
high, then we should expect many successes, and most of the
probability will correspond to larger outcomes.

```{r, echo=F}

# Explore 3 levels each of n and p
n_levels <- c(5, 15, 30)
p_levels <- c(.1, .5, .9)

# Below, we use a couple of for loops to iterate through the
# possible combinations of n and p defined above. We will
# want to plot them all at once, so we will store each
# probability distribution computed in a list (d_list) for
# later use.
d_list = vector('list', 9)
for(i in 1:3) {
  for(j in 1:3) {
    n <- n_levels[i]
    p <- p_levels[j]
    x <- 0:n
    y <- dbinom(0:n, n, p)
    d <- data.table(n, p, x, y)
    
    # On the following line, j + 3*(i-1) is a method to
    # convert two indices (i and j) into a single index.
    # This is necessary because d_list is one-deminsional.
    d_list[[j + 3*(i-1)]] = d
  }
}

# If you have a bunch of data.tables with the same format
# stored in a list then you can use the function rbindlist()
# to stack them into a single longer data.table
d <- rbindlist(d_list)

ggplot(d, aes(x=x, y=y)) +
    geom_point() +
    geom_segment(aes(xend=x, yend=0)) +
    xlab('X = number successes in n Bernoulli trials') +
    ylab('Probability') +
    facet_grid(n~p)

```

## Normal distribution

The normal distribution is defined by two parameters:

- $\mu$: population mean
- $\sigma$: population standard deviation

It is just a quirk of the normal distribution that the
parameters that define it correspond to its mean and
standard deviation. This will not generally be true for most
other distributions that we will deal with.

### Probability distribution

The normal distribution is continuous, and the continuous
probability distributions are called probability density
functions (pdfs).

The following equation defines the pdf of a normal
distribution:

$$
f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2}{\frac{(x - \mu)^2}{\sigma}}}
$$

#### The standard normal (Z)

The *standard normal* distribution is a normal distribution
with zero mean and unit variance ($\mu=0$, $\sigma^2=1$). It
is also called the $Z$ distribution. It looks like this:

```{r, echo=F}

mu <- 0
sigma <- 1

x <- seq(-5, 5, 0.01)
fx <- dnorm(x, mu, sigma)
d <- data.table(x, fx)
ggplot(d, aes(x, fx)) +
  geom_line()

```


#### Normal distribution with different parameters

Lets have a look at how the normal distribution changes with
different mean and variance.

```{r, echo=F}

## Lets more formally examine the of effect different values of n and p
mean_levels <- c(-25, 0, 25)
var_levels <- c(1, 5, 10)

x <- seq(-50, 50, 0.1)

## Watch out: `for()` loops ahead!
mean_rec <-  c()
var_rec <- c()
x_rec <- c()
p_norm_rec <- c()
for(mu in mean_levels) {
    for(sigma in var_levels) {
        mean_rec <- c(mean_rec, rep(mu, length(x)))
        var_rec <- c(var_rec, rep(sigma, length(x)))
        x_rec <- c(x_rec, x)
        p_norm_rec <- c(p_norm_rec, dnorm(x, mu, sigma))
    }
}

d <- data.table(mean=mean_rec, var=var_rec, x=x_rec, p_norm=p_norm_rec)

ggplot(d, aes(x=x, y=p_norm)) +
    geom_line() +
    xlab('X') +
    ylab('Probability Density') +
    facet_grid(mean~var)

```

<!-- ## Central tendency -->

<!-- We have previously seen that central tendency is given by -->
<!-- the expected value of a random variable, and that for a -->
<!-- discrete a random variable, expected value is given by the -->
<!-- following equation: -->

<!-- \begin{align} -->
<!-- \mathbb{E}\big[X\big] &= \mu \\ -->
<!--      &= x_1 p(x_1) + x_2 p(x_2) + \ldots + x_n p(x_n) \\ -->
<!--      &= \sum_{i}^{n} x_i p(x_i) -->
<!-- \end{align} -->

<!-- When dealing with continuous random variables, this equation -->
<!-- becomes the following: -->

<!-- \begin{align} -->
<!-- \mathbb{E}\big[X\big] &= \mu \\ -->
<!--      &= \int_{a}^{b} f(x) dx -->
<!-- \end{align} -->

<!-- where the possible outcomes of the random variable $X$ are -->
<!-- continuous in the interval $[a, b]$, and $f(x)$ is the pdf -->
<!-- of $X$. -->

<!-- For a normal distribution, this becomes -->

<!-- \begin{align} -->
<!-- \mathbb{E}\big[X\big] &= \mu \\ -->
<!--      &= \int_{-\infty}^{\infty} f(x) dx -->
<!-- \end{align} -->

<!-- because the possible outcomes of a random variable with a -->
<!-- normal distribution are continuous in the interval -->
<!-- $[-\infty, \infty]$. -->

<!-- ## Population spread -->

<!-- We saw that for discrete random variables, population -->
<!-- variance is defined as the expected squared deviation from -->
<!-- the mean, as given by the following equation: -->

<!-- \begin{align} -->
<!-- \mathbb{Var}\big[X\big] &= \sigma^2 \\ -->
<!--        &= E((X - \mu)^2) \\ -->
<!--        &= \sum_i x_{i}^2 p(X=x_{i}) - \mu^2 -->
<!-- \end{align} -->

<!-- For continuous random variables, this equation becomes the -->
<!-- following: -->

<!-- \begin{align} -->
<!-- \mathbb{Var}\big[X\big] &= \sigma^2 \\ -->
<!--        &= E((X - \mu)^2) \\ -->
<!--        &= \int_{a}^{b} (x - \mu)^2 f(x) dx -->
<!-- \end{align} -->

<!-- For a normal distribution, this becomes: -->

<!-- \begin{align} -->
<!-- \mathbb{Var}\big[X\big] &= \sigma^2 \\ -->
<!--        &= E((X - \mu)^2) \\ -->
<!--        &= \int_{-\infty}^{\infty} (x - \mu)^2 f(x) dx -->
<!-- \end{align} -->

<!-- because the possible outcomes of a random variable with a -->
<!-- normal distribution are continuous in the interval -->
<!-- $[-\infty, \infty]$. -->


<!-- <div style="color:#990000"> -->
<!-- **Computing expect values (e.g., population means and -->
<!-- population variances) from continuous random variables -->
<!-- required the evauluation of integrals. I won't expect that -->
<!-- you do that in this course. Rather, it is important that you -->
<!-- understand how to estimate these things from plots.** -->
<!-- <br></br> -->
<!-- </div> -->

## Computing Probabilities

With discrete probability distribution, we were able to
compute the probability of certain events by counting the
total number of ways for an outcome to occur and dividing by
the total number of possible outcomes. However, for
*continuous* probability distributions, this methods doesn't
work. To see this, consider a Normal distribution. We first
need to count the number of ways for an outcome to occur.
Suppose we are interested in $p(x<1)$. How many ways are
there for $x$ to be less than $1$? Since $x$ is continuous,
there are *infinite* ways for this occur! What about the
total number of possible outcomes? Right. Infinity again. So
we are left dividing one infinity by another infinity. Quite
a confusing and ill-defined situation.

<div class = "row">
<div class = "col-md-6">

### Discrete distributions

With discrete probability distributions -- also called
probability mass functions (pmfs) -- finite and well-defined
probabilities exist for every single possible outcome. This
means that you can plot the distribution, and read
probabilities right off the y-axis of the plot.

#### Binomial pmf

```{r, echo=F}

x <- seq(0, 15, 1)
fx <- dbinom(x, 15, 0.5)
d <- data.table(x=x, fx=fx)

ggplot(d, aes(x, fx)) +
    geom_point() +
    ylab('Probability') +
    theme(
        axis.text=element_text(size=18),
        axis.title=element_text(size=24)
    )

```

</div>
<div class = "col-md-6">

### Continuous distributions

With continuous probability distributions -- aslo called
probability density functions (pdfs) -- the probability of
any single outcome is zero. The distribution represents
*probability density* rather than probability. In this case,
probability is computed by taking the area under the curve.

#### Normal pdf

```{r, echo=F}

x <- seq(-5, 5, 0.01)
fx <- dnorm(x, 0, 1)
d <- data.table(x=x, fx=fx)

ggplot(d, aes(x, fx)) +
    geom_line() +
    ylab('Probability Density') +
    theme(
        axis.text=element_text(size=18),
        axis.title=element_text(size=24)
    )

```

</div>
</div>

<div class = "row">
<div class = "col-md-6">

#### Binomial example: P(X < 8)

```{r, echo=F}

x <- seq(0, 15, 1)
fx <- dbinom(x, 15, 0.5)
d <- data.table(x=x, fx=fx)
d[, px := x < 8]

ggplot(d, aes(x,
              fx,
              colour=factor(px),
              fill=factor(px))) +
    geom_point() +
    geom_segment(data=d[px==TRUE],
                 aes(x=x,
                     xend=x,
                     yend=fx,
                     y=0)) +
    ylab('Probability') +
    theme(
        axis.text=element_text(size=18),
        axis.title=element_text(size=24),
        legend.position='none'
    )

```

$$ p(X<8) = \sum_{i=0}^7 p(x_{i}) $$

</div>
<div class = "col-md-6">

#### Normal example: P(X < 1)

```{r, echo=F}

x <- seq(-5, 5, 0.01)
fx <- dnorm(x, 0, 1)
d <- data.table(x=x, fx=fx)
d[, px := x < 1]

ggplot(d, aes(x,
              fx,
              colour=factor(px),
              fill=factor(px))) +
    geom_line() +
    geom_ribbon(data=d[px==TRUE],
                aes(x=x,
                    ymin=0,
                    ymax=fx),
                alpha=0.3) +
    ylab('Probability Density') +
    theme(
        axis.text=element_text(size=18),
        axis.title=element_text(size=24),
        legend.position='none'
    )

```

$$ p(X<1) = \int_{-\inf}^{1} f(x) dx $$

</div>
</div>


### Using R to compute probabilities

For most probability distributions, R has 4 built-in
functions that tell you almost everything you will ever want
to know about them.

For the Binomial distribution, these functions are the
following:

* `dbinom(x)`: Probability mass function

* `pbinom(x)`: Cumulative distribution function

* `qbinom(p)`: quantile function

* `rbinom(n)`: function for random samples

For the Normal distributions, these functions are the
following:

* `dnorm(x)`: Probability density function

* `pnorm(x)`: Cumulative distribution function

* `qnorm(p)`: quantile function

* `rnorm(n)`: function for random samples

You can see the naming convention adopted by R right away.

* The `d` functions are mass or density functions.

  * For discrete distributions, these functions return $P(X=x)$.
  E.g., `dbinom(x, n, p)` returns the probability that a
  binomial random variable with parameters $n$ and $p$ will
  yield a value of $x$.

  * For continuous distributions, these functions return a
  probability density. To get probability, we must consider a
  range of outcomes $[a, b]$ and compute the area under the
  curve. Computing the exact area under the curve requires
  evaluating an integral, which is too hard for us and awkward
  to do using a `d` function. It will be best to use a `p`
  function in this case (see below).
  
* The `p` functions are cumulative probability functions.

  * By default these functions return $P(X \leq x)$. If you
  specify `lower.tail=FALSE` then these functions return
  $P(X>x)$. Be careful when using these functions to
  appreciate that for continuous distributions
  $P(X \leq x)=1-P(X \geq x)$ but for discrete distributions
  $P(X \leq x)=1-P(X \geq x+1)$. All that is basically just
  to say be careful when considering whether to use greater
  than or greather than and equal to etc.

* The `q` functions are quantile functions. 

  * They are the inverse of the cumulative probability
  functions. Here, you specify a cumulative probability $q$,
  and the function returns the value of $x$ such that
  $P(X<x)=q$.

* The `r` functions generate random samples.


</div>
<div class = "col-md-6">
**pmf**: probabilities are given by values on the y-axis.
```{r, echo=F}
n <- 15
p <- 0.5
x <- seq(0, n, 1)
pdf <- data.table(id='pdf', x=x, fx=dbinom(x, n, p))
ggplot(pdf, aes(x, fx)) + 
  geom_point() +
  xlab('x') +
  ylab('P(X=x)') +
  theme(text = element_text(size=20))
```

**cdf**: cumulative probabilities (X<x) are given by reading
values on the y-axis.
```{r, echo=F}
cdf <- data.table(id='cdf', x=x, fx=pbinom(x, n, p))
ggplot(cdf, aes(x, fx)) + 
  geom_point()+
  xlab('x') +
  ylab('P(X <= x)') +
  theme(text = element_text(size=20))
```

**qf**: use this function to specify a probability (x-axis)
and get the value that satisfies this probability from the
y-axis.
```{r, echo=F}
p <- seq(0.01, 0.99, 0.001)
qf <- data.table(id='qf', x=p, fx=qbinom(p, n, p))
ggplot(qf, aes(x, fx)) + 
  geom_point() +
  xlab('P(X <= x)') +
  ylab('x') +
  theme(text = element_text(size=20))
```
</div>

<div class = "col-md-6">
**pdf**: probabilities are given by the area under the
curve.
```{r, echo=F}
x <- seq(-5, 5, 0.01)
pdf <- data.table(id='pdf', x=x, fx=dnorm(x))
ggplot(pdf, aes(x, fx)) + 
  geom_line() +
  xlab('x') +
  ylab('Prob Dens. (x)') +
  theme(text = element_text(size=20))
```

**cdf**: cumulative probabilities (X<x) are given by reading
values on the y-axis.
```{r, echo=F}
cdf <- data.table(id='cdf', x=x, fx=pnorm(x))
ggplot(cdf, aes(x, fx)) + 
  geom_line() +
  xlab('x') +
  ylab('P(X <= x)') +
  theme(text = element_text(size=20))
```

**qf**: use this function to specify a probability (x-axis)
and get the value that satisfies this probability from the
y-axis.
```{r, echo=F}
p <- seq(0.01, 0.99, 0.001)
qf <- data.table(id='qf', x=p, fx=qnorm(p))
ggplot(qf, aes(x, fx)) + 
  geom_line() +
  xlab('P(X <= x') +
  ylab('x') +
  theme(text = element_text(size=20))
```
</div>
</div>

<div style="color:#990000">
**In general, I think it is important for you to be able to
read each of the types of plots above, so please really try
to encode these and think about them deeply.**
<br></br>
</div>

<div class = "row">
<div class = "col-md-6">

#### Binomial example: P(X < 8)

```{r, echo=F}

x <- seq(0, 15, 1)
fx <- dbinom(x, 15, 0.5)
d <- data.table(x=x, fx=fx)
d[, px := x < 8]

ggplot(d, aes(x,
              fx,
              colour=factor(px),
              fill=factor(px))) +
    geom_point() +
    geom_segment(data=d[px==TRUE],
                 aes(x=x,
                     xend=x,
                     yend=fx,
                     y=0)) +
    ylab('Probability') +
    theme(
        axis.text=element_text(size=18),
        axis.title=element_text(size=24),
        legend.position='none'
    )

```

```{r}
n <- 15
p <- 0.5

## Using pmf (exact)
px <- sum(dbinom(0:7, n, p))
px

## using cdf (exact)
px <- pbinom(7, n, p)
px
```
</div>

<div class = "col-md-6">

#### Normal example: P(X < 1)

```{r, echo=F}

x <- seq(-5, 5, 0.01)
fx <- dnorm(x, 0, 1)
d <- data.table(x=x, fx=fx)
d[, px := x < 1]

ggplot(d, aes(x,
              fx,
              colour=factor(px),
              fill=factor(px))) +
    geom_line() +
    geom_ribbon(data=d[px==TRUE],
                aes(x=x,
                    ymin=0,
                    ymax=fx),
                alpha=0.3) +
    ylab('Probability Density') +
    theme(
        axis.text=element_text(size=18),
        axis.title=element_text(size=24),
        legend.position='none'
    )

```

```{r}
mu <- 0
sig <- 1

## Using pdf (not exact)
px <- sum(dnorm(seq(-5, 1, .1), mu, sig)*.1)
px

## using cdf (exact)
px <- pnorm(1, mu, sig)
px
```

</div>
</div>



<div class = "row">
<div class = "col-md-6">

#### Binomial example: P(X > 9)

```{r, echo=F}

x <- seq(0, 15, 1)
fx <- dbinom(x, 15, 0.5)
d <- data.table(x=x, fx=fx)
d[, px := x > 9]

ggplot(d, aes(x,
              fx,
              colour=factor(px),
              fill=factor(px))) +
    geom_point() +
    geom_segment(data=d[px==TRUE],
                 aes(x=x,
                     xend=x,
                     yend=fx,
                     y=0)) +
    ylab('Probability') +
    theme(
        axis.text=element_text(size=18),
        axis.title=element_text(size=24),
        legend.position='none'
    )

```

```{r}
n <- 15
p <- 0.5

## Using pmf (exact)
px <- sum(dbinom(10:n, n, p))
px

## using cdf (exact)
px <- pbinom(9, n, p, lower.tail=F)
px
```
</div>

<div class = "col-md-6">

#### Normal example: P(X > 1.8)

```{r, echo=F}

x <- seq(-5, 5, 0.01)
fx <- dnorm(x, 0, 1)
d <- data.table(x=x, fx=fx)
d[, px := x > 1.8]

ggplot(d, aes(x,
              fx,
              colour=factor(px),
              fill=factor(px))) +
    geom_line() +
    geom_ribbon(data=d[px==TRUE],
                aes(x=x,
                    ymin=0,
                    ymax=fx),
                alpha=0.3) +
    ylab('Probability Density') +
    theme(
        axis.text=element_text(size=18),
        axis.title=element_text(size=24),
        legend.position='none'
    )

```


```{r}
mu <- 0
sig <- 1

## Using pdf (not exact)
p <- sum(dnorm(seq(1.8, 5, .1), mu, sig)*.1)
p

## using cdf (exact)
p <- pnorm(1.8, mu, sig, lower.tail=F)
p
```

</div>


## Distribution of sample means

* Every time we draw a sample from any random variable, we
can compute the sample mean of that sample.

* In general, if we repeat that procedure many times, we
will see that every time we draw a sample, we get different
outcomes and therefore different sample means.

* This means that the sample mean is itself a random
variable (i.e., because every time you measure it you get a
different number).

* Lets examine this by considering samples from the Binomial
random variable with distribution shown in the left column
of the following, and samples from the Normal random
variable with distribution shown on the right.

* More precisely, consider the following:

:::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 10px"}

::: {}
$X \sim \mathcal{Binomial}(n=10, p=0.5)$
```{r, echo=F, include=T}
n <- 10
p <- 0.5

x <- 1:n
fx <- dbinom(x, n, p)

dx <- data.table(x, fx)

ggplot(dx, aes(x, fx)) + 
  geom_segment(aes(x=x, xend=x, y=0, yend=fx)) +
  scale_x_continuous(breaks=1:10)
```

```{r echo=F, include=T}
n <- 10
p <- 0.5

sample_1 <- rbinom(10, n, p)
sample_2 <- rbinom(10, n, p)
sample_3 <- rbinom(10, n, p)

d_binom <- data.table(sample_1, 
                      sample_2, 
                      sample_3)

d_binom <- melt(d_binom, 
                measure.vars=c('sample_1', 
                               'sample_2', 
                               'sample_3'),
                variable.name='sample_number',
                value.name='sample_outcome')

d_binom[, 
         sample_mean := mean(sample_outcome), 
        .(sample_number)]

d_binom
```
:::

::: {}
$Y \sim \mathcal{N}({\mu=5, \sigma=2.5})$
```{r, echo=F, include=T}
mu <- n*p
sig <- n*p*(1-p)

y <- seq(mu-3*sig, mu+3*sig, 0.1)
fy <- dnorm(y, mu, sig)

dy <- data.table(y, fy)

ggplot(dy, aes(y, fy)) +
  geom_line()
```

```{r echo=F, include=T}
mu <- 5
sig <- 2.5

sample_1 <- rnorm(10, mu, sig)
sample_2 <- rnorm(10, mu, sig)
sample_3 <- rnorm(10, mu, sig)

d_norm <- data.table(sample_1, 
                     sample_2, 
                     sample_3)

d_norm <- melt(d_norm, 
               measure.vars=c('sample_1', 
                              'sample_2', 
                              'sample_3'),
               variable.name='sample_number',
               value.name='sample_outcome')

d_norm[, 
       sample_mean := mean(sample_outcome), 
       .(sample_number)]

d_norm
```

:::

::::

<!-- <div style="color:#990000"> -->
<!-- **Make sure you understand the code in the above chunks. It -->
<!-- isn't easy, but it is stuff that you are expected to learn -->
<!-- to be able to do on your own quickly and smoothly.** -->
<!-- <br></br> -->
<!-- </div> -->

We see that every time we sample from either random
variable:

(1) We get a sample mean $\bar{\boldsymbol{x}}$ that is
close to the population mean $\mu$, but doesn't match it
exactly unless by dumb luck.

(2) Every sample from $X$ leads to a different value for
$\bar{\boldsymbol{x}}$.

(3) Thus, we have verified that $\bar{\boldsymbol{x}}$ must
itself be a random variable.

Moving forward, we will denote the random variable
corresponding to the distribution of sample means with the
symbol $\bar{X}$, and continue to use $\bar{\boldsymbol{x}}$
to refer to a particular sample mean.

$$
\begin{align}
X & \rightarrow \{x_{1}, \ldots, x_{n}\} \\
\\
\bar{X} & \rightarrow \frac{1}{n} \{ x_{1} + \ldots + x_{n} \}
\end{align}
$$

* Notice that $n>1$ samples from $X$ are needed to generate
a single $n=1$ sample from $\bar{X}$.

* This means that in order to estimate the distribution of
sample means, we need to draw $n>1$ samples from $X$ many
times.

If we perform an experiment in which we draw $n=10$ samples
from $X$ or from $Y$, compute the sample means for each
($\bar{x}$ and $\bar{y}$), and repeat 500 times, then we get
the following estimate for the distribution of sample means:

:::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 10px"}

::: {}
```{r, echo=F, }
num_experiments <- 500
n <- 10
d_list <- vector("list", num_experiments)
for(i in 1:num_experiments) {
  xbar <- mean(rbinom(n, n, p))
  d_list[[i]] <- data.table(xbar=xbar, exp=i)
}
dxbar <- rbindlist(d_list)
dx <- data.table(x=rbinom(num_experiments, n, p)) 
ggplot(dxbar, aes(xbar)) +
    geom_histogram(bins=10)
```
:::

::: {}
```{r, echo=F}
num_experiments <- 500
n <- 10
d_list <- vector("list", num_experiments)
for(i in 1:num_experiments) {
  ybar <- mean(rnorm(n, mu, sig))
  d_list[[i]] <- data.table(ybar=ybar, exp=i)
}
dybar <- rbindlist(d_list)
dy <- data.table(y=rnorm(num_experiments, mu, sig)) 
ggplot(dybar, aes(ybar)) +
    geom_histogram(bins=10)
```
:::

::::

Note that even though $X$ is discrete, $\bar{X}$ is
continuous. We also note that both $\bar{X}$ and $\bar{Y}$
look bell-shaped. This is because of something called the
*central limit theorem*. Before we say more about this very
important theorem, lets investigate how the original
distributions ($X$ and $Y$) compare to their corresponding
distributions of sample means ($\bar{X}$ and $\bar{Y}$) in
terms of central tendancy and spread.

:::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 10px"}

::: {}
```{r, echo=F, }
ggplot() +
  geom_histogram(data=dx, aes(x, fill='red'), bins=10, alpha=0.5) +
  geom_histogram(data=dxbar, aes(xbar, fill='blue'), bins=10, alpha=0.5) +
  scale_x_continuous(breaks=1:10) +
  theme(legend.position="None")
```
:::

::: {}
```{r, echo=F}
ggplot() +
    geom_histogram(data=dy, aes(y, fill='red'), bins=10, alpha=0.5) +
    geom_histogram(data=dybar, aes(ybar, fill='blue'), bins=10, alpha=0.5) +
    scale_x_continuous(breaks=1:10) +
    theme(legend.position="None")
```
:::

:::: 

We can see a few important things from these plots:

* The central tendancy (i.e., mean) of the distribution of
sample means $\bar{X}$ looks to be about the same as that
for the original distribution $X$.

* The spread of the distribution of sample means $\bar{X}$
looks to be smaller than that for the original distribution
$X$.

The central limit theorem helps us formalise both of these
observations.

<!-- It seems that $\bar{X}$ depends on `n` --- where $n$ is -->
<!-- the sample size drawn from $X$ --- in a very important way. -->
<!-- In particular, the population variance of the distribution -->
<!-- of sample means $\sigma_{\bar{X}}^2$ decreases as $n$ -->
<!-- increases. -->

<!-- The following plots show the estimated distribution of -->
<!-- sample means for $n=10$, $n=20$, and $n=30$. -->

```{r, echo=F, message=F, warning=F}
# num_experiments <- 500
# n <- c(10, 20, 30)
# 
# d_list <- vector("list", num_experiments * 3)
# for(j in 1:length(n)) {
#   for(i in 1:num_experiments) {
#     xbar <- mean(rexdist(n[j]))
#     d_list[[i + num_experiments * (j-1)]] <- data.table(xbar=xbar, exp=i, n=n[j])
#   }
# }
# 
# d <- rbindlist(d_list)
# 
# g1 <- ggplot(d[n==10], aes(xbar)) +
#   geom_histogram(bins=10) +
#   xlim(1.5, 3.5) +
#   ggtitle('n=10') +
#   theme(aspect.ratio=1)
# g2 <- ggplot(d[n==20], aes(xbar)) +
#   geom_histogram(bins=10) +
#   xlim(1.5, 3.5) +
#   ggtitle('n=20') +
#   theme(aspect.ratio=1)
# g3 <- ggplot(d[n==30], aes(xbar)) +
#   geom_histogram(bins=10) +
#   xlim(1.5, 3.5) +
#   ggtitle('n=30') +
#   theme(aspect.ratio=1)
# library(gridExtra)
# grid.arrange(g1, g2, g3, ncol=3)
```


## Central limit theorem

The **central limit theorem** tells us that the sum of independent and
identically distributed random variables approximates a Normal distribution.

Let

$\boldsymbol{Y} = \boldsymbol{X}_1 + \boldsymbol{X}_2 + \dots + \boldsymbol{X}_n$

and

$\boldsymbol{X}_i \sim \boldsymbol{D}$

where $\boldsymbol{D}$ can have any distribution whatsoever. Then,

$\boldsymbol{Y} \sim \mathcal{N}(\mu_{Y}, \sigma_{y}^2)$

### Central limit theorem mean and variance

One way to see where the mean and variance parameters come
from in the central limit theorem is to use the following
rule:

Let 

$Y \sim a X + b$

Then:

$\mathbb{E}\big[Y\big] = a \mathbb{E}\big[X\big] + E(b)$

$\mathbb{Var}\big[Y\big] = (a^2) \mathbb{Var}\big[X\big]$


### Applied to the distribution of sample sums
\begin{align}

\boldsymbol{Y} &= \boldsymbol{X}_1 + \boldsymbol{X}_2 + \dots + \boldsymbol{X}_n \\
\\
E(\boldsymbol{Y}) &= E(\boldsymbol{X}_1 + \boldsymbol{X}_2 + \dots + \boldsymbol{X}_n) \\
    &= E(\boldsymbol{X}_1) + E(\boldsymbol{X}_2) + \dots + E(\boldsymbol{X}_n) \\
    &= \mu_x + \mu_x + \ldots + \mu_x \\
    &= n \mu_x \\
\\   
Var(\boldsymbol{Y}) &= Var(\boldsymbol{X}_1 + \boldsymbol{X}_2 + \dots + \boldsymbol{X}_n) \\
    &= Var(\boldsymbol{X}_1) + Var(\boldsymbol{X}_2) + \dots + Var(\boldsymbol{X}_n) \\
    &= \sigma_x^2 + \sigma_x^2 + \dots + \sigma_x^2 \\
    &= n \sigma_x^2 \\
    
\end{align}

\begin{align}
\mu_{Y} &= n \mu_{X} \\
\sigma_Y^2 &= n \sigma_X^2 \\
\end{align}

### Applied to the distribution of sample means

\begin{align}

\boldsymbol{Y} &= \frac{1}{n} (\boldsymbol{X}_1 + \boldsymbol{X}_2 + \dots + \boldsymbol{X}_n) \\
\\
E(\boldsymbol{Y}) &= E(\frac{1}{n} (\boldsymbol{X}_1 + \boldsymbol{X}_2 + \dots + \boldsymbol{X}_n)) \\
    &= \frac{1}{n} 
    (E(\boldsymbol{X}_1) + E(\boldsymbol{X}_2) + \dots + E(\boldsymbol{X}_n)) \\
    &= \frac{1}{n} (\mu_x + \mu_x + \ldots + \mu_x) \\
    &= \mu_x \\
\\   
Var(\boldsymbol{Y}) &= Var(\frac{1}{n} (\boldsymbol{X}_1 + \boldsymbol{X}_2 + \dots + \boldsymbol{X}_n)) \\
    &= Var(\frac{1}{n^2} (Var(\boldsymbol{X}_1) + Var(\boldsymbol{X}_2) + \dots + Var(\boldsymbol{X}_n)) \\
    &= \frac{1}{n^2} ( \sigma_x^2 + \sigma_x^2 + \dots + \sigma_x^2 ) \\
    &= \frac{1}{n} \sigma_x^2 \\
    
\end{align}

\begin{align}
\mu_{Y} &= \mu_{X} \\
\sigma_Y^2 &= \frac{1}{n} \sigma_X^2 \\
\end{align}

### Standard Error

When applying the central limit theorem to the distribution of sample means, we
get:

\begin{align}
\mu_{Y} &= \mu_{X} \\
\sigma_Y^2 &= \frac{1}{n} \sigma_X^2 \\
\end{align}

When expressed in terms of standard deviation:

\begin{align}
\mu_{Y} &= \mu_{X} \\
\sigma_y &= \frac{1}{\sqrt{n}} \sigma_X \\
\end{align}

Here, $\sigma_Y$ is called the **standard error of the mean (SEM)** and it is
very commonly used to draw error bars on various plots.

</div>